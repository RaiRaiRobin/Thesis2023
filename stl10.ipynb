{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"machine_shape":"hm","gpuType":"A100","mount_file_id":"1BYP9p-b7mIY5wk6yFlScGwznKAjkv0Ga","authorship_tag":"ABX9TyOxWUAVYyO2Hs2XQ0vJcSg/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#Important packages"],"metadata":{"id":"DZEAgPzP8Evy"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"7dyGzIjX2y2r","executionInfo":{"status":"ok","timestamp":1700168417400,"user_tz":300,"elapsed":4809,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"outputs":[],"source":["# Import packages\n","import torch\n","import numpy as np\n","import torchvision.transforms as transforms\n","import random\n","import PIL\n","import PIL.ImageOps\n","import PIL.ImageEnhance\n","import PIL.ImageDraw\n","from PIL import Image\n","import torchvision.datasets as torchdivision_datasets\n","import pickle\n","import os\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import copy\n","import torch.backends.cudnn as cudnn\n","from scipy.optimize import linear_sum_assignment as linear_assignment\n","import math"]},{"cell_type":"markdown","source":["#Change the working directory"],"metadata":{"id":"wSMv6YtC9h5r"}},{"cell_type":"code","source":["import os\n","\n","# Change the current working directory to a directory in Google Drive\n","new_directory_path = \"/content/drive/My Drive/Mercy college/Thesis\"\n","os.chdir(new_directory_path)"],"metadata":{"id":"jDljHfBb9laD","executionInfo":{"status":"ok","timestamp":1700168463349,"user_tz":300,"elapsed":324,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Verify the current working directory\n","current_directory = os.getcwd()\n","print(\"Current Working Directory:\", current_directory)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ehBI2Zgp9sbh","executionInfo":{"status":"ok","timestamp":1700168468116,"user_tz":300,"elapsed":378,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}},"outputId":"97b5fc50-b6c6-4831-a75d-4bfabf45478c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Current Working Directory: /content/drive/My Drive/Mercy college/Thesis\n"]}]},{"cell_type":"markdown","source":["#STL10"],"metadata":{"id":"QZqCmd7R-Mc9"}},{"cell_type":"markdown","source":["##Default variables"],"metadata":{"id":"2NeMi91I-TmU"}},{"cell_type":"code","source":["# Default variables\n","lr = 0.01\n","momentum = 0.9\n","weight_decay = 5e-4\n","epochs = 200\n","batch_size = 100\n","s_thr = 0.99\n","n_num = 100\n","o_model = 'checkpoint/selflabel_stl-10.pth.tar'\n","e_model = 'checkpoint/simclr_stl-10.pth.tar'\n","seed = 1567010775"],"metadata":{"id":"N6niElu1-75M","executionInfo":{"status":"ok","timestamp":1700168475257,"user_tz":300,"elapsed":335,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","np.random.seed(seed)"],"metadata":{"id":"flN4CRlbARXr","executionInfo":{"status":"ok","timestamp":1700168478420,"user_tz":300,"elapsed":1636,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["##Check if the GPU is available"],"metadata":{"id":"HIY6fJlxAXtB"}},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"ZPSmEIk2AZIy","executionInfo":{"status":"ok","timestamp":1700168479422,"user_tz":300,"elapsed":332,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}},"outputId":"52511d2e-189a-42ad-a190-0064ce1569fe"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["##RandAugmentMC"],"metadata":{"id":"qQEN0HTuGcMz"}},{"cell_type":"code","source":["PARAMETER_MAX = 10\n","\n","def AutoContrast(img, **kwarg):\n","    return PIL.ImageOps.autocontrast(img)\n","\n","\n","def Brightness(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    return PIL.ImageEnhance.Brightness(img).enhance(v)\n","\n","\n","def Color(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    return PIL.ImageEnhance.Color(img).enhance(v)\n","\n","\n","def Contrast(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    return PIL.ImageEnhance.Contrast(img).enhance(v)\n","\n","\n","def Cutout(img, v, max_v, bias=0):\n","    if v == 0:\n","        return img\n","    v = _float_parameter(v, max_v) + bias\n","    v = int(v * min(img.size))\n","    return CutoutAbs(img, v)\n","\n","\n","def CutoutAbs(img, v, **kwarg):\n","    w, h = img.size\n","    x0 = np.random.uniform(0, w)\n","    y0 = np.random.uniform(0, h)\n","    x0 = int(max(0, x0 - v / 2.))\n","    y0 = int(max(0, y0 - v / 2.))\n","    x1 = int(min(w, x0 + v))\n","    y1 = int(min(h, y0 + v))\n","    xy = (x0, y0, x1, y1)\n","    # gray\n","    color = (127, 127, 127)\n","    img = img.copy()\n","    PIL.ImageDraw.Draw(img).rectangle(xy, color)\n","    return img\n","\n","\n","def Equalize(img, **kwarg):\n","    return PIL.ImageOps.equalize(img)\n","\n","\n","def Identity(img, **kwarg):\n","    return img\n","\n","\n","def Invert(img, **kwarg):\n","    return PIL.ImageOps.invert(img)\n","\n","\n","def Posterize(img, v, max_v, bias=0):\n","    v = _int_parameter(v, max_v) + bias\n","    return PIL.ImageOps.posterize(img, v)\n","\n","\n","def Rotate(img, v, max_v, bias=0):\n","    v = _int_parameter(v, max_v) + bias\n","    if random.random() < 0.5:\n","        v = -v\n","    return img.rotate(v)\n","\n","\n","def Sharpness(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    return PIL.ImageEnhance.Sharpness(img).enhance(v)\n","\n","\n","def ShearX(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    if random.random() < 0.5:\n","        v = -v\n","    return img.transform(img.size, PIL.Image.AFFINE, (1, v, 0, 0, 1, 0))\n","\n","\n","def ShearY(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    if random.random() < 0.5:\n","        v = -v\n","    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, v, 1, 0))\n","\n","\n","def Solarize(img, v, max_v, bias=0):\n","    v = _int_parameter(v, max_v) + bias\n","    return PIL.ImageOps.solarize(img, 256 - v)\n","\n","\n","def SolarizeAdd(img, v, max_v, bias=0, threshold=128):\n","    v = _int_parameter(v, max_v) + bias\n","    if random.random() < 0.5:\n","        v = -v\n","    img_np = np.array(img).astype(np.int)\n","    img_np = img_np + v\n","    img_np = np.clip(img_np, 0, 255)\n","    img_np = img_np.astype(np.uint8)\n","    img = Image.fromarray(img_np)\n","    return PIL.ImageOps.solarize(img, threshold)\n","\n","\n","def TranslateX(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    if random.random() < 0.5:\n","        v = -v\n","    v = int(v * img.size[0])\n","    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n","\n","\n","def TranslateY(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    if random.random() < 0.5:\n","        v = -v\n","    v = int(v * img.size[1])\n","    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n","\n","\n","def _float_parameter(v, max_v):\n","    return float(v) * max_v / PARAMETER_MAX\n","\n","\n","def _int_parameter(v, max_v):\n","    return int(v * max_v / PARAMETER_MAX)\n","\n","\n","def augment_pool():\n","    # FixMatch paper\n","    augs = [(AutoContrast, None, None),\n","            (Brightness, 0.99, 0.01),\n","            (Color, 0.99, 0.01),\n","            (Contrast, 0.99, 0.01),\n","            (Equalize, None, None),\n","            (Identity, None, None),\n","            (Posterize, 1, 8),\n","            (Rotate, 45, -45),\n","            (Sharpness, 0.99, 0.01),\n","            (ShearX, 0.3, -0.3),\n","            (ShearY, 0.3, -0.3),\n","            (Solarize, 256, 0),\n","            (TranslateX, 0.3, -0.3),\n","            (TranslateY, 0.3, -0.3)]\n","    return augs\n","\n","\n","class RandAugmentMC(object):\n","    def __init__(self, n, m):\n","        assert n >= 1\n","        assert 1 <= m <= 10\n","        self.n = n\n","        self.m = m\n","        self.augment_pool = augment_pool()\n","\n","    def __call__(self, img):\n","        ops = random.sample(self.augment_pool, k=self.n)\n","        for op, max_v, bias in ops:\n","            v = np.random.randint(1, self.m)\n","            if random.random() < 0.5:\n","                img = op(img, v=v, max_v=max_v, bias=bias)\n","        img = CutoutAbs(img, 16)\n","        return img"],"metadata":{"id":"K22k-WH5GeMg","executionInfo":{"status":"ok","timestamp":1700168481827,"user_tz":300,"elapsed":449,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["##STLRUC"],"metadata":{"id":"i64tDCb5IT3g"}},{"cell_type":"code","source":["class STLRUC(torchdivision_datasets.STL10):\n","    def __init__(self, root, split='labeled', folds=None, transform=None, transform2=None, transform3=None, transform4 =None, target_transform=None, download=False):\n","        self.root = root\n","        self.split = split\n","        self.folds = folds\n","        self.transform = transform\n","        self.transform2 = transform2\n","        self.transform3 = transform3\n","        self.transform4 = transform4\n","        self.target_transform = target_transform\n","\n","        if download:\n","            self.download()\n","\n","        # self.download()\n","\n","\n","        # now load the picked numpy arrays\n","        if self.split == 'train':\n","            self.data, self.labels = self.__loadfile(\n","                self.train_list[0][0], self.train_list[1][0])\n","            self.__load_folds(folds)\n","\n","        elif self.split == 'train+unlabeled':\n","            self.data, self.labels = self.__loadfile(\n","                self.train_list[0][0], self.train_list[1][0])\n","            self.__load_folds(folds)\n","            unlabeled_data, _ = self.__loadfile(self.train_list[2][0])\n","            self.data = np.concatenate((self.data, unlabeled_data))\n","            self.labels = np.concatenate(\n","                (self.labels, np.asarray([-1] * unlabeled_data.shape[0])))\n","\n","        elif self.split == 'unlabeled':\n","            self.data, _ = self.__loadfile(self.train_list[2][0])\n","            self.labels = np.asarray([-1] * self.data.shape[0])\n","\n","        elif self.split == 'labeled':\n","            self.data, self.labels = self.__loadfile(self.train_list[0][0], self.train_list[1][0])\n","            self.__load_folds(folds)\n","            test_data, test_labels = self.__loadfile(self.test_list[0][0], self.test_list[1][0])\n","            self.data = np.concatenate((self.data, test_data))\n","            self.labels = np.concatenate((self.labels, test_labels))\n","        else:  # self.split == 'test':\n","            self.test_data, self.test_labels = self.__loadfile(self.test_list[0][0], self.test_list[1][0])\n","\n","        class_file = os.path.join(self.root, self.base_folder, self.class_names_file)\n","        if os.path.isfile(class_file):\n","            with open(class_file) as f:\n","                self.classes = f.read().splitlines()\n","\n","    def __loadfile(self, data_file, labels_file=None):\n","        labels = None\n","        if labels_file:\n","            path_to_labels = os.path.join(\n","                self.root, self.base_folder, labels_file)\n","            with open(path_to_labels, 'rb') as f:\n","                labels = np.fromfile(f, dtype=np.uint8) - 1  # 0-based\n","\n","        path_to_data = os.path.join(self.root, self.base_folder, data_file)\n","        with open(path_to_data, 'rb') as f:\n","            # read whole file in uint8 chunks\n","            everything = np.fromfile(f, dtype=np.uint8)\n","            images = np.reshape(everything, (-1, 3, 96, 96))\n","            images = np.transpose(images, (0, 1, 3, 2))\n","\n","        return images, labels\n","\n","    def __load_folds(self, folds):\n","        # loads one of the folds if specified\n","        if folds is None:\n","            return\n","        path_to_folds = os.path.join(\n","            self.root, self.base_folder, self.folds_list_file)\n","        with open(path_to_folds, 'r') as f:\n","            str_idx = f.read().splitlines()[folds]\n","            list_idx = np.fromstring(str_idx, dtype=np.uint8, sep=' ')\n","            self.data, self.labels = self.data[list_idx, :, :, :], self.labels[list_idx]\n","\n","    def __getitem__(self, index):\n","        img, target = self.data[index], self.labels[index]\n","        img = Image.fromarray(np.transpose(img, (1, 2, 0)))\n","\n","        if self.target_transform is not None:\n","            target = self.target_transform(target)\n","\n","        if self.transform is not None:\n","            img1 = self.transform(img)\n","            if self.split == 'labeled' or 'train+unlabeled':\n","                img2 = self.transform2(img)\n","                img3 = self.transform3(img)\n","            if self.transform4 != None:\n","                img4 = self.transform4(img)\n","\n","        if self.split == 'labeled' or 'train+unlabeled':\n","            if self.transform4 != None:\n","                return img1, img2, img3, img4, target, index\n","            else:\n","                return img1, img2, img3, target, index\n","        else:\n","            return img1, target, index"],"metadata":{"id":"VsV3HAd4IVUR","executionInfo":{"status":"ok","timestamp":1700169146802,"user_tz":300,"elapsed":347,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["##preprocess"],"metadata":{"id":"JxyC8r7sA8LW"}},{"cell_type":"code","source":["def preprocess():\n","    mean = (0.4914, 0.4822, 0.4465)\n","    std = (0.2023, 0.1994, 0.2010)\n","    transform_train = transforms.Compose([\n","        transforms.RandomResizedCrop(size=96, scale=(0.2,1.)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=mean, std=std),\n","    ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.Resize(96),\n","        transforms.CenterCrop(96),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=mean, std=std),\n","    ])\n","\n","    transform_strong = transforms.Compose([\n","            transforms.RandomResizedCrop(size=96, scale=(0.2,1.)),\n","            transforms.RandomHorizontalFlip(),\n","            RandAugmentMC(n=2, m=2),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=mean, std=std)])\n","\n","    trainset = STLRUC(root=\"./data/stl10\", transform=transform_test, transform2 = transform_train, transform3 = transform_train,transform4 = transform_strong, download=False)\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last= False)\n","    testset = STLRUC(root=\"./data/stl10\",transform=transform_test, transform2 = transform_test, transform3 = transform_test,  download=False)\n","    evalloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=4)\n","\n","    return trainset, trainloader, testset, evalloader, 10"],"metadata":{"id":"ULO8SHsdA_5V","executionInfo":{"status":"ok","timestamp":1700168507025,"user_tz":300,"elapsed":3,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["##Download and split dataset"],"metadata":{"id":"vs9s84kBJYnz"}},{"cell_type":"code","source":["trainset, trainloader, testset, evalloader, class_num = preprocess()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cFEP5lIpA3Ew","executionInfo":{"status":"ok","timestamp":1700169116493,"user_tz":300,"elapsed":387175,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}},"outputId":"c23b4755-de3a-4d09-e0ad-e65357e53e1e"},"execution_count":11,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to ./data/stl10/stl10_binary.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2640397119/2640397119 [09:06<00:00, 4831083.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/stl10/stl10_binary.tar.gz to ./data/stl10\n","Files already downloaded and verified\n"]}]},{"cell_type":"markdown","source":["##ClusteringModel"],"metadata":{"id":"s5MaCF8VwMRs"}},{"cell_type":"code","source":["class ClusteringModel(nn.Module):\n","    def __init__(self, backbone, class_num):\n","        super(ClusteringModel, self).__init__()\n","        self.backbone = backbone\n","        self.cluster_head = nn.ModuleList([nn.Linear(512, class_num) for _ in range(1)])\n","\n","    def forward(self, x):\n","        features = self.backbone(x)\n","        out = [cluster_head(features) for cluster_head in self.cluster_head]\n","\n","        return out[0]"],"metadata":{"id":"_SrctFBXwNId","executionInfo":{"status":"ok","timestamp":1700169152365,"user_tz":300,"elapsed":324,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["##BasicBlock"],"metadata":{"id":"lDM97wBAxxG8"}},{"cell_type":"code","source":["class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1, is_last=False):\n","        super(BasicBlock, self).__init__()\n","        self.is_last = is_last\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion * planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion * planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        preact = out\n","        out = F.relu(out)\n","        if self.is_last:\n","            return out, preact\n","        else:\n","            return out"],"metadata":{"id":"qkL8lsQWxvOI","executionInfo":{"status":"ok","timestamp":1700169154896,"user_tz":300,"elapsed":329,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["##Bottleneck"],"metadata":{"id":"ATi-MVrhx42Y"}},{"cell_type":"code","source":["class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1, is_last=False):\n","        super(Bottleneck, self).__init__()\n","        self.is_last = is_last\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion * planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion * planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        preact = out\n","        out = F.relu(out)\n","        if self.is_last:\n","            return out, preact\n","        else:\n","            return out"],"metadata":{"id":"k4FDP30ax4IQ","executionInfo":{"status":"ok","timestamp":1700169157740,"user_tz":300,"elapsed":346,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["##ResNet"],"metadata":{"id":"Cyo8U1ZuwlI1"}},{"cell_type":"code","source":["class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, in_channel=3, zero_init_residual=False):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1,bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.avgpool = nn.AvgPool2d(7, stride=1)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","        # Zero-initialize the last BN in each residual branch,\n","        # so that the residual branch starts with zeros, and each residual block behaves\n","        # like an identity. This improves the model by 0.2~0.3% according to:\n","        # https://arxiv.org/abs/1706.02677\n","        if zero_init_residual:\n","            for m in self.modules():\n","                if isinstance(m, Bottleneck):\n","                    nn.init.constant_(m.bn3.weight, 0)\n","                elif isinstance(m, BasicBlock):\n","                    nn.init.constant_(m.bn2.weight, 0)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1] * (num_blocks - 1)\n","        layers = []\n","        for i in range(num_blocks):\n","            stride = strides[i]\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = self.maxpool(F.relu(self.bn1(self.conv1(x))))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = self.avgpool(out)\n","        out = torch.flatten(out, 1)\n","        return out"],"metadata":{"id":"35FqWtoUwiBG","executionInfo":{"status":"ok","timestamp":1700169161352,"user_tz":300,"elapsed":330,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def Resnet_STL():\n","    return ResNet(BasicBlock, [2, 2, 2, 2])"],"metadata":{"id":"kMj0luQRwdpf","executionInfo":{"status":"ok","timestamp":1700169171984,"user_tz":300,"elapsed":343,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["net = ClusteringModel(Resnet_STL(), class_num)"],"metadata":{"id":"TbVSW1gNvu6v","executionInfo":{"status":"ok","timestamp":1700169173594,"user_tz":300,"elapsed":351,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["net2 = copy.deepcopy(net)"],"metadata":{"id":"xAPmukoByOpy","executionInfo":{"status":"ok","timestamp":1700169175583,"user_tz":300,"elapsed":303,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["net_uc = copy.deepcopy(net)"],"metadata":{"id":"U0woehpbyP9-","executionInfo":{"status":"ok","timestamp":1700169179039,"user_tz":300,"elapsed":599,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["##ContrastiveModel"],"metadata":{"id":"UQcho7vcycbA"}},{"cell_type":"code","source":["class ContrastiveModel(nn.Module):\n","    def __init__(self, backbone, head='mlp', features_dim=128):\n","        super(ContrastiveModel, self).__init__()\n","        self.backbone = backbone\n","        self.backbone_dim = 512\n","        self.head = head\n","\n","        if head == 'linear':\n","            self.contrastive_head = nn.Linear(self.backbone_dim, features_dim)\n","\n","        elif head == 'mlp':\n","            self.contrastive_head = nn.Sequential(\n","                    nn.Linear(self.backbone_dim, self.backbone_dim),\n","                    nn.ReLU(), nn.Linear(self.backbone_dim, features_dim))\n","\n","        else:\n","            raise ValueError('Invalid head {}'.format(head))\n","\n","    def forward(self, x):\n","        features = self.contrastive_head(self.backbone(x))\n","        features = F.normalize(features, dim = 1)\n","        return features"],"metadata":{"id":"zedpuAnjybsG","executionInfo":{"status":"ok","timestamp":1700169182598,"user_tz":300,"elapsed":359,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["net_embd = ContrastiveModel(Resnet_STL())"],"metadata":{"id":"8BfePrUEyRfz","executionInfo":{"status":"ok","timestamp":1700169185517,"user_tz":300,"elapsed":324,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["try:\n","    state_dict = torch.load(o_model)\n","    state_dict2 = torch.load(e_model)\n","    net_uc.load_state_dict(state_dict)\n","    net_embd.load_state_dict(state_dict2, strict = True)\n","    net.load_state_dict(state_dict, strict = False)\n","    net2.load_state_dict(state_dict, strict = False)\n","    net.cluster_head = nn.ModuleList([nn.Linear(512, class_num) for _ in range(1)])\n","    net2.cluster_head = nn.ModuleList([nn.Linear(512, class_num) for _ in range(1)])\n","except:\n","    print(\"Check Model Directory!\")\n","    # exit(0)"],"metadata":{"id":"aaJ8xzHLymrx","executionInfo":{"status":"ok","timestamp":1700169195284,"user_tz":300,"elapsed":8767,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"MPPbxao0yvFf","executionInfo":{"status":"ok","timestamp":1700169195284,"user_tz":300,"elapsed":10,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}},"outputId":"3e37e0d5-a0da-467f-b269-7c3a138cb6a5"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["if device == 'cuda':\n","  net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n","  net2 = torch.nn.DataParallel(net2, device_ids=range(torch.cuda.device_count()))\n","  net_uc = torch.nn.DataParallel(net_uc, device_ids=range(torch.cuda.device_count()))\n","  net_embd = torch.nn.DataParallel(net_embd, device_ids=range(torch.cuda.device_count()))\n","  cudnn.benchmark = True"],"metadata":{"id":"rFRPLebtyw61","executionInfo":{"status":"ok","timestamp":1700169200364,"user_tz":300,"elapsed":331,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["net.to(device)\n","net2.to(device)\n","net_uc.to(device)\n","net_embd.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mFqAnkLAy3gw","executionInfo":{"status":"ok","timestamp":1700169202624,"user_tz":300,"elapsed":295,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}},"outputId":"5d66999f-4775-4f61-d3fb-1902386646cd"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataParallel(\n","  (module): ContrastiveModel(\n","    (backbone): ResNet(\n","      (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): BasicBlock(\n","          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (shortcut): Sequential()\n","        )\n","        (1): BasicBlock(\n","          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (shortcut): Sequential()\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): BasicBlock(\n","          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (shortcut): Sequential(\n","            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): BasicBlock(\n","          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (shortcut): Sequential()\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): BasicBlock(\n","          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (shortcut): Sequential(\n","            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): BasicBlock(\n","          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (shortcut): Sequential()\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): BasicBlock(\n","          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (shortcut): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): BasicBlock(\n","          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (shortcut): Sequential()\n","        )\n","      )\n","      (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n","    )\n","    (contrastive_head): Sequential(\n","      (0): Linear(in_features=512, out_features=512, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=512, out_features=128, bias=True)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["##criterion_rb"],"metadata":{"id":"L9-PhoX1zPQO"}},{"cell_type":"code","source":["class criterion_rb(object):\n","    def __call__(self, outputs_x, targets_x, outputs_u, targets_u, epoch):\n","        # Clean sample Loss\n","        probs_u = torch.softmax(outputs_u, dim=1)\n","        Lx = -torch.mean(torch.sum(F.log_softmax(outputs_x, dim=1) * targets_x, dim=1))\n","        Lu = 50*torch.mean((probs_u - targets_u)**2)\n","        Lu = linear_rampup(epoch) * Lu\n","        return Lx, Lu"],"metadata":{"id":"dlqxgdTBzOjY","executionInfo":{"status":"ok","timestamp":1700169206283,"user_tz":300,"elapsed":354,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["optimizer1 = torch.optim.SGD(net.parameters(), lr, momentum=momentum, weight_decay=weight_decay, nesterov=True)\n","optimizer2 = torch.optim.SGD(net2.parameters(), lr, momentum=momentum, weight_decay=weight_decay, nesterov=True)\n","criterion = criterion_rb()"],"metadata":{"id":"ah65_NXfzDgr","executionInfo":{"status":"ok","timestamp":1700169209712,"user_tz":300,"elapsed":322,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["##_hungarian_match"],"metadata":{"id":"3NWr7i-i0RlT"}},{"cell_type":"code","source":["def _hungarian_match(flat_preds, flat_targets, num_samples, class_num):\n","    num_k = class_num\n","    num_correct = np.zeros((num_k, num_k))\n","\n","    for c1 in range(0, num_k):\n","        for c2 in range(0, num_k):\n","        # elementwise, so each sample contributes once\n","            votes = int(((flat_preds == c1) * (flat_targets == c2)).sum())\n","            num_correct[c1, c2] = votes\n","\n","    # num_correct is small\n","    match = linear_assignment(num_samples - num_correct)\n","\n","    # return as list of tuples, out_c to gt_c\n","    res = []\n","    for i in range(len(match[0])):\n","        out_c = match[0][i]\n","        gt_c = match[1][i]\n","        res.append((out_c, gt_c))\n","\n","    return res"],"metadata":{"id":"kvoySVCW0Q1s","executionInfo":{"status":"ok","timestamp":1700169211090,"user_tz":300,"elapsed":462,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["def test(net, testloader,device, class_num):\n","    net.eval()\n","    predicted_all = []\n","    targets_all = []\n","    for batch_idx, (inputs, _,_, targets, indexes) in enumerate(testloader):\n","        batchSize = inputs.size(0)\n","        targets, inputs = targets.to(device), inputs.to(device)\n","        output = net(inputs)\n","        predicted = torch.argmax(output, 1)\n","        predicted_all.append(predicted)\n","        targets_all.append(targets)\n","\n","\n","    flat_predict = torch.cat(predicted_all).to(device)\n","    flat_target = torch.cat(targets_all).to(device)\n","    num_samples = flat_predict.shape[0]\n","    match = _hungarian_match(flat_predict, flat_target, num_samples, class_num)\n","    reordered_preds = torch.zeros(num_samples).to(device)\n","\n","    for pred_i, target_i in match:\n","        reordered_preds[flat_predict == pred_i] = int(target_i)\n","\n","    acc = int((reordered_preds == flat_target.float()).sum()) / float(num_samples) * 100\n","\n","    return acc, reordered_preds"],"metadata":{"id":"F4eHR_hxz_n8","executionInfo":{"status":"ok","timestamp":1700169213476,"user_tz":300,"elapsed":711,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["##Extract Pseudo Label"],"metadata":{"id":"ujuMLeFw1xkG"}},{"cell_type":"code","source":["# Extract Pseudo Label\n","acc_uc, p_label= test(net_uc, evalloader, device, class_num)\n","print(acc_uc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cOrWbejczezl","executionInfo":{"status":"ok","timestamp":1700169229589,"user_tz":300,"elapsed":13072,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}},"outputId":"c5f89c79-d1e0-44aa-8b5c-de181a75565a"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["81.38461538461539\n"]}]},{"cell_type":"markdown","source":["##extract_confidence"],"metadata":{"id":"E_HDAYm92E3s"}},{"cell_type":"code","source":["def extract_confidence(net, p_label, evalloader, threshold):\n","    net.eval()\n","    devide = torch.tensor([]).cuda()\n","    clean_num = 0\n","    correct_num = 0\n","    for batch_idx, (inputs1, _, _, targets, indexes) in enumerate(evalloader):\n","        inputs1, targets = inputs1.cuda(), targets.cuda().float()\n","        labels = p_label[indexes].float()\n","        logits = net(inputs1)\n","        prob = torch.softmax(logits.detach_(), dim=-1)\n","        max_probs, _ = torch.max(prob, dim=-1)\n","        mask = max_probs.ge(threshold).float()\n","        devide = torch.cat([devide, mask])\n","        s_idx = (mask == 1)\n","        clean_num += labels[s_idx].shape[0]\n","        correct_num += torch.sum((labels[s_idx] == targets[s_idx])).item()\n","\n","    print(correct_num, clean_num)\n","    return devide"],"metadata":{"id":"f8so-KGx2DoP","executionInfo":{"status":"ok","timestamp":1700169233977,"user_tz":300,"elapsed":315,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":["##extract_metric"],"metadata":{"id":"li0J1hrQ2MaH"}},{"cell_type":"code","source":["def extract_metric(net, p_label, evalloader, n_num):\n","    net.eval()\n","    feature_bank = []\n","    with torch.no_grad():\n","        for batch_idx, (inputs1 , _, _, _, indexes) in enumerate(evalloader):\n","            out = net(inputs1.cuda())\n","            feature_bank.append(out)\n","        feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()\n","        sim_indices_list = []\n","        for batch_idx, (inputs1 , _, _, _, indexes) in enumerate(evalloader):\n","            out = net(inputs1.cuda(non_blocking=True))\n","            sim_matrix = torch.mm(out, feature_bank)\n","            _, sim_indices = sim_matrix.topk(k=n_num, dim=-1)\n","            sim_indices_list.append(sim_indices)\n","        feature_labels = p_label.cuda()\n","        first = True\n","        count = 0\n","        clean_num = 0\n","        correct_num = 0\n","        for batch_idx, (inputs1 , _, _, targets, indexes) in enumerate(evalloader):\n","            labels = p_label[indexes].cuda().long()\n","            sim_indices = sim_indices_list[count]\n","            sim_labels = torch.gather(feature_labels.expand(inputs1.size(0), -1), dim=-1, index=sim_indices)\n","            # counts for each class\n","            one_hot_label = torch.zeros(inputs1.size(0) * sim_indices.size(1), 10).cuda()\n","            one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1).long(), value=1.0)\n","            pred_scores = torch.sum(one_hot_label.view(inputs1.size(0), -1, 10), dim=1)\n","            count += 1\n","            pred_labels = pred_scores.argsort(dim=-1, descending=True)\n","            prob, _ = torch.max(F.softmax(pred_scores, dim=-1), 1)\n","            # Check whether prediction and current label are same\n","            noisy_label = labels\n","            s_idx1 = (pred_labels[:, :1].float() == labels.unsqueeze(dim=-1).float()).any(dim=-1).float()\n","            s_idx = (s_idx1 == 1.0)\n","            clean_num += labels[s_idx].shape[0]\n","            # correct_num += torch.sum((labels[s_idx].float() == targets[s_idx].cuda().float())).item()\n","            correct_num += torch.sum((labels[s_idx].float() == targets[s_idx.to(targets.device)].cuda().float())).item()\n","\n","            if first:\n","                prob_set = prob\n","                pred_same_label_set = s_idx\n","                first = False\n","            else:\n","                prob_set = torch.cat((prob_set, prob), dim = 0)\n","                pred_same_label_set = torch.cat((pred_same_label_set, s_idx), dim = 0)\n","\n","        print(correct_num, clean_num)\n","        return pred_same_label_set"],"metadata":{"id":"skzU-YLU2Lof","executionInfo":{"status":"ok","timestamp":1700169237835,"user_tz":300,"elapsed":328,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["##extract_hybrid"],"metadata":{"id":"wTbT9VTT2Vpb"}},{"cell_type":"code","source":["def extract_hybrid(devide1, devide2, p_label, evalloader):\n","    devide = (devide1.float() + devide2.float() == 2)\n","    clean_num = 0\n","    correct_num = 0\n","    for batch_idx, (inputs1, _, _, targets, indexes) in enumerate(evalloader):\n","        inputs1, targets = inputs1.cuda(), targets.cuda().float()\n","        labels = p_label[indexes].float()\n","        mask = devide[indexes]\n","        s_idx = (mask == 1)\n","        clean_num += labels[s_idx].shape[0]\n","        correct_num += torch.sum((labels[s_idx] == targets[s_idx])).item()\n","\n","    print(correct_num, clean_num)\n","    return devide"],"metadata":{"id":"rFmhOR7J2UEn","executionInfo":{"status":"ok","timestamp":1700169241201,"user_tz":300,"elapsed":322,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["##Divide Clean and Noisy set"],"metadata":{"id":"vgUYLiOo13E8"}},{"cell_type":"code","source":["# Divide Clean and Noisy set\n","devide1 = extract_confidence(net_uc, p_label, evalloader, s_thr)\n","devide2 = extract_metric(net_embd, p_label, evalloader, n_num)\n","devide = extract_hybrid(devide1, devide2, p_label, evalloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4LjRQm5Z10aH","executionInfo":{"status":"ok","timestamp":1700169269319,"user_tz":300,"elapsed":26002,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}},"outputId":"968f65a6-b0c7-4991-de12-461305589994"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["8399 9028\n","9986 11400\n","8269 8795\n"]}]},{"cell_type":"code","source":["conf1 =  torch.zeros(13000)\n","conf2 =  torch.zeros(13000)"],"metadata":{"id":"TXABlnWe2tqj","executionInfo":{"status":"ok","timestamp":1700169269319,"user_tz":300,"elapsed":10,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["##test_ruc"],"metadata":{"id":"tVyhb2-K3IRY"}},{"cell_type":"code","source":["def test_ruc(net, net2, testloader, device, class_num):\n","    net.eval()\n","    net2.eval()\n","\n","    predicted_all = [[] for i in range(0,3)]\n","    targets_all = []\n","    acc_list = []\n","    p_label_list = []\n","\n","    for batch_idx, (inputs, _, _, targets, indexes) in enumerate(testloader):\n","        batchSize = inputs.size(0)\n","        targets, inputs = targets.to(device), inputs.to(device)\n","        logit = net(inputs)\n","        logit2 = net2(inputs)\n","        _, predicted = torch.max(logit, 1)\n","        _, predicted2 = torch.max(logit2, 1)\n","        _, predicted3 = torch.max(logit + logit2, 1)\n","\n","        predicted_all[0].append(predicted)\n","        predicted_all[1].append(predicted2)\n","        predicted_all[2].append(predicted3)\n","        targets_all.append(targets)\n","\n","    for i in range(0, 3):\n","        flat_predict = torch.cat(predicted_all[i]).to(device)\n","        flat_target = torch.cat(targets_all).to(device)\n","        num_samples = flat_predict.shape[0]\n","        acc = int((flat_predict.float() == flat_target.float()).sum()) / float(num_samples) * 100\n","        acc_list.append(acc)\n","        p_label_list.append(flat_predict)\n","\n","    return acc_list, p_label_list"],"metadata":{"id":"IwBu4XPL26YM","executionInfo":{"status":"ok","timestamp":1700169272110,"user_tz":300,"elapsed":344,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["##adjust_learning_rate"],"metadata":{"id":"P6C2Cdqh3kI9"}},{"cell_type":"code","source":["def adjust_learning_rate(lr, epochs, optimizer, epoch):\n","    # cosine learning rate schedule\n","    lr = lr\n","    lr *= 0.5 * (1. + math.cos(math.pi * epoch / epochs))\n","\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr"],"metadata":{"id":"9FipkTDa3jfm","executionInfo":{"status":"ok","timestamp":1700169275674,"user_tz":300,"elapsed":340,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["##AverageMeter"],"metadata":{"id":"xmhlA3Md4ZOm"}},{"cell_type":"code","source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"metadata":{"id":"l1Exwqoq4Yn4","executionInfo":{"status":"ok","timestamp":1700169277663,"user_tz":300,"elapsed":345,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":["##get_threshold"],"metadata":{"id":"6ji36FDi4zCv"}},{"cell_type":"code","source":["def get_threshold(current):\n","    return 0.9 + 0.02*int(current / 40)"],"metadata":{"id":"OEbUGoUr4yfb","executionInfo":{"status":"ok","timestamp":1700169280527,"user_tz":300,"elapsed":337,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":["##linear_rampup"],"metadata":{"id":"Xq3aA32s46_-"}},{"cell_type":"code","source":["def linear_rampup(current, rampup_length=200):\n","    if rampup_length == 0:\n","        return 1.0\n","    else:\n","        current = np.clip((current) / rampup_length, 0.1, 1.0)\n","        return float(current)"],"metadata":{"id":"dnLBgD1R46gS","executionInfo":{"status":"ok","timestamp":1700169283292,"user_tz":300,"elapsed":379,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["##LabelSmoothLoss"],"metadata":{"id":"b9JafNDS5Ybo"}},{"cell_type":"code","source":["class LabelSmoothLoss(nn.Module):\n","\n","    def __init__(self, smoothing=0.0):\n","        super(LabelSmoothLoss, self).__init__()\n","        self.smoothing = smoothing\n","\n","    def forward(self, input, target):\n","        log_prob = F.log_softmax(input, dim=-1)\n","        weight = input.new_ones(input.size()) * \\\n","            self.smoothing / (input.size(-1) - 1.)\n","        weight.scatter_(-1, target.unsqueeze(-1).long(), (1. - self.smoothing))\n","        loss = (-weight * log_prob).sum(dim=-1).mean()\n","        return loss"],"metadata":{"id":"otiiiwR_5X1d","executionInfo":{"status":"ok","timestamp":1700169285569,"user_tz":300,"elapsed":350,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["LSloss = LabelSmoothLoss(smoothing=0.5)"],"metadata":{"id":"GAl14Jxw5cUs","executionInfo":{"status":"ok","timestamp":1700169286997,"user_tz":300,"elapsed":2,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":["##Train"],"metadata":{"id":"U0hG6fhw3Jvo"}},{"cell_type":"code","source":["def train(epoch, net, net2, trainloader, optimizer, criterion_rb, devide, p_label, conf, batch_size):\n","    train_loss = AverageMeter()\n","    net.train()\n","    net2.train()\n","\n","    num_iter = (len(trainloader.dataset)//batch_size)+1\n","    # adjust learning rate\n","    adjust_learning_rate(lr, epochs, optimizer, epoch)\n","    optimizer.zero_grad()\n","    correct_u = 0\n","    unsupervised = 0\n","    conf_self = torch.zeros(13000)\n","    for batch_idx, (inputs1 , inputs2, inputs3, inputs4, targets, indexes) in enumerate(trainloader):\n","        inputs1, inputs2, inputs3, inputs4, targets = inputs1.float().cuda(), inputs2.float().cuda(), inputs3.float().cuda(), inputs4.float().cuda(), targets.cuda().long()\n","        s_idx = (devide[indexes] == 1)\n","        u_idx = (devide[indexes] == 0)\n","        labels = p_label[indexes].cuda().long()\n","        labels_x = torch.tensor(p_label[indexes][s_idx]).squeeze().long().cpu()\n","        target_x = torch.zeros(labels_x.shape[0], 10).scatter_(1, labels_x.view(-1,1), 1).float().cuda()\n","\n","        logit_o, logit_w1, logit_w2, logit_s = net(inputs1), net(inputs2), net(inputs3), net(inputs4)\n","        logit_s = logit_s[s_idx]\n","        max_probs, _ = torch.max(torch.softmax(logit_o, dim=1), dim=-1)\n","        conf_self[indexes] = max_probs.detach().cpu()\n","        optimizer.zero_grad()\n","\n","        with torch.no_grad():\n","            # compute guessed labels of unlabel samples\n","            outputs_u11 = logit_w1[u_idx]\n","            outputs_u21  = logit_w2[u_idx]\n","            logit_o2 = net2(inputs1)\n","            logit_w12 = net2(inputs2)\n","            logit_w22 = net2(inputs3)\n","            outputs_u12 = logit_w12[u_idx]\n","            outputs_u22  = logit_w22[u_idx]\n","            pu = (torch.softmax(outputs_u11, dim=1) + torch.softmax(outputs_u21, dim=1) + torch.softmax(outputs_u12, dim=1) + torch.softmax(outputs_u22, dim=1)) / 4\n","            ptu = pu**(1/0.5) # temparature sharpening\n","            target_u = ptu / ptu.sum(dim=1, keepdim=True) # normalize\n","            target_u = target_u.detach().float()\n","\n","            px = torch.softmax(logit_o2[s_idx], dim=1)\n","\n","            indexes = indexes.cuda()\n","            conf = conf.cuda()\n","            w_x = conf[indexes][s_idx]\n","\n","            w_x = w_x.view(-1,1).float().cuda()\n","            px = (1-w_x)*target_x + w_x*px\n","            ptx = px**(1/0.5) # temparature sharpening\n","            target_x = ptx / ptx.sum(dim=1, keepdim=True) # normalize\n","            target_x = target_x.detach().float()\n","\n","            if logit_o[u_idx].shape[0] > 0:\n","                max_probs, targets_u1 = torch.max(torch.softmax(logit_o[u_idx], dim=1), dim=-1)\n","                thr = get_threshold(epoch)\n","                mask_u = max_probs.ge(thr).float()\n","                u_idx2 = (mask_u == 1)\n","                unsupervised += torch.sum(mask_u).item()\n","                correct_u += torch.sum((targets_u1[u_idx2] == targets[u_idx][u_idx2])).item()\n","                update = indexes[u_idx][u_idx2]\n","                devide[update] = True\n","                p_label[update] = targets_u1[u_idx2].float()\n","\n","\n","        l = np.random.beta(4.0, 4.0)\n","        l = max(l, 1-l)\n","\n","        all_inputs = torch.cat([inputs2[s_idx], inputs3[s_idx], inputs2[u_idx], inputs3[u_idx]],dim=0)\n","        all_targets = torch.cat([target_x, target_x, target_u, target_u], dim=0)\n","        idx = torch.randperm(all_inputs.size(0))\n","\n","        input_a, input_b = all_inputs, all_inputs[idx]\n","        target_a, target_b = all_targets, all_targets[idx]\n","\n","        mixed_input = l * input_a + (1 - l) * input_b\n","        mixed_target = l * target_a + (1 - l) * target_b\n","\n","        logits = net(mixed_input)\n","        batch_size = target_x.shape[0]\n","\n","        Lx, Lu = criterion_rb(logits[:batch_size*2], mixed_target[:batch_size*2], logits[batch_size*2:], mixed_target[batch_size*2:], epoch+batch_idx/num_iter)\n","        total_loss = Lx + Lu + LSloss(logit_s, labels_x.cuda())\n","\n","        total_loss.backward()\n","        train_loss.update(total_loss.item(), inputs2.size(0))\n","        optimizer.step()\n","\n","        if batch_idx % 80 == 0:\n","            print('Epoch: [{epoch}][{elps_iters}/{tot_iters}] '\n","                  'Train loss: {train_loss.val:.4f} ({train_loss.avg:.4f}) '.format(\n","                      epoch=epoch, elps_iters=batch_idx,tot_iters=len(trainloader),\n","                      train_loss=train_loss))\n","    conf_self = (conf_self - conf_self.min()) / (conf_self.max() - conf_self.min())\n","    return train_loss.avg, devide, p_label, conf_self"],"metadata":{"id":"dc2vxWXs3Gj1","executionInfo":{"status":"ok","timestamp":1700169289295,"user_tz":300,"elapsed":689,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["for epoch in range(epochs):\n","    print(\"== Train RUC ==\")\n","    loss, devide, p_label, conf1 = train(epoch, net, net2, trainloader, optimizer1, criterion, devide, p_label, conf2, batch_size)\n","    loss, devide, p_label, conf2 = train(epoch, net2, net, trainloader, optimizer2, criterion, devide, p_label, conf1, batch_size)\n","    acc, p_list = test_ruc(net, net2, evalloader, device, class_num)\n","    print(\"accuracy: {}\\n\".format(acc))\n","\n","    state = {'net1': net.state_dict(),\n","              'net2': net2.state_dict() }\n","    torch.save(state, './checkpoint/ruc_stl10.t7')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yp4PY2Ck2xc4","executionInfo":{"status":"ok","timestamp":1700179803699,"user_tz":300,"elapsed":10416842,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}},"outputId":"6789d7a7-4a24-4ffb-f6e9-574894a0ecb6"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["== Train RUC ==\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-44-94f2b9c4d4c4>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels_x = torch.tensor(p_label[indexes][s_idx]).squeeze().long().cpu()\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [0][0/130] Train loss: 3.2200 (3.2200) \n","Epoch: [0][80/130] Train loss: 3.1520 (3.3858) \n","Epoch: [0][0/130] Train loss: 3.2503 (3.2503) \n","Epoch: [0][80/130] Train loss: 3.4344 (3.3664) \n","accuracy: [82.1, 81.83076923076923, 82.37692307692308]\n","\n","== Train RUC ==\n","Epoch: [1][0/130] Train loss: 3.4905 (3.4905) \n","Epoch: [1][80/130] Train loss: 3.3985 (3.3056) \n","Epoch: [1][0/130] Train loss: 2.9961 (2.9961) \n","Epoch: [1][80/130] Train loss: 3.3659 (3.3058) \n","accuracy: [82.06923076923077, 82.62307692307692, 82.58461538461539]\n","\n","== Train RUC ==\n","Epoch: [2][0/130] Train loss: 2.9455 (2.9455) \n","Epoch: [2][80/130] Train loss: 2.8485 (3.2366) \n","Epoch: [2][0/130] Train loss: 3.2989 (3.2989) \n","Epoch: [2][80/130] Train loss: 3.2798 (3.2464) \n","accuracy: [81.95384615384616, 82.13846153846154, 82.42307692307692]\n","\n","== Train RUC ==\n","Epoch: [3][0/130] Train loss: 3.0385 (3.0385) \n","Epoch: [3][80/130] Train loss: 2.9686 (3.2140) \n","Epoch: [3][0/130] Train loss: 3.4876 (3.4876) \n","Epoch: [3][80/130] Train loss: 3.3163 (3.1913) \n","accuracy: [82.0, 82.8076923076923, 82.67692307692307]\n","\n","== Train RUC ==\n","Epoch: [4][0/130] Train loss: 3.2864 (3.2864) \n","Epoch: [4][80/130] Train loss: 3.3375 (3.1879) \n","Epoch: [4][0/130] Train loss: 3.0308 (3.0308) \n","Epoch: [4][80/130] Train loss: 3.0689 (3.1747) \n","accuracy: [82.97692307692309, 82.86153846153846, 83.22307692307692]\n","\n","== Train RUC ==\n","Epoch: [5][0/130] Train loss: 3.2663 (3.2663) \n","Epoch: [5][80/130] Train loss: 3.2522 (3.1629) \n","Epoch: [5][0/130] Train loss: 3.2623 (3.2623) \n","Epoch: [5][80/130] Train loss: 3.2239 (3.1472) \n","accuracy: [82.1, 82.83846153846154, 82.78461538461539]\n","\n","== Train RUC ==\n","Epoch: [6][0/130] Train loss: 3.0340 (3.0340) \n","Epoch: [6][80/130] Train loss: 3.0291 (3.0994) \n","Epoch: [6][0/130] Train loss: 3.1611 (3.1611) \n","Epoch: [6][80/130] Train loss: 2.9567 (3.1408) \n","accuracy: [82.86923076923077, 83.34615384615385, 83.46153846153847]\n","\n","== Train RUC ==\n","Epoch: [7][0/130] Train loss: 3.2050 (3.2050) \n","Epoch: [7][80/130] Train loss: 3.0985 (3.1055) \n","Epoch: [7][0/130] Train loss: 3.1128 (3.1128) \n","Epoch: [7][80/130] Train loss: 3.3111 (3.1094) \n","accuracy: [83.35384615384615, 82.6, 83.33076923076923]\n","\n","== Train RUC ==\n","Epoch: [8][0/130] Train loss: 3.2246 (3.2246) \n","Epoch: [8][80/130] Train loss: 3.1471 (3.1043) \n","Epoch: [8][0/130] Train loss: 3.1238 (3.1238) \n","Epoch: [8][80/130] Train loss: 3.0999 (3.1059) \n","accuracy: [83.3076923076923, 83.54615384615384, 83.73076923076923]\n","\n","== Train RUC ==\n","Epoch: [9][0/130] Train loss: 2.8713 (2.8713) \n","Epoch: [9][80/130] Train loss: 3.1377 (3.0381) \n","Epoch: [9][0/130] Train loss: 2.9799 (2.9799) \n","Epoch: [9][80/130] Train loss: 2.7572 (3.0912) \n","accuracy: [83.82307692307693, 83.17692307692309, 83.86923076923077]\n","\n","== Train RUC ==\n","Epoch: [10][0/130] Train loss: 3.0479 (3.0479) \n","Epoch: [10][80/130] Train loss: 3.1432 (3.0684) \n","Epoch: [10][0/130] Train loss: 3.0278 (3.0278) \n","Epoch: [10][80/130] Train loss: 3.0180 (3.0646) \n","accuracy: [81.88461538461539, 83.3, 83.23846153846154]\n","\n","== Train RUC ==\n","Epoch: [11][0/130] Train loss: 3.2990 (3.2990) \n","Epoch: [11][80/130] Train loss: 3.0610 (3.1655) \n","Epoch: [11][0/130] Train loss: 2.8632 (2.8632) \n","Epoch: [11][80/130] Train loss: 3.1921 (3.0785) \n","accuracy: [82.97692307692309, 82.88461538461539, 83.34615384615385]\n","\n","== Train RUC ==\n","Epoch: [12][0/130] Train loss: 3.2440 (3.2440) \n","Epoch: [12][80/130] Train loss: 3.0607 (3.1043) \n","Epoch: [12][0/130] Train loss: 2.9941 (2.9941) \n","Epoch: [12][80/130] Train loss: 3.0999 (3.0467) \n","accuracy: [83.34615384615385, 83.78461538461538, 83.97692307692309]\n","\n","== Train RUC ==\n","Epoch: [13][0/130] Train loss: 3.0991 (3.0991) \n","Epoch: [13][80/130] Train loss: 3.0972 (3.0985) \n","Epoch: [13][0/130] Train loss: 2.7136 (2.7136) \n","Epoch: [13][80/130] Train loss: 3.0617 (3.0363) \n","accuracy: [83.76923076923077, 83.82307692307693, 84.32307692307693]\n","\n","== Train RUC ==\n","Epoch: [14][0/130] Train loss: 3.1937 (3.1937) \n","Epoch: [14][80/130] Train loss: 2.8591 (3.0712) \n","Epoch: [14][0/130] Train loss: 3.0337 (3.0337) \n","Epoch: [14][80/130] Train loss: 2.9991 (3.0015) \n","accuracy: [84.33846153846154, 84.18461538461538, 84.57692307692307]\n","\n","== Train RUC ==\n","Epoch: [15][0/130] Train loss: 3.0721 (3.0721) \n","Epoch: [15][80/130] Train loss: 3.0847 (3.0707) \n","Epoch: [15][0/130] Train loss: 2.8679 (2.8679) \n","Epoch: [15][80/130] Train loss: 3.1703 (3.0313) \n","accuracy: [84.01538461538462, 83.63076923076923, 84.29230769230769]\n","\n","== Train RUC ==\n","Epoch: [16][0/130] Train loss: 3.2942 (3.2942) \n","Epoch: [16][80/130] Train loss: 2.7959 (3.0342) \n","Epoch: [16][0/130] Train loss: 3.0036 (3.0036) \n","Epoch: [16][80/130] Train loss: 3.0246 (3.0408) \n","accuracy: [83.92307692307692, 83.77692307692308, 84.13076923076923]\n","\n","== Train RUC ==\n","Epoch: [17][0/130] Train loss: 2.8440 (2.8440) \n","Epoch: [17][80/130] Train loss: 2.9864 (3.0320) \n","Epoch: [17][0/130] Train loss: 3.1944 (3.1944) \n","Epoch: [17][80/130] Train loss: 3.1556 (3.0572) \n","accuracy: [83.79230769230769, 83.6076923076923, 84.31538461538462]\n","\n","== Train RUC ==\n","Epoch: [18][0/130] Train loss: 3.0713 (3.0713) \n","Epoch: [18][80/130] Train loss: 3.0456 (3.0512) \n","Epoch: [18][0/130] Train loss: 3.0862 (3.0862) \n","Epoch: [18][80/130] Train loss: 3.0270 (3.0318) \n","accuracy: [83.74615384615385, 83.98461538461538, 84.28461538461538]\n","\n","== Train RUC ==\n","Epoch: [19][0/130] Train loss: 2.9264 (2.9264) \n","Epoch: [19][80/130] Train loss: 3.0939 (2.9976) \n","Epoch: [19][0/130] Train loss: 2.9112 (2.9112) \n","Epoch: [19][80/130] Train loss: 2.9633 (3.0223) \n","accuracy: [84.23846153846154, 83.84615384615385, 84.4076923076923]\n","\n","== Train RUC ==\n","Epoch: [20][0/130] Train loss: 3.0886 (3.0886) \n","Epoch: [20][80/130] Train loss: 3.2058 (3.0373) \n","Epoch: [20][0/130] Train loss: 3.0099 (3.0099) \n","Epoch: [20][80/130] Train loss: 3.0970 (3.0180) \n","accuracy: [84.49230769230769, 84.63846153846154, 84.91538461538461]\n","\n","== Train RUC ==\n","Epoch: [21][0/130] Train loss: 3.0685 (3.0685) \n","Epoch: [21][80/130] Train loss: 3.0291 (3.0372) \n","Epoch: [21][0/130] Train loss: 3.1274 (3.1274) \n","Epoch: [21][80/130] Train loss: 3.0203 (3.0873) \n","accuracy: [83.63846153846154, 84.11538461538461, 84.56153846153846]\n","\n","== Train RUC ==\n","Epoch: [22][0/130] Train loss: 3.2596 (3.2596) \n","Epoch: [22][80/130] Train loss: 3.1126 (3.0608) \n","Epoch: [22][0/130] Train loss: 3.0485 (3.0485) \n","Epoch: [22][80/130] Train loss: 2.9099 (3.0297) \n","accuracy: [83.37692307692308, 83.63846153846154, 83.68461538461538]\n","\n","== Train RUC ==\n","Epoch: [23][0/130] Train loss: 3.0917 (3.0917) \n","Epoch: [23][80/130] Train loss: 3.0788 (3.0171) \n","Epoch: [23][0/130] Train loss: 2.9268 (2.9268) \n","Epoch: [23][80/130] Train loss: 2.9852 (3.0110) \n","accuracy: [83.86923076923077, 83.99230769230769, 84.37692307692308]\n","\n","== Train RUC ==\n","Epoch: [24][0/130] Train loss: 3.1134 (3.1134) \n","Epoch: [24][80/130] Train loss: 3.1249 (3.0506) \n","Epoch: [24][0/130] Train loss: 2.7806 (2.7806) \n","Epoch: [24][80/130] Train loss: 3.0425 (3.0379) \n","accuracy: [84.28461538461538, 83.9076923076923, 84.43846153846154]\n","\n","== Train RUC ==\n","Epoch: [25][0/130] Train loss: 3.0528 (3.0528) \n","Epoch: [25][80/130] Train loss: 3.0749 (3.0078) \n","Epoch: [25][0/130] Train loss: 2.9728 (2.9728) \n","Epoch: [25][80/130] Train loss: 2.9351 (3.0434) \n","accuracy: [84.33846153846154, 84.41538461538461, 84.68461538461538]\n","\n","== Train RUC ==\n","Epoch: [26][0/130] Train loss: 2.9256 (2.9256) \n","Epoch: [26][80/130] Train loss: 3.2238 (3.0431) \n","Epoch: [26][0/130] Train loss: 3.1020 (3.1020) \n","Epoch: [26][80/130] Train loss: 2.9832 (3.0073) \n","accuracy: [84.39230769230768, 84.6, 84.71538461538461]\n","\n","== Train RUC ==\n","Epoch: [27][0/130] Train loss: 3.0011 (3.0011) \n","Epoch: [27][80/130] Train loss: 3.0291 (3.0573) \n","Epoch: [27][0/130] Train loss: 2.6943 (2.6943) \n","Epoch: [27][80/130] Train loss: 2.8428 (3.0133) \n","accuracy: [84.6, 83.89999999999999, 84.68461538461538]\n","\n","== Train RUC ==\n","Epoch: [28][0/130] Train loss: 3.1741 (3.1741) \n","Epoch: [28][80/130] Train loss: 3.0376 (3.0479) \n","Epoch: [28][0/130] Train loss: 3.0652 (3.0652) \n","Epoch: [28][80/130] Train loss: 3.0240 (3.0488) \n","accuracy: [84.29230769230769, 83.87692307692308, 84.36923076923077]\n","\n","== Train RUC ==\n","Epoch: [29][0/130] Train loss: 2.9980 (2.9980) \n","Epoch: [29][80/130] Train loss: 3.1250 (3.0465) \n","Epoch: [29][0/130] Train loss: 3.0028 (3.0028) \n","Epoch: [29][80/130] Train loss: 3.0481 (3.0127) \n","accuracy: [84.58461538461538, 84.67692307692307, 84.85384615384616]\n","\n","== Train RUC ==\n","Epoch: [30][0/130] Train loss: 3.1825 (3.1825) \n","Epoch: [30][80/130] Train loss: 2.6433 (3.0467) \n","Epoch: [30][0/130] Train loss: 3.0257 (3.0257) \n","Epoch: [30][80/130] Train loss: 2.9820 (3.0242) \n","accuracy: [84.53076923076924, 84.66923076923078, 84.92307692307692]\n","\n","== Train RUC ==\n","Epoch: [31][0/130] Train loss: 3.0621 (3.0621) \n","Epoch: [31][80/130] Train loss: 3.2152 (3.0391) \n","Epoch: [31][0/130] Train loss: 2.9750 (2.9750) \n","Epoch: [31][80/130] Train loss: 3.0339 (3.0547) \n","accuracy: [84.78461538461538, 84.56153846153846, 85.1]\n","\n","== Train RUC ==\n","Epoch: [32][0/130] Train loss: 2.7980 (2.7980) \n","Epoch: [32][80/130] Train loss: 3.0760 (3.0428) \n","Epoch: [32][0/130] Train loss: 3.0412 (3.0412) \n","Epoch: [32][80/130] Train loss: 3.2202 (3.0231) \n","accuracy: [84.53846153846155, 84.73076923076923, 84.9076923076923]\n","\n","== Train RUC ==\n","Epoch: [33][0/130] Train loss: 3.0487 (3.0487) \n","Epoch: [33][80/130] Train loss: 3.0091 (3.0614) \n","Epoch: [33][0/130] Train loss: 3.0614 (3.0614) \n","Epoch: [33][80/130] Train loss: 3.0878 (3.0461) \n","accuracy: [85.50769230769231, 85.07692307692307, 85.46153846153847]\n","\n","== Train RUC ==\n","Epoch: [34][0/130] Train loss: 3.0386 (3.0386) \n","Epoch: [34][80/130] Train loss: 2.9667 (3.0187) \n","Epoch: [34][0/130] Train loss: 2.5616 (2.5616) \n","Epoch: [34][80/130] Train loss: 2.7841 (3.0065) \n","accuracy: [85.05384615384615, 84.78461538461538, 85.23846153846154]\n","\n","== Train RUC ==\n","Epoch: [35][0/130] Train loss: 3.0747 (3.0747) \n","Epoch: [35][80/130] Train loss: 2.9922 (3.0627) \n","Epoch: [35][0/130] Train loss: 3.0765 (3.0765) \n","Epoch: [35][80/130] Train loss: 2.8886 (3.0251) \n","accuracy: [84.81538461538462, 84.76923076923077, 85.02307692307693]\n","\n","== Train RUC ==\n","Epoch: [36][0/130] Train loss: 3.0378 (3.0378) \n","Epoch: [36][80/130] Train loss: 3.0884 (3.0179) \n","Epoch: [36][0/130] Train loss: 3.1248 (3.1248) \n","Epoch: [36][80/130] Train loss: 3.0775 (3.0393) \n","accuracy: [85.52307692307693, 84.86153846153847, 85.46153846153847]\n","\n","== Train RUC ==\n","Epoch: [37][0/130] Train loss: 2.8805 (2.8805) \n","Epoch: [37][80/130] Train loss: 2.9828 (3.0431) \n","Epoch: [37][0/130] Train loss: 3.2546 (3.2546) \n","Epoch: [37][80/130] Train loss: 2.9598 (3.0848) \n","accuracy: [84.72307692307692, 84.8923076923077, 84.86923076923077]\n","\n","== Train RUC ==\n","Epoch: [38][0/130] Train loss: 3.0964 (3.0964) \n","Epoch: [38][80/130] Train loss: 3.0918 (3.0745) \n","Epoch: [38][0/130] Train loss: 3.2507 (3.2507) \n","Epoch: [38][80/130] Train loss: 2.8743 (3.0399) \n","accuracy: [84.96923076923078, 85.02307692307693, 85.21538461538462]\n","\n","== Train RUC ==\n","Epoch: [39][0/130] Train loss: 3.1002 (3.1002) \n","Epoch: [39][80/130] Train loss: 2.7051 (3.0335) \n","Epoch: [39][0/130] Train loss: 2.8508 (2.8508) \n","Epoch: [39][80/130] Train loss: 2.9817 (3.0249) \n","accuracy: [84.33076923076923, 84.95384615384616, 85.11538461538461]\n","\n","== Train RUC ==\n","Epoch: [40][0/130] Train loss: 3.1001 (3.1001) \n","Epoch: [40][80/130] Train loss: 2.9159 (3.0600) \n","Epoch: [40][0/130] Train loss: 3.1066 (3.1066) \n","Epoch: [40][80/130] Train loss: 3.1439 (3.0614) \n","accuracy: [85.33846153846154, 85.07692307692307, 85.4076923076923]\n","\n","== Train RUC ==\n","Epoch: [41][0/130] Train loss: 3.0885 (3.0885) \n","Epoch: [41][80/130] Train loss: 3.1330 (3.0441) \n","Epoch: [41][0/130] Train loss: 3.1586 (3.1586) \n","Epoch: [41][80/130] Train loss: 3.0430 (3.0552) \n","accuracy: [85.11538461538461, 84.53076923076924, 85.16153846153847]\n","\n","== Train RUC ==\n","Epoch: [42][0/130] Train loss: 3.0987 (3.0987) \n","Epoch: [42][80/130] Train loss: 3.0560 (3.0200) \n","Epoch: [42][0/130] Train loss: 3.1161 (3.1161) \n","Epoch: [42][80/130] Train loss: 3.0443 (3.0568) \n","accuracy: [84.85384615384616, 84.93076923076923, 85.18461538461538]\n","\n","== Train RUC ==\n","Epoch: [43][0/130] Train loss: 3.1499 (3.1499) \n","Epoch: [43][80/130] Train loss: 3.0543 (3.0579) \n","Epoch: [43][0/130] Train loss: 2.7776 (2.7776) \n","Epoch: [43][80/130] Train loss: 3.1595 (3.0246) \n","accuracy: [85.29230769230769, 85.43076923076923, 85.64615384615385]\n","\n","== Train RUC ==\n","Epoch: [44][0/130] Train loss: 3.1358 (3.1358) \n","Epoch: [44][80/130] Train loss: 2.8725 (3.0284) \n","Epoch: [44][0/130] Train loss: 3.0891 (3.0891) \n","Epoch: [44][80/130] Train loss: 3.0544 (3.0468) \n","accuracy: [85.53076923076924, 85.27692307692307, 85.81538461538462]\n","\n","== Train RUC ==\n","Epoch: [45][0/130] Train loss: 2.8189 (2.8189) \n","Epoch: [45][80/130] Train loss: 3.1594 (3.0472) \n","Epoch: [45][0/130] Train loss: 2.5659 (2.5659) \n","Epoch: [45][80/130] Train loss: 3.1066 (3.0622) \n","accuracy: [85.73846153846154, 85.33076923076923, 85.8]\n","\n","== Train RUC ==\n","Epoch: [46][0/130] Train loss: 3.0581 (3.0581) \n","Epoch: [46][80/130] Train loss: 3.0748 (3.0608) \n","Epoch: [46][0/130] Train loss: 3.0558 (3.0558) \n","Epoch: [46][80/130] Train loss: 3.2464 (3.0507) \n","accuracy: [85.35384615384616, 85.12307692307692, 85.63846153846154]\n","\n","== Train RUC ==\n","Epoch: [47][0/130] Train loss: 3.2042 (3.2042) \n","Epoch: [47][80/130] Train loss: 3.0867 (3.0912) \n","Epoch: [47][0/130] Train loss: 3.0681 (3.0681) \n","Epoch: [47][80/130] Train loss: 3.1181 (3.0593) \n","accuracy: [85.86153846153847, 85.59230769230768, 85.96923076923076]\n","\n","== Train RUC ==\n","Epoch: [48][0/130] Train loss: 2.9445 (2.9445) \n","Epoch: [48][80/130] Train loss: 2.9760 (3.0591) \n","Epoch: [48][0/130] Train loss: 3.0955 (3.0955) \n","Epoch: [48][80/130] Train loss: 2.8184 (3.0638) \n","accuracy: [85.26153846153846, 85.33846153846154, 85.83846153846154]\n","\n","== Train RUC ==\n","Epoch: [49][0/130] Train loss: 3.1838 (3.1838) \n","Epoch: [49][80/130] Train loss: 2.9182 (3.0871) \n","Epoch: [49][0/130] Train loss: 3.2415 (3.2415) \n","Epoch: [49][80/130] Train loss: 3.0825 (3.0851) \n","accuracy: [85.34615384615385, 85.86923076923077, 85.84615384615385]\n","\n","== Train RUC ==\n","Epoch: [50][0/130] Train loss: 3.1014 (3.1014) \n","Epoch: [50][80/130] Train loss: 3.1596 (3.0723) \n","Epoch: [50][0/130] Train loss: 3.0911 (3.0911) \n","Epoch: [50][80/130] Train loss: 2.9375 (3.0446) \n","accuracy: [86.02307692307693, 85.53076923076924, 85.92307692307692]\n","\n","== Train RUC ==\n","Epoch: [51][0/130] Train loss: 3.2014 (3.2014) \n","Epoch: [51][80/130] Train loss: 3.1594 (3.0711) \n","Epoch: [51][0/130] Train loss: 3.0722 (3.0722) \n","Epoch: [51][80/130] Train loss: 3.2448 (3.0907) \n","accuracy: [85.82307692307693, 85.77692307692307, 85.98461538461538]\n","\n","== Train RUC ==\n","Epoch: [52][0/130] Train loss: 3.0658 (3.0658) \n","Epoch: [52][80/130] Train loss: 3.1338 (3.0557) \n","Epoch: [52][0/130] Train loss: 3.0518 (3.0518) \n","Epoch: [52][80/130] Train loss: 2.7858 (3.0452) \n","accuracy: [85.72307692307692, 85.49230769230769, 85.8076923076923]\n","\n","== Train RUC ==\n","Epoch: [53][0/130] Train loss: 2.9684 (2.9684) \n","Epoch: [53][80/130] Train loss: 3.0299 (3.0721) \n","Epoch: [53][0/130] Train loss: 3.1246 (3.1246) \n","Epoch: [53][80/130] Train loss: 3.1545 (3.1046) \n","accuracy: [85.45384615384616, 85.53076923076924, 85.83076923076923]\n","\n","== Train RUC ==\n","Epoch: [54][0/130] Train loss: 2.7575 (2.7575) \n","Epoch: [54][80/130] Train loss: 2.8959 (3.0593) \n","Epoch: [54][0/130] Train loss: 3.0988 (3.0988) \n","Epoch: [54][80/130] Train loss: 3.0400 (3.0695) \n","accuracy: [85.65384615384616, 85.37692307692308, 85.8076923076923]\n","\n","== Train RUC ==\n","Epoch: [55][0/130] Train loss: 3.1683 (3.1683) \n","Epoch: [55][80/130] Train loss: 3.0773 (3.0619) \n","Epoch: [55][0/130] Train loss: 3.0953 (3.0953) \n","Epoch: [55][80/130] Train loss: 3.0511 (3.0934) \n","accuracy: [85.17692307692307, 85.5, 85.65384615384616]\n","\n","== Train RUC ==\n","Epoch: [56][0/130] Train loss: 3.0675 (3.0675) \n","Epoch: [56][80/130] Train loss: 3.0089 (3.0557) \n","Epoch: [56][0/130] Train loss: 3.1274 (3.1274) \n","Epoch: [56][80/130] Train loss: 2.9311 (3.1187) \n","accuracy: [85.36923076923077, 85.8076923076923, 86.05384615384615]\n","\n","== Train RUC ==\n","Epoch: [57][0/130] Train loss: 3.1214 (3.1214) \n","Epoch: [57][80/130] Train loss: 2.8657 (3.0741) \n","Epoch: [57][0/130] Train loss: 3.1527 (3.1527) \n","Epoch: [57][80/130] Train loss: 3.1090 (3.0821) \n","accuracy: [85.87692307692308, 85.93846153846154, 86.15384615384616]\n","\n","== Train RUC ==\n","Epoch: [58][0/130] Train loss: 3.0564 (3.0564) \n","Epoch: [58][80/130] Train loss: 3.0973 (3.0633) \n","Epoch: [58][0/130] Train loss: 3.0831 (3.0831) \n","Epoch: [58][80/130] Train loss: 3.2216 (3.0754) \n","accuracy: [85.67692307692307, 85.41538461538461, 85.8]\n","\n","== Train RUC ==\n","Epoch: [59][0/130] Train loss: 3.0678 (3.0678) \n","Epoch: [59][80/130] Train loss: 3.1014 (3.1193) \n","Epoch: [59][0/130] Train loss: 3.0194 (3.0194) \n","Epoch: [59][80/130] Train loss: 3.1213 (3.0624) \n","accuracy: [85.56153846153846, 85.56153846153846, 85.9076923076923]\n","\n","== Train RUC ==\n","Epoch: [60][0/130] Train loss: 3.1573 (3.1573) \n","Epoch: [60][80/130] Train loss: 3.1753 (3.0893) \n","Epoch: [60][0/130] Train loss: 3.2275 (3.2275) \n","Epoch: [60][80/130] Train loss: 2.6111 (3.0809) \n","accuracy: [85.86923076923077, 85.38461538461539, 85.94615384615385]\n","\n","== Train RUC ==\n","Epoch: [61][0/130] Train loss: 3.2508 (3.2508) \n","Epoch: [61][80/130] Train loss: 2.8657 (3.1161) \n","Epoch: [61][0/130] Train loss: 3.0859 (3.0859) \n","Epoch: [61][80/130] Train loss: 3.1749 (3.0914) \n","accuracy: [85.62307692307692, 85.81538461538462, 85.98461538461538]\n","\n","== Train RUC ==\n","Epoch: [62][0/130] Train loss: 3.0242 (3.0242) \n","Epoch: [62][80/130] Train loss: 2.9862 (3.1074) \n","Epoch: [62][0/130] Train loss: 2.7676 (2.7676) \n","Epoch: [62][80/130] Train loss: 2.9274 (3.0842) \n","accuracy: [85.84615384615385, 86.08461538461538, 86.3]\n","\n","== Train RUC ==\n","Epoch: [63][0/130] Train loss: 3.1470 (3.1470) \n","Epoch: [63][80/130] Train loss: 2.9209 (3.0879) \n","Epoch: [63][0/130] Train loss: 3.1434 (3.1434) \n","Epoch: [63][80/130] Train loss: 3.2119 (3.0836) \n","accuracy: [85.52307692307693, 85.53846153846155, 85.82307692307693]\n","\n","== Train RUC ==\n","Epoch: [64][0/130] Train loss: 3.2508 (3.2508) \n","Epoch: [64][80/130] Train loss: 2.6840 (3.1125) \n","Epoch: [64][0/130] Train loss: 2.9944 (2.9944) \n","Epoch: [64][80/130] Train loss: 3.2005 (3.1105) \n","accuracy: [86.3923076923077, 85.84615384615385, 86.25384615384615]\n","\n","== Train RUC ==\n","Epoch: [65][0/130] Train loss: 3.2062 (3.2062) \n","Epoch: [65][80/130] Train loss: 3.2392 (3.0838) \n","Epoch: [65][0/130] Train loss: 3.1642 (3.1642) \n","Epoch: [65][80/130] Train loss: 3.2172 (3.0876) \n","accuracy: [85.62307692307692, 85.64615384615385, 85.97692307692307]\n","\n","== Train RUC ==\n","Epoch: [66][0/130] Train loss: 3.1620 (3.1620) \n","Epoch: [66][80/130] Train loss: 3.0580 (3.0991) \n","Epoch: [66][0/130] Train loss: 3.0215 (3.0215) \n","Epoch: [66][80/130] Train loss: 2.9957 (3.1145) \n","accuracy: [85.8923076923077, 86.03076923076924, 86.03076923076924]\n","\n","== Train RUC ==\n","Epoch: [67][0/130] Train loss: 3.1018 (3.1018) \n","Epoch: [67][80/130] Train loss: 3.1790 (3.0904) \n","Epoch: [67][0/130] Train loss: 3.1295 (3.1295) \n","Epoch: [67][80/130] Train loss: 3.1324 (3.1021) \n","accuracy: [85.56153846153846, 85.59230769230768, 85.94615384615385]\n","\n","== Train RUC ==\n","Epoch: [68][0/130] Train loss: 3.1327 (3.1327) \n","Epoch: [68][80/130] Train loss: 3.1289 (3.0983) \n","Epoch: [68][0/130] Train loss: 3.1531 (3.1531) \n","Epoch: [68][80/130] Train loss: 3.0368 (3.1323) \n","accuracy: [85.44615384615385, 85.84615384615385, 85.81538461538462]\n","\n","== Train RUC ==\n","Epoch: [69][0/130] Train loss: 3.0185 (3.0185) \n","Epoch: [69][80/130] Train loss: 3.1869 (3.1153) \n","Epoch: [69][0/130] Train loss: 3.2570 (3.2570) \n","Epoch: [69][80/130] Train loss: 3.1907 (3.1157) \n","accuracy: [86.21538461538462, 85.98461538461538, 86.23846153846154]\n","\n","== Train RUC ==\n","Epoch: [70][0/130] Train loss: 3.1105 (3.1105) \n","Epoch: [70][80/130] Train loss: 3.0750 (3.1398) \n","Epoch: [70][0/130] Train loss: 3.0588 (3.0588) \n","Epoch: [70][80/130] Train loss: 2.6293 (3.0717) \n","accuracy: [85.72307692307692, 85.43846153846154, 85.76153846153846]\n","\n","== Train RUC ==\n","Epoch: [71][0/130] Train loss: 3.2336 (3.2336) \n","Epoch: [71][80/130] Train loss: 3.3277 (3.1567) \n","Epoch: [71][0/130] Train loss: 3.1483 (3.1483) \n","Epoch: [71][80/130] Train loss: 3.2567 (3.1312) \n","accuracy: [85.59230769230768, 86.03076923076924, 86.05384615384615]\n","\n","== Train RUC ==\n","Epoch: [72][0/130] Train loss: 2.9691 (2.9691) \n","Epoch: [72][80/130] Train loss: 3.2423 (3.1502) \n","Epoch: [72][0/130] Train loss: 3.1504 (3.1504) \n","Epoch: [72][80/130] Train loss: 3.4058 (3.1414) \n","accuracy: [85.75384615384615, 85.42307692307692, 85.84615384615385]\n","\n","== Train RUC ==\n","Epoch: [73][0/130] Train loss: 3.1931 (3.1931) \n","Epoch: [73][80/130] Train loss: 3.1229 (3.1944) \n","Epoch: [73][0/130] Train loss: 3.0518 (3.0518) \n","Epoch: [73][80/130] Train loss: 3.0301 (3.1231) \n","accuracy: [85.94615384615385, 85.75384615384615, 86.24615384615385]\n","\n","== Train RUC ==\n","Epoch: [74][0/130] Train loss: 3.1990 (3.1990) \n","Epoch: [74][80/130] Train loss: 3.0781 (3.1460) \n","Epoch: [74][0/130] Train loss: 3.2494 (3.2494) \n","Epoch: [74][80/130] Train loss: 3.1242 (3.1380) \n","accuracy: [85.91538461538461, 85.70769230769231, 86.1]\n","\n","== Train RUC ==\n","Epoch: [75][0/130] Train loss: 2.8782 (2.8782) \n","Epoch: [75][80/130] Train loss: 3.2296 (3.1340) \n","Epoch: [75][0/130] Train loss: 3.2292 (3.2292) \n","Epoch: [75][80/130] Train loss: 2.8624 (3.1378) \n","accuracy: [85.77692307692307, 85.9076923076923, 86.17692307692307]\n","\n","== Train RUC ==\n","Epoch: [76][0/130] Train loss: 3.2783 (3.2783) \n","Epoch: [76][80/130] Train loss: 3.1849 (3.1525) \n","Epoch: [76][0/130] Train loss: 3.2115 (3.2115) \n","Epoch: [76][80/130] Train loss: 3.2448 (3.1656) \n","accuracy: [86.07692307692307, 85.6076923076923, 86.12307692307692]\n","\n","== Train RUC ==\n","Epoch: [77][0/130] Train loss: 3.1916 (3.1916) \n","Epoch: [77][80/130] Train loss: 3.2872 (3.1626) \n","Epoch: [77][0/130] Train loss: 3.1763 (3.1763) \n","Epoch: [77][80/130] Train loss: 3.1658 (3.1481) \n","accuracy: [85.86153846153847, 85.79230769230769, 86.07692307692307]\n","\n","== Train RUC ==\n","Epoch: [78][0/130] Train loss: 3.2239 (3.2239) \n","Epoch: [78][80/130] Train loss: 3.1235 (3.1123) \n","Epoch: [78][0/130] Train loss: 3.2552 (3.2552) \n","Epoch: [78][80/130] Train loss: 3.3014 (3.1734) \n","accuracy: [85.92307692307692, 85.88461538461539, 86.2]\n","\n","== Train RUC ==\n","Epoch: [79][0/130] Train loss: 3.1323 (3.1323) \n","Epoch: [79][80/130] Train loss: 3.2008 (3.1528) \n","Epoch: [79][0/130] Train loss: 3.0835 (3.0835) \n","Epoch: [79][80/130] Train loss: 3.2719 (3.1417) \n","accuracy: [85.81538461538462, 85.78461538461538, 86.13846153846154]\n","\n","== Train RUC ==\n","Epoch: [80][0/130] Train loss: 2.8238 (2.8238) \n","Epoch: [80][80/130] Train loss: 3.2046 (3.1546) \n","Epoch: [80][0/130] Train loss: 3.0646 (3.0646) \n","Epoch: [80][80/130] Train loss: 3.3226 (3.1607) \n","accuracy: [85.95384615384616, 85.95384615384616, 86.09230769230768]\n","\n","== Train RUC ==\n","Epoch: [81][0/130] Train loss: 3.2381 (3.2381) \n","Epoch: [81][80/130] Train loss: 3.1279 (3.1678) \n","Epoch: [81][0/130] Train loss: 3.1785 (3.1785) \n","Epoch: [81][80/130] Train loss: 3.2461 (3.1645) \n","accuracy: [85.96923076923076, 85.87692307692308, 86.16153846153846]\n","\n","== Train RUC ==\n","Epoch: [82][0/130] Train loss: 3.2498 (3.2498) \n","Epoch: [82][80/130] Train loss: 3.2021 (3.1411) \n","Epoch: [82][0/130] Train loss: 2.5702 (2.5702) \n","Epoch: [82][80/130] Train loss: 3.1790 (3.1352) \n","accuracy: [86.07692307692307, 85.65384615384616, 86.13846153846154]\n","\n","== Train RUC ==\n","Epoch: [83][0/130] Train loss: 3.2850 (3.2850) \n","Epoch: [83][80/130] Train loss: 3.1802 (3.1514) \n","Epoch: [83][0/130] Train loss: 3.1644 (3.1644) \n","Epoch: [83][80/130] Train loss: 3.4138 (3.1691) \n","accuracy: [86.1076923076923, 85.57692307692307, 86.16153846153846]\n","\n","== Train RUC ==\n","Epoch: [84][0/130] Train loss: 3.2409 (3.2409) \n","Epoch: [84][80/130] Train loss: 3.0516 (3.1518) \n","Epoch: [84][0/130] Train loss: 3.3509 (3.3509) \n","Epoch: [84][80/130] Train loss: 3.1515 (3.1868) \n","accuracy: [86.17692307692307, 85.64615384615385, 86.22307692307693]\n","\n","== Train RUC ==\n","Epoch: [85][0/130] Train loss: 3.2763 (3.2763) \n","Epoch: [85][80/130] Train loss: 3.2222 (3.1639) \n","Epoch: [85][0/130] Train loss: 3.2045 (3.2045) \n","Epoch: [85][80/130] Train loss: 2.8095 (3.1520) \n","accuracy: [86.23076923076923, 85.94615384615385, 86.3]\n","\n","== Train RUC ==\n","Epoch: [86][0/130] Train loss: 2.9210 (2.9210) \n","Epoch: [86][80/130] Train loss: 3.1497 (3.1509) \n","Epoch: [86][0/130] Train loss: 3.2762 (3.2762) \n","Epoch: [86][80/130] Train loss: 3.2228 (3.1855) \n","accuracy: [86.18461538461538, 85.87692307692308, 86.28461538461538]\n","\n","== Train RUC ==\n","Epoch: [87][0/130] Train loss: 3.3291 (3.3291) \n","Epoch: [87][80/130] Train loss: 3.2678 (3.1881) \n","Epoch: [87][0/130] Train loss: 3.0539 (3.0539) \n","Epoch: [87][80/130] Train loss: 3.2828 (3.1807) \n","accuracy: [86.13846153846154, 85.83846153846154, 86.1]\n","\n","== Train RUC ==\n","Epoch: [88][0/130] Train loss: 3.3653 (3.3653) \n","Epoch: [88][80/130] Train loss: 3.1603 (3.1972) \n","Epoch: [88][0/130] Train loss: 3.1352 (3.1352) \n","Epoch: [88][80/130] Train loss: 3.1748 (3.1598) \n","accuracy: [85.99230769230769, 86.03846153846155, 86.23846153846154]\n","\n","== Train RUC ==\n","Epoch: [89][0/130] Train loss: 3.3966 (3.3966) \n","Epoch: [89][80/130] Train loss: 3.2164 (3.1840) \n","Epoch: [89][0/130] Train loss: 3.1113 (3.1113) \n","Epoch: [89][80/130] Train loss: 3.1948 (3.1972) \n","accuracy: [86.29230769230769, 86.22307692307693, 86.37692307692308]\n","\n","== Train RUC ==\n","Epoch: [90][0/130] Train loss: 3.1559 (3.1559) \n","Epoch: [90][80/130] Train loss: 3.2148 (3.1996) \n","Epoch: [90][0/130] Train loss: 3.1458 (3.1458) \n","Epoch: [90][80/130] Train loss: 3.1798 (3.1566) \n","accuracy: [86.45384615384616, 86.16153846153846, 86.38461538461539]\n","\n","== Train RUC ==\n","Epoch: [91][0/130] Train loss: 3.2879 (3.2879) \n","Epoch: [91][80/130] Train loss: 3.0434 (3.2100) \n","Epoch: [91][0/130] Train loss: 3.0018 (3.0018) \n","Epoch: [91][80/130] Train loss: 3.2942 (3.1806) \n","accuracy: [86.20769230769231, 86.16923076923077, 86.4076923076923]\n","\n","== Train RUC ==\n","Epoch: [92][0/130] Train loss: 3.3139 (3.3139) \n","Epoch: [92][80/130] Train loss: 3.1700 (3.2024) \n","Epoch: [92][0/130] Train loss: 3.1721 (3.1721) \n","Epoch: [92][80/130] Train loss: 3.1658 (3.1856) \n","accuracy: [86.03846153846155, 86.11538461538461, 86.13846153846154]\n","\n","== Train RUC ==\n","Epoch: [93][0/130] Train loss: 3.0896 (3.0896) \n","Epoch: [93][80/130] Train loss: 3.3709 (3.1679) \n","Epoch: [93][0/130] Train loss: 3.2031 (3.2031) \n","Epoch: [93][80/130] Train loss: 3.2401 (3.2036) \n","accuracy: [85.85384615384616, 86.1076923076923, 86.24615384615385]\n","\n","== Train RUC ==\n","Epoch: [94][0/130] Train loss: 3.2248 (3.2248) \n","Epoch: [94][80/130] Train loss: 3.3183 (3.2018) \n","Epoch: [94][0/130] Train loss: 3.4133 (3.4133) \n","Epoch: [94][80/130] Train loss: 2.7806 (3.1501) \n","accuracy: [86.23846153846154, 86.37692307692308, 86.3923076923077]\n","\n","== Train RUC ==\n","Epoch: [95][0/130] Train loss: 3.1796 (3.1796) \n","Epoch: [95][80/130] Train loss: 3.1662 (3.1873) \n","Epoch: [95][0/130] Train loss: 3.2336 (3.2336) \n","Epoch: [95][80/130] Train loss: 3.3673 (3.1699) \n","accuracy: [86.41538461538461, 86.06923076923077, 86.44615384615385]\n","\n","== Train RUC ==\n","Epoch: [96][0/130] Train loss: 3.2902 (3.2902) \n","Epoch: [96][80/130] Train loss: 3.3208 (3.1824) \n","Epoch: [96][0/130] Train loss: 2.9709 (2.9709) \n","Epoch: [96][80/130] Train loss: 3.3252 (3.2186) \n","accuracy: [86.26153846153845, 86.28461538461538, 86.33076923076923]\n","\n","== Train RUC ==\n","Epoch: [97][0/130] Train loss: 3.1919 (3.1919) \n","Epoch: [97][80/130] Train loss: 3.3070 (3.1555) \n","Epoch: [97][0/130] Train loss: 3.0963 (3.0963) \n","Epoch: [97][80/130] Train loss: 3.1471 (3.1835) \n","accuracy: [86.08461538461538, 86.32307692307693, 86.31538461538462]\n","\n","== Train RUC ==\n","Epoch: [98][0/130] Train loss: 3.2443 (3.2443) \n","Epoch: [98][80/130] Train loss: 3.4706 (3.2029) \n","Epoch: [98][0/130] Train loss: 3.1007 (3.1007) \n","Epoch: [98][80/130] Train loss: 3.3292 (3.2101) \n","accuracy: [86.24615384615385, 86.1923076923077, 86.33076923076923]\n","\n","== Train RUC ==\n","Epoch: [99][0/130] Train loss: 3.2928 (3.2928) \n","Epoch: [99][80/130] Train loss: 3.3172 (3.1928) \n","Epoch: [99][0/130] Train loss: 3.0557 (3.0557) \n","Epoch: [99][80/130] Train loss: 3.2243 (3.3034) \n","accuracy: [86.20769230769231, 86.51538461538462, 86.53846153846155]\n","\n","== Train RUC ==\n","Epoch: [100][0/130] Train loss: 2.9353 (2.9353) \n","Epoch: [100][80/130] Train loss: 3.1040 (3.2073) \n","Epoch: [100][0/130] Train loss: 3.2613 (3.2613) \n","Epoch: [100][80/130] Train loss: 3.4368 (3.2127) \n","accuracy: [86.27692307692307, 86.1923076923077, 86.43846153846154]\n","\n","== Train RUC ==\n","Epoch: [101][0/130] Train loss: 3.3506 (3.3506) \n","Epoch: [101][80/130] Train loss: 3.1875 (3.1747) \n","Epoch: [101][0/130] Train loss: 3.2817 (3.2817) \n","Epoch: [101][80/130] Train loss: 3.1311 (3.1880) \n","accuracy: [85.83076923076923, 86.42307692307692, 86.3]\n","\n","== Train RUC ==\n","Epoch: [102][0/130] Train loss: 2.7207 (2.7207) \n","Epoch: [102][80/130] Train loss: 3.2564 (3.1886) \n","Epoch: [102][0/130] Train loss: 3.1465 (3.1465) \n","Epoch: [102][80/130] Train loss: 3.3173 (3.2112) \n","accuracy: [86.6, 86.22307692307693, 86.6076923076923]\n","\n","== Train RUC ==\n","Epoch: [103][0/130] Train loss: 3.2209 (3.2209) \n","Epoch: [103][80/130] Train loss: 3.2599 (3.1956) \n","Epoch: [103][0/130] Train loss: 3.3341 (3.3341) \n","Epoch: [103][80/130] Train loss: 3.1897 (3.1976) \n","accuracy: [86.16153846153846, 86.42307692307692, 86.36153846153846]\n","\n","== Train RUC ==\n","Epoch: [104][0/130] Train loss: 3.1991 (3.1991) \n","Epoch: [104][80/130] Train loss: 3.4214 (3.1678) \n","Epoch: [104][0/130] Train loss: 3.3116 (3.3116) \n","Epoch: [104][80/130] Train loss: 3.1694 (3.1864) \n","accuracy: [86.54615384615386, 86.32307692307693, 86.45384615384616]\n","\n","== Train RUC ==\n","Epoch: [105][0/130] Train loss: 3.3529 (3.3529) \n","Epoch: [105][80/130] Train loss: 3.3224 (3.2024) \n","Epoch: [105][0/130] Train loss: 3.1418 (3.1418) \n","Epoch: [105][80/130] Train loss: 3.3130 (3.2136) \n","accuracy: [86.06923076923077, 86.35384615384616, 86.37692307692308]\n","\n","== Train RUC ==\n","Epoch: [106][0/130] Train loss: 3.3206 (3.3206) \n","Epoch: [106][80/130] Train loss: 3.2900 (3.1866) \n","Epoch: [106][0/130] Train loss: 3.3388 (3.3388) \n","Epoch: [106][80/130] Train loss: 2.9158 (3.1915) \n","accuracy: [86.26923076923076, 85.99230769230769, 86.28461538461538]\n","\n","== Train RUC ==\n","Epoch: [107][0/130] Train loss: 3.0874 (3.0874) \n","Epoch: [107][80/130] Train loss: 3.2288 (3.2152) \n","Epoch: [107][0/130] Train loss: 3.3311 (3.3311) \n","Epoch: [107][80/130] Train loss: 3.3178 (3.2637) \n","accuracy: [86.06923076923077, 86.16923076923077, 86.26153846153845]\n","\n","== Train RUC ==\n","Epoch: [108][0/130] Train loss: 3.2224 (3.2224) \n","Epoch: [108][80/130] Train loss: 3.1358 (3.2243) \n","Epoch: [108][0/130] Train loss: 3.1727 (3.1727) \n","Epoch: [108][80/130] Train loss: 2.9728 (3.2030) \n","accuracy: [86.37692307692308, 86.04615384615386, 86.32307692307693]\n","\n","== Train RUC ==\n","Epoch: [109][0/130] Train loss: 2.9573 (2.9573) \n","Epoch: [109][80/130] Train loss: 3.4692 (3.2125) \n","Epoch: [109][0/130] Train loss: 3.3037 (3.3037) \n","Epoch: [109][80/130] Train loss: 3.0820 (3.2132) \n","accuracy: [86.3, 86.03076923076924, 86.34615384615385]\n","\n","== Train RUC ==\n","Epoch: [110][0/130] Train loss: 3.3143 (3.3143) \n","Epoch: [110][80/130] Train loss: 2.8711 (3.1907) \n","Epoch: [110][0/130] Train loss: 3.3029 (3.3029) \n","Epoch: [110][80/130] Train loss: 3.3002 (3.2371) \n","accuracy: [86.06153846153846, 86.18461538461538, 86.21538461538462]\n","\n","== Train RUC ==\n","Epoch: [111][0/130] Train loss: 3.2445 (3.2445) \n","Epoch: [111][80/130] Train loss: 3.4973 (3.2142) \n","Epoch: [111][0/130] Train loss: 3.1955 (3.1955) \n","Epoch: [111][80/130] Train loss: 3.3017 (3.2346) \n","accuracy: [86.46923076923076, 86.22307692307693, 86.37692307692308]\n","\n","== Train RUC ==\n","Epoch: [112][0/130] Train loss: 3.3030 (3.3030) \n","Epoch: [112][80/130] Train loss: 3.3855 (3.2067) \n","Epoch: [112][0/130] Train loss: 3.2984 (3.2984) \n","Epoch: [112][80/130] Train loss: 3.0546 (3.2148) \n","accuracy: [86.32307692307693, 86.3, 86.41538461538461]\n","\n","== Train RUC ==\n","Epoch: [113][0/130] Train loss: 2.9739 (2.9739) \n","Epoch: [113][80/130] Train loss: 2.9267 (3.2187) \n","Epoch: [113][0/130] Train loss: 3.2962 (3.2962) \n","Epoch: [113][80/130] Train loss: 3.1961 (3.2282) \n","accuracy: [86.50769230769231, 86.45384615384616, 86.53846153846155]\n","\n","== Train RUC ==\n","Epoch: [114][0/130] Train loss: 3.2595 (3.2595) \n","Epoch: [114][80/130] Train loss: 3.2247 (3.2163) \n","Epoch: [114][0/130] Train loss: 3.4322 (3.4322) \n","Epoch: [114][80/130] Train loss: 3.1800 (3.2431) \n","accuracy: [86.36153846153846, 86.16923076923077, 86.4]\n","\n","== Train RUC ==\n","Epoch: [115][0/130] Train loss: 3.2793 (3.2793) \n","Epoch: [115][80/130] Train loss: 3.2973 (3.2300) \n","Epoch: [115][0/130] Train loss: 3.2629 (3.2629) \n","Epoch: [115][80/130] Train loss: 3.3192 (3.2184) \n","accuracy: [86.49230769230769, 86.18461538461538, 86.38461538461539]\n","\n","== Train RUC ==\n","Epoch: [116][0/130] Train loss: 3.3193 (3.3193) \n","Epoch: [116][80/130] Train loss: 2.9557 (3.2214) \n","Epoch: [116][0/130] Train loss: 3.4994 (3.4994) \n","Epoch: [116][80/130] Train loss: 3.3291 (3.2442) \n","accuracy: [86.36923076923077, 86.35384615384616, 86.42307692307692]\n","\n","== Train RUC ==\n","Epoch: [117][0/130] Train loss: 3.1930 (3.1930) \n","Epoch: [117][80/130] Train loss: 3.3618 (3.1919) \n","Epoch: [117][0/130] Train loss: 3.3756 (3.3756) \n","Epoch: [117][80/130] Train loss: 3.2446 (3.2427) \n","accuracy: [86.37692307692308, 86.29230769230769, 86.45384615384616]\n","\n","== Train RUC ==\n","Epoch: [118][0/130] Train loss: 3.3685 (3.3685) \n","Epoch: [118][80/130] Train loss: 3.2592 (3.2365) \n","Epoch: [118][0/130] Train loss: 3.2916 (3.2916) \n","Epoch: [118][80/130] Train loss: 3.1022 (3.2416) \n","accuracy: [86.46153846153845, 86.25384615384615, 86.43846153846154]\n","\n","== Train RUC ==\n","Epoch: [119][0/130] Train loss: 3.3160 (3.3160) \n","Epoch: [119][80/130] Train loss: 2.8031 (3.2342) \n","Epoch: [119][0/130] Train loss: 3.2795 (3.2795) \n","Epoch: [119][80/130] Train loss: 3.1162 (3.2595) \n","accuracy: [86.48461538461538, 86.26923076923076, 86.43076923076923]\n","\n","== Train RUC ==\n","Epoch: [120][0/130] Train loss: 2.9293 (2.9293) \n","Epoch: [120][80/130] Train loss: 3.3477 (3.2282) \n","Epoch: [120][0/130] Train loss: 3.3387 (3.3387) \n","Epoch: [120][80/130] Train loss: 3.3272 (3.2323) \n","accuracy: [86.35384615384616, 86.22307692307693, 86.36153846153846]\n","\n","== Train RUC ==\n","Epoch: [121][0/130] Train loss: 3.4412 (3.4412) \n","Epoch: [121][80/130] Train loss: 3.2964 (3.2271) \n","Epoch: [121][0/130] Train loss: 3.1703 (3.1703) \n","Epoch: [121][80/130] Train loss: 2.7558 (3.2609) \n","accuracy: [86.4, 86.4076923076923, 86.48461538461538]\n","\n","== Train RUC ==\n","Epoch: [122][0/130] Train loss: 3.1886 (3.1886) \n","Epoch: [122][80/130] Train loss: 3.4262 (3.2547) \n","Epoch: [122][0/130] Train loss: 3.1763 (3.1763) \n","Epoch: [122][80/130] Train loss: 3.2051 (3.2411) \n","accuracy: [86.55384615384617, 86.06153846153846, 86.4]\n","\n","== Train RUC ==\n","Epoch: [123][0/130] Train loss: 2.9214 (2.9214) \n","Epoch: [123][80/130] Train loss: 3.0792 (3.2279) \n","Epoch: [123][0/130] Train loss: 3.3208 (3.3208) \n","Epoch: [123][80/130] Train loss: 3.1123 (3.2452) \n","accuracy: [86.26153846153845, 86.09230769230768, 86.38461538461539]\n","\n","== Train RUC ==\n","Epoch: [124][0/130] Train loss: 3.2668 (3.2668) \n","Epoch: [124][80/130] Train loss: 2.7242 (3.2507) \n","Epoch: [124][0/130] Train loss: 3.2523 (3.2523) \n","Epoch: [124][80/130] Train loss: 3.3445 (3.2520) \n","accuracy: [86.38461538461539, 86.3923076923077, 86.46153846153845]\n","\n","== Train RUC ==\n","Epoch: [125][0/130] Train loss: 3.4290 (3.4290) \n","Epoch: [125][80/130] Train loss: 3.2254 (3.2444) \n","Epoch: [125][0/130] Train loss: 3.3071 (3.3071) \n","Epoch: [125][80/130] Train loss: 3.4374 (3.2416) \n","accuracy: [86.59230769230768, 86.3, 86.51538461538462]\n","\n","== Train RUC ==\n","Epoch: [126][0/130] Train loss: 3.1652 (3.1652) \n","Epoch: [126][80/130] Train loss: 3.4266 (3.2324) \n","Epoch: [126][0/130] Train loss: 3.2216 (3.2216) \n","Epoch: [126][80/130] Train loss: 3.1312 (3.1944) \n","accuracy: [86.37692307692308, 86.36153846153846, 86.43846153846154]\n","\n","== Train RUC ==\n","Epoch: [127][0/130] Train loss: 3.4031 (3.4031) \n","Epoch: [127][80/130] Train loss: 3.3356 (3.2352) \n","Epoch: [127][0/130] Train loss: 2.9766 (2.9766) \n","Epoch: [127][80/130] Train loss: 3.3947 (3.2492) \n","accuracy: [86.58461538461538, 86.3923076923077, 86.48461538461538]\n","\n","== Train RUC ==\n","Epoch: [128][0/130] Train loss: 3.2294 (3.2294) \n","Epoch: [128][80/130] Train loss: 2.9171 (3.2552) \n","Epoch: [128][0/130] Train loss: 3.0606 (3.0606) \n","Epoch: [128][80/130] Train loss: 2.6702 (3.2277) \n","accuracy: [86.46923076923076, 86.2, 86.3923076923077]\n","\n","== Train RUC ==\n","Epoch: [129][0/130] Train loss: 3.0690 (3.0690) \n","Epoch: [129][80/130] Train loss: 3.3316 (3.2557) \n","Epoch: [129][0/130] Train loss: 3.1113 (3.1113) \n","Epoch: [129][80/130] Train loss: 3.1402 (3.2376) \n","accuracy: [86.37692307692308, 86.36923076923077, 86.33846153846154]\n","\n","== Train RUC ==\n","Epoch: [130][0/130] Train loss: 3.3613 (3.3613) \n","Epoch: [130][80/130] Train loss: 3.3478 (3.2454) \n","Epoch: [130][0/130] Train loss: 3.3324 (3.3324) \n","Epoch: [130][80/130] Train loss: 3.1790 (3.2186) \n","accuracy: [86.5, 86.37692307692308, 86.46923076923076]\n","\n","== Train RUC ==\n","Epoch: [131][0/130] Train loss: 2.8636 (2.8636) \n","Epoch: [131][80/130] Train loss: 3.3271 (3.2428) \n","Epoch: [131][0/130] Train loss: 2.8322 (2.8322) \n","Epoch: [131][80/130] Train loss: 3.3140 (3.2652) \n","accuracy: [86.51538461538462, 86.34615384615385, 86.49230769230769]\n","\n","== Train RUC ==\n","Epoch: [132][0/130] Train loss: 3.0386 (3.0386) \n","Epoch: [132][80/130] Train loss: 3.2351 (3.2843) \n","Epoch: [132][0/130] Train loss: 3.3389 (3.3389) \n","Epoch: [132][80/130] Train loss: 3.2871 (3.2652) \n","accuracy: [86.44615384615385, 86.44615384615385, 86.48461538461538]\n","\n","== Train RUC ==\n","Epoch: [133][0/130] Train loss: 3.0568 (3.0568) \n","Epoch: [133][80/130] Train loss: 3.3728 (3.2905) \n","Epoch: [133][0/130] Train loss: 2.9652 (2.9652) \n","Epoch: [133][80/130] Train loss: 2.9750 (3.2396) \n","accuracy: [86.50769230769231, 86.43846153846154, 86.53846153846155]\n","\n","== Train RUC ==\n","Epoch: [134][0/130] Train loss: 3.3201 (3.3201) \n","Epoch: [134][80/130] Train loss: 3.5582 (3.2919) \n","Epoch: [134][0/130] Train loss: 3.3803 (3.3803) \n","Epoch: [134][80/130] Train loss: 3.1815 (3.2136) \n","accuracy: [86.36923076923077, 86.46153846153845, 86.47692307692307]\n","\n","== Train RUC ==\n","Epoch: [135][0/130] Train loss: 3.4407 (3.4407) \n","Epoch: [135][80/130] Train loss: 3.2510 (3.2033) \n","Epoch: [135][0/130] Train loss: 3.1617 (3.1617) \n","Epoch: [135][80/130] Train loss: 3.3836 (3.2958) \n","accuracy: [86.45384615384616, 86.49230769230769, 86.48461538461538]\n","\n","== Train RUC ==\n","Epoch: [136][0/130] Train loss: 3.1720 (3.1720) \n","Epoch: [136][80/130] Train loss: 3.2323 (3.2508) \n","Epoch: [136][0/130] Train loss: 3.2709 (3.2709) \n","Epoch: [136][80/130] Train loss: 3.2566 (3.2792) \n","accuracy: [86.38461538461539, 86.41538461538461, 86.43846153846154]\n","\n","== Train RUC ==\n","Epoch: [137][0/130] Train loss: 2.4809 (2.4809) \n","Epoch: [137][80/130] Train loss: 3.3206 (3.2390) \n","Epoch: [137][0/130] Train loss: 3.3264 (3.3264) \n","Epoch: [137][80/130] Train loss: 2.4974 (3.2366) \n","accuracy: [86.33076923076923, 86.37692307692308, 86.43076923076923]\n","\n","== Train RUC ==\n","Epoch: [138][0/130] Train loss: 3.1925 (3.1925) \n","Epoch: [138][80/130] Train loss: 3.3905 (3.2346) \n","Epoch: [138][0/130] Train loss: 3.3336 (3.3336) \n","Epoch: [138][80/130] Train loss: 3.4653 (3.2481) \n","accuracy: [86.48461538461538, 86.35384615384616, 86.46923076923076]\n","\n","== Train RUC ==\n","Epoch: [139][0/130] Train loss: 3.2770 (3.2770) \n","Epoch: [139][80/130] Train loss: 2.8223 (3.2425) \n","Epoch: [139][0/130] Train loss: 3.2977 (3.2977) \n","Epoch: [139][80/130] Train loss: 3.1111 (3.2434) \n","accuracy: [86.46153846153845, 86.38461538461539, 86.45384615384616]\n","\n","== Train RUC ==\n","Epoch: [140][0/130] Train loss: 3.0472 (3.0472) \n","Epoch: [140][80/130] Train loss: 3.4269 (3.2474) \n","Epoch: [140][0/130] Train loss: 3.2883 (3.2883) \n","Epoch: [140][80/130] Train loss: 3.3083 (3.2901) \n","accuracy: [86.47692307692307, 86.55384615384617, 86.51538461538462]\n","\n","== Train RUC ==\n","Epoch: [141][0/130] Train loss: 3.4414 (3.4414) \n","Epoch: [141][80/130] Train loss: 3.0243 (3.2677) \n","Epoch: [141][0/130] Train loss: 3.2962 (3.2962) \n","Epoch: [141][80/130] Train loss: 3.3525 (3.2296) \n","accuracy: [86.44615384615385, 86.3923076923077, 86.46153846153845]\n","\n","== Train RUC ==\n","Epoch: [142][0/130] Train loss: 3.2949 (3.2949) \n","Epoch: [142][80/130] Train loss: 3.5231 (3.2371) \n","Epoch: [142][0/130] Train loss: 3.4680 (3.4680) \n","Epoch: [142][80/130] Train loss: 3.4443 (3.2656) \n","accuracy: [86.4076923076923, 86.44615384615385, 86.43076923076923]\n","\n","== Train RUC ==\n","Epoch: [143][0/130] Train loss: 3.0932 (3.0932) \n","Epoch: [143][80/130] Train loss: 3.3201 (3.2254) \n","Epoch: [143][0/130] Train loss: 3.1886 (3.1886) \n","Epoch: [143][80/130] Train loss: 3.2815 (3.2222) \n","accuracy: [86.46153846153845, 86.33076923076923, 86.51538461538462]\n","\n","== Train RUC ==\n","Epoch: [144][0/130] Train loss: 3.2831 (3.2831) \n","Epoch: [144][80/130] Train loss: 3.3711 (3.2121) \n","Epoch: [144][0/130] Train loss: 3.3055 (3.3055) \n","Epoch: [144][80/130] Train loss: 3.2770 (3.2795) \n","accuracy: [86.45384615384616, 86.41538461538461, 86.43846153846154]\n","\n","== Train RUC ==\n","Epoch: [145][0/130] Train loss: 2.8873 (2.8873) \n","Epoch: [145][80/130] Train loss: 3.2005 (3.2672) \n","Epoch: [145][0/130] Train loss: 3.4285 (3.4285) \n","Epoch: [145][80/130] Train loss: 3.4595 (3.2532) \n","accuracy: [86.49230769230769, 86.36153846153846, 86.46923076923076]\n","\n","== Train RUC ==\n","Epoch: [146][0/130] Train loss: 3.3700 (3.3700) \n","Epoch: [146][80/130] Train loss: 3.3999 (3.2740) \n","Epoch: [146][0/130] Train loss: 3.0714 (3.0714) \n","Epoch: [146][80/130] Train loss: 3.2236 (3.2712) \n","accuracy: [86.46923076923076, 86.32307692307693, 86.42307692307692]\n","\n","== Train RUC ==\n","Epoch: [147][0/130] Train loss: 2.7526 (2.7526) \n","Epoch: [147][80/130] Train loss: 3.1462 (3.2799) \n","Epoch: [147][0/130] Train loss: 3.2112 (3.2112) \n","Epoch: [147][80/130] Train loss: 3.3761 (3.2558) \n","accuracy: [86.36153846153846, 86.47692307692307, 86.35384615384616]\n","\n","== Train RUC ==\n","Epoch: [148][0/130] Train loss: 2.8488 (2.8488) \n","Epoch: [148][80/130] Train loss: 3.0577 (3.2449) \n","Epoch: [148][0/130] Train loss: 3.3604 (3.3604) \n","Epoch: [148][80/130] Train loss: 3.2354 (3.2668) \n","accuracy: [86.38461538461539, 86.41538461538461, 86.43846153846154]\n","\n","== Train RUC ==\n","Epoch: [149][0/130] Train loss: 3.2676 (3.2676) \n","Epoch: [149][80/130] Train loss: 3.2830 (3.2647) \n","Epoch: [149][0/130] Train loss: 3.3510 (3.3510) \n","Epoch: [149][80/130] Train loss: 3.3016 (3.2451) \n","accuracy: [86.48461538461538, 86.44615384615385, 86.48461538461538]\n","\n","== Train RUC ==\n","Epoch: [150][0/130] Train loss: 3.4661 (3.4661) \n","Epoch: [150][80/130] Train loss: 3.2606 (3.2331) \n","Epoch: [150][0/130] Train loss: 2.7381 (2.7381) \n","Epoch: [150][80/130] Train loss: 3.3391 (3.2416) \n","accuracy: [86.54615384615386, 86.49230769230769, 86.45384615384616]\n","\n","== Train RUC ==\n","Epoch: [151][0/130] Train loss: 3.2589 (3.2589) \n","Epoch: [151][80/130] Train loss: 3.4482 (3.2912) \n","Epoch: [151][0/130] Train loss: 3.3188 (3.3188) \n","Epoch: [151][80/130] Train loss: 3.3145 (3.2242) \n","accuracy: [86.52307692307693, 86.33846153846154, 86.43846153846154]\n","\n","== Train RUC ==\n","Epoch: [152][0/130] Train loss: 3.2482 (3.2482) \n","Epoch: [152][80/130] Train loss: 3.3109 (3.2070) \n","Epoch: [152][0/130] Train loss: 3.3944 (3.3944) \n","Epoch: [152][80/130] Train loss: 3.2147 (3.2526) \n","accuracy: [86.46923076923076, 86.49230769230769, 86.44615384615385]\n","\n","== Train RUC ==\n","Epoch: [153][0/130] Train loss: 2.8177 (2.8177) \n","Epoch: [153][80/130] Train loss: 3.3282 (3.2393) \n","Epoch: [153][0/130] Train loss: 3.1499 (3.1499) \n","Epoch: [153][80/130] Train loss: 3.3154 (3.2505) \n","accuracy: [86.44615384615385, 86.36923076923077, 86.44615384615385]\n","\n","== Train RUC ==\n","Epoch: [154][0/130] Train loss: 3.3983 (3.3983) \n","Epoch: [154][80/130] Train loss: 3.2364 (3.2334) \n","Epoch: [154][0/130] Train loss: 3.4156 (3.4156) \n","Epoch: [154][80/130] Train loss: 3.3765 (3.2449) \n","accuracy: [86.42307692307692, 86.48461538461538, 86.42307692307692]\n","\n","== Train RUC ==\n","Epoch: [155][0/130] Train loss: 3.3425 (3.3425) \n","Epoch: [155][80/130] Train loss: 3.3487 (3.2670) \n","Epoch: [155][0/130] Train loss: 3.3306 (3.3306) \n","Epoch: [155][80/130] Train loss: 3.5571 (3.2415) \n","accuracy: [86.46153846153845, 86.42307692307692, 86.48461538461538]\n","\n","== Train RUC ==\n","Epoch: [156][0/130] Train loss: 3.0984 (3.0984) \n","Epoch: [156][80/130] Train loss: 3.3035 (3.2405) \n","Epoch: [156][0/130] Train loss: 3.1807 (3.1807) \n","Epoch: [156][80/130] Train loss: 3.1798 (3.2354) \n","accuracy: [86.43076923076923, 86.44615384615385, 86.44615384615385]\n","\n","== Train RUC ==\n","Epoch: [157][0/130] Train loss: 3.3818 (3.3818) \n","Epoch: [157][80/130] Train loss: 3.1825 (3.2607) \n","Epoch: [157][0/130] Train loss: 3.3411 (3.3411) \n","Epoch: [157][80/130] Train loss: 3.2446 (3.2706) \n","accuracy: [86.46923076923076, 86.58461538461538, 86.54615384615386]\n","\n","== Train RUC ==\n","Epoch: [158][0/130] Train loss: 3.3402 (3.3402) \n","Epoch: [158][80/130] Train loss: 3.1966 (3.2585) \n","Epoch: [158][0/130] Train loss: 3.2406 (3.2406) \n","Epoch: [158][80/130] Train loss: 3.1468 (3.2377) \n","accuracy: [86.46153846153845, 86.5, 86.48461538461538]\n","\n","== Train RUC ==\n","Epoch: [159][0/130] Train loss: 3.1010 (3.1010) \n","Epoch: [159][80/130] Train loss: 3.4430 (3.2488) \n","Epoch: [159][0/130] Train loss: 3.2834 (3.2834) \n","Epoch: [159][80/130] Train loss: 3.3566 (3.2191) \n","accuracy: [86.50769230769231, 86.46923076923076, 86.48461538461538]\n","\n","== Train RUC ==\n","Epoch: [160][0/130] Train loss: 3.3512 (3.3512) \n","Epoch: [160][80/130] Train loss: 3.3249 (3.2225) \n","Epoch: [160][0/130] Train loss: 3.1624 (3.1624) \n","Epoch: [160][80/130] Train loss: 3.4765 (3.2047) \n","accuracy: [86.41538461538461, 86.46153846153845, 86.47692307692307]\n","\n","== Train RUC ==\n","Epoch: [161][0/130] Train loss: 3.1875 (3.1875) \n","Epoch: [161][80/130] Train loss: 3.4358 (3.2667) \n","Epoch: [161][0/130] Train loss: 3.3861 (3.3861) \n","Epoch: [161][80/130] Train loss: 3.4355 (3.2409) \n","accuracy: [86.45384615384616, 86.46923076923076, 86.45384615384616]\n","\n","== Train RUC ==\n","Epoch: [162][0/130] Train loss: 3.2428 (3.2428) \n","Epoch: [162][80/130] Train loss: 3.3043 (3.2626) \n","Epoch: [162][0/130] Train loss: 2.9860 (2.9860) \n","Epoch: [162][80/130] Train loss: 3.2763 (3.2317) \n","accuracy: [86.48461538461538, 86.50769230769231, 86.47692307692307]\n","\n","== Train RUC ==\n","Epoch: [163][0/130] Train loss: 3.4063 (3.4063) \n","Epoch: [163][80/130] Train loss: 3.4724 (3.2206) \n","Epoch: [163][0/130] Train loss: 3.2733 (3.2733) \n","Epoch: [163][80/130] Train loss: 3.3134 (3.2037) \n","accuracy: [86.46153846153845, 86.43846153846154, 86.45384615384616]\n","\n","== Train RUC ==\n","Epoch: [164][0/130] Train loss: 2.6116 (2.6116) \n","Epoch: [164][80/130] Train loss: 3.1650 (3.2448) \n","Epoch: [164][0/130] Train loss: 3.3394 (3.3394) \n","Epoch: [164][80/130] Train loss: 3.2626 (3.2232) \n","accuracy: [86.43076923076923, 86.5, 86.46923076923076]\n","\n","== Train RUC ==\n","Epoch: [165][0/130] Train loss: 3.0116 (3.0116) \n","Epoch: [165][80/130] Train loss: 3.5288 (3.2298) \n","Epoch: [165][0/130] Train loss: 3.3487 (3.3487) \n","Epoch: [165][80/130] Train loss: 3.2069 (3.2429) \n","accuracy: [86.43076923076923, 86.44615384615385, 86.44615384615385]\n","\n","== Train RUC ==\n","Epoch: [166][0/130] Train loss: 3.5874 (3.5874) \n","Epoch: [166][80/130] Train loss: 3.4263 (3.2404) \n","Epoch: [166][0/130] Train loss: 3.3703 (3.3703) \n","Epoch: [166][80/130] Train loss: 3.4566 (3.2601) \n","accuracy: [86.48461538461538, 86.50769230769231, 86.49230769230769]\n","\n","== Train RUC ==\n","Epoch: [167][0/130] Train loss: 3.2984 (3.2984) \n","Epoch: [167][80/130] Train loss: 3.3502 (3.2646) \n","Epoch: [167][0/130] Train loss: 3.3494 (3.3494) \n","Epoch: [167][80/130] Train loss: 3.3506 (3.2363) \n","accuracy: [86.46923076923076, 86.46153846153845, 86.47692307692307]\n","\n","== Train RUC ==\n","Epoch: [168][0/130] Train loss: 3.2072 (3.2072) \n","Epoch: [168][80/130] Train loss: 3.3536 (3.2546) \n","Epoch: [168][0/130] Train loss: 3.4747 (3.4747) \n","Epoch: [168][80/130] Train loss: 3.0662 (3.2576) \n","accuracy: [86.5, 86.43846153846154, 86.47692307692307]\n","\n","== Train RUC ==\n","Epoch: [169][0/130] Train loss: 3.3188 (3.3188) \n","Epoch: [169][80/130] Train loss: 3.4962 (3.2290) \n","Epoch: [169][0/130] Train loss: 3.0669 (3.0669) \n","Epoch: [169][80/130] Train loss: 3.3532 (3.2179) \n","accuracy: [86.46923076923076, 86.46153846153845, 86.47692307692307]\n","\n","== Train RUC ==\n","Epoch: [170][0/130] Train loss: 3.3136 (3.3136) \n","Epoch: [170][80/130] Train loss: 3.1550 (3.2197) \n","Epoch: [170][0/130] Train loss: 3.3727 (3.3727) \n","Epoch: [170][80/130] Train loss: 3.2892 (3.2522) \n","accuracy: [86.49230769230769, 86.42307692307692, 86.47692307692307]\n","\n","== Train RUC ==\n","Epoch: [171][0/130] Train loss: 3.3466 (3.3466) \n","Epoch: [171][80/130] Train loss: 3.2760 (3.2449) \n","Epoch: [171][0/130] Train loss: 2.8768 (2.8768) \n","Epoch: [171][80/130] Train loss: 3.3445 (3.2473) \n","accuracy: [86.46153846153845, 86.42307692307692, 86.47692307692307]\n","\n","== Train RUC ==\n","Epoch: [172][0/130] Train loss: 2.7314 (2.7314) \n","Epoch: [172][80/130] Train loss: 3.4234 (3.2386) \n","Epoch: [172][0/130] Train loss: 2.7435 (2.7435) \n","Epoch: [172][80/130] Train loss: 3.4002 (3.2180) \n","accuracy: [86.48461538461538, 86.45384615384616, 86.49230769230769]\n","\n","== Train RUC ==\n","Epoch: [173][0/130] Train loss: 3.2683 (3.2683) \n","Epoch: [173][80/130] Train loss: 3.3582 (3.2372) \n","Epoch: [173][0/130] Train loss: 3.3782 (3.3782) \n","Epoch: [173][80/130] Train loss: 3.3774 (3.2165) \n","accuracy: [86.43846153846154, 86.46153846153845, 86.46153846153845]\n","\n","== Train RUC ==\n","Epoch: [174][0/130] Train loss: 3.2548 (3.2548) \n","Epoch: [174][80/130] Train loss: 3.3720 (3.2738) \n","Epoch: [174][0/130] Train loss: 2.8676 (2.8676) \n","Epoch: [174][80/130] Train loss: 3.1858 (3.1873) \n","accuracy: [86.48461538461538, 86.47692307692307, 86.51538461538462]\n","\n","== Train RUC ==\n","Epoch: [175][0/130] Train loss: 3.3087 (3.3087) \n","Epoch: [175][80/130] Train loss: 3.3991 (3.2514) \n","Epoch: [175][0/130] Train loss: 3.1902 (3.1902) \n","Epoch: [175][80/130] Train loss: 3.3054 (3.2505) \n","accuracy: [86.46153846153845, 86.48461538461538, 86.48461538461538]\n","\n","== Train RUC ==\n","Epoch: [176][0/130] Train loss: 3.1843 (3.1843) \n","Epoch: [176][80/130] Train loss: 3.0067 (3.1840) \n","Epoch: [176][0/130] Train loss: 3.4664 (3.4664) \n","Epoch: [176][80/130] Train loss: 3.1680 (3.2136) \n","accuracy: [86.47692307692307, 86.45384615384616, 86.47692307692307]\n","\n","== Train RUC ==\n","Epoch: [177][0/130] Train loss: 3.4295 (3.4295) \n","Epoch: [177][80/130] Train loss: 2.8099 (3.2331) \n","Epoch: [177][0/130] Train loss: 3.3775 (3.3775) \n","Epoch: [177][80/130] Train loss: 3.1695 (3.2396) \n","accuracy: [86.43846153846154, 86.43076923076923, 86.47692307692307]\n","\n","== Train RUC ==\n","Epoch: [178][0/130] Train loss: 3.2251 (3.2251) \n","Epoch: [178][80/130] Train loss: 3.4042 (3.2434) \n","Epoch: [178][0/130] Train loss: 2.7175 (2.7175) \n","Epoch: [178][80/130] Train loss: 3.3334 (3.2205) \n","accuracy: [86.46153846153845, 86.43076923076923, 86.47692307692307]\n","\n","== Train RUC ==\n","Epoch: [179][0/130] Train loss: 3.1319 (3.1319) \n","Epoch: [179][80/130] Train loss: 3.1970 (3.2390) \n","Epoch: [179][0/130] Train loss: 3.3528 (3.3528) \n","Epoch: [179][80/130] Train loss: 3.3842 (3.2325) \n","accuracy: [86.46923076923076, 86.49230769230769, 86.46923076923076]\n","\n","== Train RUC ==\n","Epoch: [180][0/130] Train loss: 3.3256 (3.3256) \n","Epoch: [180][80/130] Train loss: 3.3466 (3.2888) \n","Epoch: [180][0/130] Train loss: 3.3637 (3.3637) \n","Epoch: [180][80/130] Train loss: 3.4678 (3.2715) \n","accuracy: [86.46923076923076, 86.5, 86.50769230769231]\n","\n","== Train RUC ==\n","Epoch: [181][0/130] Train loss: 3.3021 (3.3021) \n","Epoch: [181][80/130] Train loss: 3.4195 (3.2071) \n","Epoch: [181][0/130] Train loss: 3.4136 (3.4136) \n","Epoch: [181][80/130] Train loss: 3.1582 (3.2407) \n","accuracy: [86.49230769230769, 86.47692307692307, 86.49230769230769]\n","\n","== Train RUC ==\n","Epoch: [182][0/130] Train loss: 3.4598 (3.4598) \n","Epoch: [182][80/130] Train loss: 3.3551 (3.2296) \n","Epoch: [182][0/130] Train loss: 3.1083 (3.1083) \n","Epoch: [182][80/130] Train loss: 3.3381 (3.2248) \n","accuracy: [86.46923076923076, 86.48461538461538, 86.49230769230769]\n","\n","== Train RUC ==\n","Epoch: [183][0/130] Train loss: 3.3028 (3.3028) \n","Epoch: [183][80/130] Train loss: 3.3685 (3.2091) \n","Epoch: [183][0/130] Train loss: 3.3849 (3.3849) \n","Epoch: [183][80/130] Train loss: 3.1228 (3.2168) \n","accuracy: [86.51538461538462, 86.49230769230769, 86.49230769230769]\n","\n","== Train RUC ==\n","Epoch: [184][0/130] Train loss: 3.3521 (3.3521) \n","Epoch: [184][80/130] Train loss: 3.3458 (3.2537) \n","Epoch: [184][0/130] Train loss: 3.1988 (3.1988) \n","Epoch: [184][80/130] Train loss: 3.5374 (3.2324) \n","accuracy: [86.47692307692307, 86.46153846153845, 86.50769230769231]\n","\n","== Train RUC ==\n","Epoch: [185][0/130] Train loss: 3.0329 (3.0329) \n","Epoch: [185][80/130] Train loss: 3.4347 (3.1813) \n","Epoch: [185][0/130] Train loss: 3.2786 (3.2786) \n","Epoch: [185][80/130] Train loss: 3.2852 (3.2223) \n","accuracy: [86.48461538461538, 86.47692307692307, 86.49230769230769]\n","\n","== Train RUC ==\n","Epoch: [186][0/130] Train loss: 3.2251 (3.2251) \n","Epoch: [186][80/130] Train loss: 2.9532 (3.2709) \n","Epoch: [186][0/130] Train loss: 3.2564 (3.2564) \n","Epoch: [186][80/130] Train loss: 3.5004 (3.2472) \n","accuracy: [86.51538461538462, 86.49230769230769, 86.50769230769231]\n","\n","== Train RUC ==\n","Epoch: [187][0/130] Train loss: 3.4030 (3.4030) \n","Epoch: [187][80/130] Train loss: 3.2596 (3.2339) \n","Epoch: [187][0/130] Train loss: 3.3328 (3.3328) \n","Epoch: [187][80/130] Train loss: 3.0734 (3.2599) \n","accuracy: [86.5, 86.49230769230769, 86.50769230769231]\n","\n","== Train RUC ==\n","Epoch: [188][0/130] Train loss: 2.7887 (2.7887) \n","Epoch: [188][80/130] Train loss: 3.2937 (3.2267) \n","Epoch: [188][0/130] Train loss: 3.4587 (3.4587) \n","Epoch: [188][80/130] Train loss: 3.2659 (3.2467) \n","accuracy: [86.47692307692307, 86.49230769230769, 86.48461538461538]\n","\n","== Train RUC ==\n","Epoch: [189][0/130] Train loss: 3.2068 (3.2068) \n","Epoch: [189][80/130] Train loss: 3.4333 (3.2552) \n","Epoch: [189][0/130] Train loss: 3.2764 (3.2764) \n","Epoch: [189][80/130] Train loss: 3.2762 (3.2511) \n","accuracy: [86.5, 86.49230769230769, 86.50769230769231]\n","\n","== Train RUC ==\n","Epoch: [190][0/130] Train loss: 3.3256 (3.3256) \n","Epoch: [190][80/130] Train loss: 3.3799 (3.2173) \n","Epoch: [190][0/130] Train loss: 3.0591 (3.0591) \n","Epoch: [190][80/130] Train loss: 3.2346 (3.2437) \n","accuracy: [86.47692307692307, 86.5, 86.5]\n","\n","== Train RUC ==\n","Epoch: [191][0/130] Train loss: 3.3405 (3.3405) \n","Epoch: [191][80/130] Train loss: 2.8808 (3.2510) \n","Epoch: [191][0/130] Train loss: 3.2726 (3.2726) \n","Epoch: [191][80/130] Train loss: 3.5012 (3.2508) \n","accuracy: [86.48461538461538, 86.46153846153845, 86.49230769230769]\n","\n","== Train RUC ==\n","Epoch: [192][0/130] Train loss: 2.8436 (2.8436) \n","Epoch: [192][80/130] Train loss: 3.2391 (3.2403) \n","Epoch: [192][0/130] Train loss: 3.0349 (3.0349) \n","Epoch: [192][80/130] Train loss: 3.3938 (3.2487) \n","accuracy: [86.49230769230769, 86.49230769230769, 86.48461538461538]\n","\n","== Train RUC ==\n","Epoch: [193][0/130] Train loss: 3.3900 (3.3900) \n","Epoch: [193][80/130] Train loss: 3.2691 (3.2438) \n","Epoch: [193][0/130] Train loss: 3.1887 (3.1887) \n","Epoch: [193][80/130] Train loss: 3.1564 (3.2446) \n","accuracy: [86.47692307692307, 86.48461538461538, 86.47692307692307]\n","\n","== Train RUC ==\n","Epoch: [194][0/130] Train loss: 3.3208 (3.3208) \n","Epoch: [194][80/130] Train loss: 3.1113 (3.2324) \n","Epoch: [194][0/130] Train loss: 2.6448 (2.6448) \n","Epoch: [194][80/130] Train loss: 3.3328 (3.2321) \n","accuracy: [86.49230769230769, 86.48461538461538, 86.48461538461538]\n","\n","== Train RUC ==\n","Epoch: [195][0/130] Train loss: 3.3619 (3.3619) \n","Epoch: [195][80/130] Train loss: 3.1511 (3.2457) \n","Epoch: [195][0/130] Train loss: 2.7788 (2.7788) \n","Epoch: [195][80/130] Train loss: 3.3283 (3.2753) \n","accuracy: [86.47692307692307, 86.47692307692307, 86.48461538461538]\n","\n","== Train RUC ==\n","Epoch: [196][0/130] Train loss: 3.2552 (3.2552) \n","Epoch: [196][80/130] Train loss: 3.3531 (3.2971) \n","Epoch: [196][0/130] Train loss: 3.1937 (3.1937) \n","Epoch: [196][80/130] Train loss: 2.9478 (3.2467) \n","accuracy: [86.49230769230769, 86.47692307692307, 86.48461538461538]\n","\n","== Train RUC ==\n","Epoch: [197][0/130] Train loss: 3.5059 (3.5059) \n","Epoch: [197][80/130] Train loss: 2.8169 (3.2742) \n","Epoch: [197][0/130] Train loss: 3.1230 (3.1230) \n","Epoch: [197][80/130] Train loss: 3.4051 (3.2958) \n","accuracy: [86.48461538461538, 86.46923076923076, 86.48461538461538]\n","\n","== Train RUC ==\n","Epoch: [198][0/130] Train loss: 3.4160 (3.4160) \n","Epoch: [198][80/130] Train loss: 3.3896 (3.2585) \n","Epoch: [198][0/130] Train loss: 3.3495 (3.3495) \n","Epoch: [198][80/130] Train loss: 3.1451 (3.2905) \n","accuracy: [86.51538461538462, 86.47692307692307, 86.49230769230769]\n","\n","== Train RUC ==\n","Epoch: [199][0/130] Train loss: 3.1322 (3.1322) \n","Epoch: [199][80/130] Train loss: 2.5657 (3.2625) \n","Epoch: [199][0/130] Train loss: 3.2499 (3.2499) \n","Epoch: [199][80/130] Train loss: 3.2992 (3.2874) \n","accuracy: [86.48461538461538, 86.49230769230769, 86.47692307692307]\n","\n"]}]}]}