{"cells":[{"cell_type":"markdown","metadata":{"id":"RfhVE17sOKlw"},"source":["# Import packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rwt4l8WKOTHT"},"outputs":[],"source":["# Import packages\n","import torch\n","import numpy as np\n","import torchvision.transforms as transforms\n","import random\n","import PIL\n","import PIL.ImageOps\n","import PIL.ImageEnhance\n","import PIL.ImageDraw\n","from PIL import Image\n","import torchvision.datasets as torchdivision_datasets\n","import pickle\n","import os\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import copy\n","import torch.backends.cudnn as cudnn\n","from scipy.optimize import linear_sum_assignment as linear_assignment\n","import math"]},{"cell_type":"markdown","metadata":{"id":"1Tuu5vTVUogt"},"source":["#Change the working directory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hMmrEPsCUOY9"},"outputs":[],"source":["import os\n","\n","# Change the current working directory to a directory in Google Drive\n","new_directory_path = \"/content/drive/My Drive/Mercy college/Thesis\"\n","os.chdir(new_directory_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1699568559730,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"},"user_tz":300},"id":"UQpJNgbtUmFU","outputId":"384fd561-8188-4064-8fe0-a72e034bf9eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Current Working Directory: /content/drive/My Drive/Mercy college/Thesis\n"]}],"source":["# Verify the current working directory\n","current_directory = os.getcwd()\n","print(\"Current Working Directory:\", current_directory)"]},{"cell_type":"markdown","metadata":{"id":"srBqMHAqe4xx"},"source":["#Common Codes"]},{"cell_type":"markdown","metadata":{"id":"bUvNXWRqfs18"},"source":["##BasicBlock"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yXpqRJrsfrsF"},"outputs":[],"source":["class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1, is_last=False):\n","        super(BasicBlock, self).__init__()\n","        self.is_last = is_last\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion * planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion * planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        preact = out\n","        out = F.relu(out)\n","        if self.is_last:\n","            return out, preact\n","        else:\n","            return out"]},{"cell_type":"markdown","metadata":{"id":"uV7EVUbGf3Uu"},"source":["##Bottleneck"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9GI8fhfQf4Jd"},"outputs":[],"source":["class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1, is_last=False):\n","        super(Bottleneck, self).__init__()\n","        self.is_last = is_last\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion * planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion * planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        preact = out\n","        out = F.relu(out)\n","        if self.is_last:\n","            return out, preact\n","        else:\n","            return out"]},{"cell_type":"markdown","metadata":{"id":"S4JDNm9-fOFf"},"source":["##ResNet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Okz-2aJFe87O"},"outputs":[],"source":["class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, in_channel=3, zero_init_residual=False):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1,bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","        # Zero-initialize the last BN in each residual branch,\n","        # so that the residual branch starts with zeros, and each residual block behaves\n","        # like an identity. This improves the model by 0.2~0.3% according to:\n","        # https://arxiv.org/abs/1706.02677\n","        if zero_init_residual:\n","            for m in self.modules():\n","                if isinstance(m, Bottleneck):\n","                    nn.init.constant_(m.bn3.weight, 0)\n","                elif isinstance(m, BasicBlock):\n","                    nn.init.constant_(m.bn2.weight, 0)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1] * (num_blocks - 1)\n","        layers = []\n","        for i in range(num_blocks):\n","            stride = strides[i]\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = self.avgpool(out)\n","        out = torch.flatten(out, 1)\n","        return out\n"]},{"cell_type":"markdown","metadata":{"id":"m_DJzeqVgem2"},"source":["##Resnet_CIFAR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SZ8ANayJgdrK"},"outputs":[],"source":["def Resnet_CIFAR():\n","    return ResNet(BasicBlock, [2, 2, 2, 2])"]},{"cell_type":"markdown","metadata":{"id":"ojLykrplgo2y"},"source":["##ContrastiveModel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iq96XtcVgn00"},"outputs":[],"source":["class ContrastiveModel(nn.Module):\n","    def __init__(self, backbone, head='mlp', features_dim=128):\n","        super(ContrastiveModel, self).__init__()\n","        self.backbone = backbone\n","        self.backbone_dim = 512\n","        self.head = head\n","\n","        if head == 'linear':\n","            self.contrastive_head = nn.Linear(self.backbone_dim, features_dim)\n","\n","        elif head == 'mlp':\n","            self.contrastive_head = nn.Sequential(\n","                    nn.Linear(self.backbone_dim, self.backbone_dim),\n","                    nn.ReLU(), nn.Linear(self.backbone_dim, features_dim))\n","\n","        else:\n","            raise ValueError('Invalid head {}'.format(head))\n","\n","    def forward(self, x):\n","        features = self.contrastive_head(self.backbone(x))\n","        features = F.normalize(features, dim = 1)\n","        return features\n"]},{"cell_type":"markdown","metadata":{"id":"vh3fmXfSgymA"},"source":["##ClusteringModel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4KbEjmDpgxa2"},"outputs":[],"source":["class ClusteringModel(nn.Module):\n","    def __init__(self, backbone, class_num):\n","        super(ClusteringModel, self).__init__()\n","        self.backbone = backbone\n","        self.cluster_head = nn.ModuleList([nn.Linear(512, class_num) for _ in range(1)])\n","\n","    def forward(self, x):\n","        features = self.backbone(x)\n","        out = [cluster_head(features) for cluster_head in self.cluster_head]\n","\n","        return out[0]"]},{"cell_type":"markdown","metadata":{"id":"lSepSnpLhuf5"},"source":["##RandAugmentMC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HMBze49VhsOt"},"outputs":[],"source":["PARAMETER_MAX = 10\n","\n","def AutoContrast(img, **kwarg):\n","    return PIL.ImageOps.autocontrast(img)\n","\n","\n","def Brightness(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    return PIL.ImageEnhance.Brightness(img).enhance(v)\n","\n","\n","def Color(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    return PIL.ImageEnhance.Color(img).enhance(v)\n","\n","\n","def Contrast(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    return PIL.ImageEnhance.Contrast(img).enhance(v)\n","\n","\n","def Cutout(img, v, max_v, bias=0):\n","    if v == 0:\n","        return img\n","    v = _float_parameter(v, max_v) + bias\n","    v = int(v * min(img.size))\n","    return CutoutAbs(img, v)\n","\n","\n","def CutoutAbs(img, v, **kwarg):\n","    w, h = img.size\n","    x0 = np.random.uniform(0, w)\n","    y0 = np.random.uniform(0, h)\n","    x0 = int(max(0, x0 - v / 2.))\n","    y0 = int(max(0, y0 - v / 2.))\n","    x1 = int(min(w, x0 + v))\n","    y1 = int(min(h, y0 + v))\n","    xy = (x0, y0, x1, y1)\n","    # gray\n","    color = (127, 127, 127)\n","    img = img.copy()\n","    PIL.ImageDraw.Draw(img).rectangle(xy, color)\n","    return img\n","\n","\n","def Equalize(img, **kwarg):\n","    return PIL.ImageOps.equalize(img)\n","\n","\n","def Identity(img, **kwarg):\n","    return img\n","\n","\n","def Invert(img, **kwarg):\n","    return PIL.ImageOps.invert(img)\n","\n","\n","def Posterize(img, v, max_v, bias=0):\n","    v = _int_parameter(v, max_v) + bias\n","    return PIL.ImageOps.posterize(img, v)\n","\n","\n","def Rotate(img, v, max_v, bias=0):\n","    v = _int_parameter(v, max_v) + bias\n","    if random.random() < 0.5:\n","        v = -v\n","    return img.rotate(v)\n","\n","\n","def Sharpness(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    return PIL.ImageEnhance.Sharpness(img).enhance(v)\n","\n","\n","def ShearX(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    if random.random() < 0.5:\n","        v = -v\n","    return img.transform(img.size, PIL.Image.AFFINE, (1, v, 0, 0, 1, 0))\n","\n","\n","def ShearY(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    if random.random() < 0.5:\n","        v = -v\n","    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, v, 1, 0))\n","\n","\n","def Solarize(img, v, max_v, bias=0):\n","    v = _int_parameter(v, max_v) + bias\n","    return PIL.ImageOps.solarize(img, 256 - v)\n","\n","\n","def SolarizeAdd(img, v, max_v, bias=0, threshold=128):\n","    v = _int_parameter(v, max_v) + bias\n","    if random.random() < 0.5:\n","        v = -v\n","    img_np = np.array(img).astype(np.int)\n","    img_np = img_np + v\n","    img_np = np.clip(img_np, 0, 255)\n","    img_np = img_np.astype(np.uint8)\n","    img = Image.fromarray(img_np)\n","    return PIL.ImageOps.solarize(img, threshold)\n","\n","\n","def TranslateX(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    if random.random() < 0.5:\n","        v = -v\n","    v = int(v * img.size[0])\n","    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n","\n","\n","def TranslateY(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    if random.random() < 0.5:\n","        v = -v\n","    v = int(v * img.size[1])\n","    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n","\n","\n","def _float_parameter(v, max_v):\n","    return float(v) * max_v / PARAMETER_MAX\n","\n","\n","def _int_parameter(v, max_v):\n","    return int(v * max_v / PARAMETER_MAX)\n","\n","\n","def augment_pool():\n","    # FixMatch paper\n","    augs = [(AutoContrast, None, None),\n","            (Brightness, 0.99, 0.01),\n","            (Color, 0.99, 0.01),\n","            (Contrast, 0.99, 0.01),\n","            (Equalize, None, None),\n","            (Identity, None, None),\n","            (Posterize, 1, 8),\n","            (Rotate, 45, -45),\n","            (Sharpness, 0.99, 0.01),\n","            (ShearX, 0.3, -0.3),\n","            (ShearY, 0.3, -0.3),\n","            (Solarize, 256, 0),\n","            (TranslateX, 0.3, -0.3),\n","            (TranslateY, 0.3, -0.3)]\n","    return augs\n","\n","\n","class RandAugmentMC(object):\n","    def __init__(self, n, m):\n","        assert n >= 1\n","        assert 1 <= m <= 10\n","        self.n = n\n","        self.m = m\n","        self.augment_pool = augment_pool()\n","\n","    def __call__(self, img):\n","        ops = random.sample(self.augment_pool, k=self.n)\n","        for op, max_v, bias in ops:\n","            v = np.random.randint(1, self.m)\n","            if random.random() < 0.5:\n","                img = op(img, v=v, max_v=max_v, bias=bias)\n","        img = CutoutAbs(img, 16)\n","        return img"]},{"cell_type":"markdown","metadata":{"id":"sRoB6LJOjVrZ"},"source":["##AverageMeter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GvXfp6NajUDv"},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"]},{"cell_type":"markdown","metadata":{"id":"JjuwT-GcjlZm"},"source":["##_hungarian_match"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJi1OHhnjjyf"},"outputs":[],"source":["def _hungarian_match(flat_preds, flat_targets, num_samples, class_num):\n","    num_k = class_num\n","    num_correct = np.zeros((num_k, num_k))\n","\n","    for c1 in range(0, num_k):\n","        for c2 in range(0, num_k):\n","        # elementwise, so each sample contributes once\n","            votes = int(((flat_preds == c1) * (flat_targets == c2)).sum())\n","            num_correct[c1, c2] = votes\n","\n","    # num_correct is small\n","    match = linear_assignment(num_samples - num_correct)\n","\n","    # return as list of tuples, out_c to gt_c\n","    res = []\n","    for i in range(len(match[0])):\n","        out_c = match[0][i]\n","        gt_c = match[1][i]\n","        res.append((out_c, gt_c))\n","\n","    return res"]},{"cell_type":"markdown","metadata":{"id":"FGuyouNcjvcR"},"source":["##test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bq2heQW5juIZ"},"outputs":[],"source":["def test(net, testloader,device, class_num):\n","    net.eval()\n","    predicted_all = []\n","    targets_all = []\n","    for batch_idx, (inputs, _,_, targets, indexes) in enumerate(testloader):\n","        batchSize = inputs.size(0)\n","        targets, inputs = targets.to(device), inputs.to(device)\n","        output = net(inputs)\n","        predicted = torch.argmax(output, 1)\n","        predicted_all.append(predicted)\n","        targets_all.append(targets)\n","\n","\n","    flat_predict = torch.cat(predicted_all).to(device)\n","    flat_target = torch.cat(targets_all).to(device)\n","    num_samples = flat_predict.shape[0]\n","    match = _hungarian_match(flat_predict, flat_target, num_samples, class_num)\n","    reordered_preds = torch.zeros(num_samples).to(device)\n","\n","    for pred_i, target_i in match:\n","        reordered_preds[flat_predict == pred_i] = int(target_i)\n","\n","    acc = int((reordered_preds == flat_target.float()).sum()) / float(num_samples) * 100\n","\n","    return acc, reordered_preds"]},{"cell_type":"markdown","metadata":{"id":"VQ_aY6poj4ci"},"source":["##test_ruc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R56AcuT8j3Rj"},"outputs":[],"source":["def test_ruc(net, net2, testloader, device, class_num):\n","    net.eval()\n","    net2.eval()\n","\n","    predicted_all = [[] for i in range(0,3)]\n","    targets_all = []\n","    acc_list = []\n","    p_label_list = []\n","\n","    for batch_idx, (inputs, _, _, targets, indexes) in enumerate(testloader):\n","        batchSize = inputs.size(0)\n","        targets, inputs = targets.to(device), inputs.to(device)\n","        logit = net(inputs)\n","        logit2 = net2(inputs)\n","        _, predicted = torch.max(logit, 1)\n","        _, predicted2 = torch.max(logit2, 1)\n","        _, predicted3 = torch.max(logit + logit2, 1)\n","\n","        predicted_all[0].append(predicted)\n","        predicted_all[1].append(predicted2)\n","        predicted_all[2].append(predicted3)\n","        targets_all.append(targets)\n","\n","    for i in range(0, 3):\n","        flat_predict = torch.cat(predicted_all[i]).to(device)\n","        flat_target = torch.cat(targets_all).to(device)\n","        num_samples = flat_predict.shape[0]\n","        acc = int((flat_predict.float() == flat_target.float()).sum()) / float(num_samples) * 100\n","        acc_list.append(acc)\n","        p_label_list.append(flat_predict)\n","\n","    return acc_list, p_label_list"]},{"cell_type":"markdown","metadata":{"id":"_ISS5dIES3tG"},"source":["# CIFAR10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dkbeNC1BW8ds"},"outputs":[],"source":["# Random seed\n","seed = 1567010775\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","np.random.seed(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8v1o0arflLxk"},"outputs":[],"source":["# Default variables\n","lr = 0.01\n","momentum = 0.9\n","weight_decay = 5e-4\n","epochs = 200\n","batch_size = 250\n","s_thr = 0.99\n","n_num = 100\n","o_model = 'checkpoint/selflabel_cifar-10.pth.tar'\n","e_model = 'checkpoint/simclr_cifar-10.pth.tar'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1699568580741,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"},"user_tz":300},"id":"KRGbc0O0Xf3q","outputId":"cce5a3be-c54b-403b-e11f-0ed1651187fa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}],"source":["device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"markdown","metadata":{"id":"DPpaaGdwuzxj"},"source":["## Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xkcpUlHxuuxc"},"outputs":[],"source":["class CIFAR10RUC(torchdivision_datasets.CIFAR10):\n","    def __init__(self, root, transform, transform2, transform3, transform4=None, target_transform=None,train=True, download = False):\n","        self.root = root\n","        self.train = train  # training set or test set\n","        self.transform = transform\n","        self.transform2 = transform2\n","        self.transform3 = transform3\n","        self.transform4 = transform4\n","\n","        if download:\n","            self.download()\n","        # self.download()\n","\n","        if not self._check_integrity():\n","            raise RuntimeError('Dataset not found or corrupted.' +\n","                               ' You can use download=True to download it')\n","\n","        if self.train:\n","            downloaded_list = self.train_list\n","        else:\n","            downloaded_list = self.test_list\n","\n","        self.data = []\n","        self.targets = []\n","\n","        # now load the picked numpy arrays\n","        for file_name, checksum in downloaded_list:\n","            file_path = os.path.join(self.root, self.base_folder, file_name)\n","            with open(file_path, 'rb') as f:\n","                entry = pickle.load(f, encoding='latin1')\n","                self.data.append(entry['data'])\n","                if 'labels' in entry:\n","                    self.targets.extend(entry['labels'])\n","                else:\n","                    self.targets.extend(entry['fine_labels'])\n","\n","        self.data = np.vstack(self.data).reshape(-1, 3, 32, 32)\n","        self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n","        self._load_meta()\n","\n","    def __getitem__(self, index) :\n","        img, target = self.data[index], self.targets[index]\n","        img = Image.fromarray(img)\n","\n","        if self.transform is not None:\n","            img1 = self.transform(img)\n","            img2 = self.transform2(img)\n","            img3 = self.transform3(img)\n","\n","        if self.transform4 != None:\n","            img4 = self.transform4(img)\n","            return img1, img2, img3, img4, target, index\n","        else:\n","            return img1, img2, img3, target, index\n","\n","        return img1, img2, img3, target, index"]},{"cell_type":"markdown","metadata":{"id":"oZo4L0XNr77g"},"source":["## Preprocess function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ApBxew3dY1DL"},"outputs":[],"source":["def preprocess():\n","  mean = (0.4914, 0.4822, 0.4465)\n","  std = (0.2023, 0.1994, 0.2010)\n","  transform_train = transforms.Compose([\n","      transforms.RandomResizedCrop(size=32, scale=(0.2,1.)),\n","      transforms.RandomHorizontalFlip(),\n","      transforms.ToTensor(),\n","      transforms.Normalize(mean=mean, std=std),\n","  ])\n","\n","  transform_test = transforms.Compose([\n","      transforms.Resize(32),\n","      transforms.CenterCrop(32),\n","      transforms.ToTensor(),\n","      transforms.Normalize(mean=mean, std=std),\n","  ])\n","\n","  transform_strong = transforms.Compose([\n","      transforms.RandomResizedCrop(size=32, scale=(0.2,1.)),\n","      transforms.RandomHorizontalFlip(),\n","      RandAugmentMC(n=2, m=2),\n","      transforms.ToTensor(),\n","      transforms.Normalize(mean=mean, std=std)\n","  ])\n","\n","  trainset = CIFAR10RUC(root=\"./data\", transform=transform_test, transform2=transform_train, transform3=transform_train, transform4=transform_strong, download=False)\n","  trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4,  drop_last=False)\n","  testset = CIFAR10RUC(root=\"./data\", transform=transform_test, transform2=transform_test, transform3=transform_test, download=False)\n","  evalloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=4)\n","\n","  return trainset, trainloader, testset, evalloader, 10\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QxUueHR8WKGn"},"outputs":[],"source":["trainset, trainloader, testset, evalloader, class_num = preprocess()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZ9EDgomp6sq"},"outputs":[],"source":["net = ClusteringModel(Resnet_CIFAR(), class_num)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XORkpf1Z-Yop"},"outputs":[],"source":["net2 = copy.deepcopy(net)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z5tdnaqh-uD8"},"outputs":[],"source":["net_uc = copy.deepcopy(net)"]},{"cell_type":"markdown","metadata":{"id":"UE9JsjtrcLBy"},"source":["##Create Contrastive model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RjrgYSx0-yLL"},"outputs":[],"source":["net_embd = ContrastiveModel(Resnet_CIFAR())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RgxfkTJTAgER"},"outputs":[],"source":["try:\n","  state_dict = torch.load(o_model)\n","  state_dict2 = torch.load(e_model)\n","  net_uc.load_state_dict(state_dict)\n","  net_embd.load_state_dict(state_dict2, strict = True)\n","  net.load_state_dict(state_dict, strict = False)\n","  net2.load_state_dict(state_dict, strict = False)\n","  net.cluster_head = nn.ModuleList([nn.Linear(512, class_num) for _ in range(1)])\n","  net2.cluster_head = nn.ModuleList([nn.Linear(512, class_num) for _ in range(1)])\n","except:\n","  print(\"Check Model Directory!\")\n","  # exit(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zqQEu7ueHlGu"},"outputs":[],"source":["if device == 'cuda':\n","  net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n","  net2 = torch.nn.DataParallel(net2, device_ids=range(torch.cuda.device_count()))\n","  net_uc = torch.nn.DataParallel(net_uc, device_ids=range(torch.cuda.device_count()))\n","  net_embd = torch.nn.DataParallel(net_embd, device_ids=range(torch.cuda.device_count()))\n","  cudnn.benchmark = True"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1699568584711,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"},"user_tz":300},"id":"h6UJIUlI29In","outputId":"6ad497de-4f09-44f1-fe78-f22ad67e41b5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataParallel(\n","  (module): ContrastiveModel(\n","    (backbone): ResNet(\n","      (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (layer1): Sequential(\n","        (0): BasicBlock(\n","          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (shortcut): Sequential()\n","        )\n","        (1): BasicBlock(\n","          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (shortcut): Sequential()\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): BasicBlock(\n","          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (shortcut): Sequential(\n","            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): BasicBlock(\n","          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (shortcut): Sequential()\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): BasicBlock(\n","          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (shortcut): Sequential(\n","            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): BasicBlock(\n","          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (shortcut): Sequential()\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): BasicBlock(\n","          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (shortcut): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): BasicBlock(\n","          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (shortcut): Sequential()\n","        )\n","      )\n","      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    )\n","    (contrastive_head): Sequential(\n","      (0): Linear(in_features=512, out_features=512, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=512, out_features=128, bias=True)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":28}],"source":["net.to(device)\n","net2.to(device)\n","net_uc.to(device)\n","net_embd.to(device)"]},{"cell_type":"markdown","metadata":{"id":"lfL9n_Q2UVR3"},"source":["##linear_rampup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LkyMcx8DUUKW"},"outputs":[],"source":["def linear_rampup(current, rampup_length=200):\n","    if rampup_length == 0:\n","        return 1.0\n","    else:\n","        current = np.clip((current) / rampup_length, 0.1, 1.0)\n","        return float(current)"]},{"cell_type":"markdown","metadata":{"id":"B2qhRZOOJ_JG"},"source":["## Criterion_rb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x2IQyLg1KCPZ"},"outputs":[],"source":["class criterion_rb(object):\n","    def __call__(self, outputs_x, targets_x, outputs_u, targets_u, epoch):\n","        # Clean sample Loss\n","        probs_u = torch.softmax(outputs_u, dim=1)\n","        Lx = -torch.mean(torch.sum(F.log_softmax(outputs_x, dim=1) * targets_x, dim=1))\n","        Lu = 25*torch.mean((probs_u - targets_u)**2)\n","        Lu = linear_rampup(epoch) * Lu\n","        return Lx, Lu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L4c7hw9r4_mT"},"outputs":[],"source":["optimizer1 = torch.optim.SGD(net.parameters(), lr, momentum=momentum, weight_decay=weight_decay, nesterov=True)\n","optimizer2 = torch.optim.SGD(net2.parameters(), lr, momentum=momentum, weight_decay=weight_decay, nesterov=True)\n","criterion = criterion_rb()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2MkwNmGl5JIO"},"outputs":[],"source":["# Extract Pseudo Label\n","acc_uc, p_label = test(net_uc, evalloader, device, class_num)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1699568596696,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"},"user_tz":300},"id":"-r0yd4u0HgD-","outputId":"e2c397bf-de3a-4156-b6b4-04bdc650796a"},"outputs":[{"output_type":"stream","name":"stdout","text":["88.682\n"]}],"source":["print(acc_uc)"]},{"cell_type":"markdown","metadata":{"id":"baW5f_SpHqX2"},"source":["##extract_confidence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7-tfDumaHsdQ"},"outputs":[],"source":["def extract_confidence(net, p_label, evalloader, threshold):\n","  net.eval()\n","  devide = torch.tensor([]).cuda()\n","  clean_num = 0\n","  correct_num = 0\n","  for batch_idx, (inputs1, _, _, targets, indexes) in enumerate(evalloader):\n","    inputs1, targets = inputs1.cuda(), targets.cuda().float()\n","    labels = p_label[indexes].float()\n","    logits = net(inputs1)\n","    prob = torch.softmax(logits.detach_(), dim=-1)\n","    max_probs, _ = torch.max(prob, dim=-1)\n","    mask = max_probs.ge(threshold).float()\n","    devide = torch.cat([devide, mask])\n","    s_idx = (mask == 1)\n","    clean_num += labels[s_idx].shape[0]\n","    correct_num += torch.sum((labels[s_idx] == targets[s_idx])).item()\n","\n","  print(correct_num, clean_num)\n","  return devide"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10824,"status":"ok","timestamp":1699568607512,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"},"user_tz":300},"id":"ARczH_rEHgwq","outputId":"9111247e-9b13-467d-948a-be928aaf254d"},"outputs":[{"output_type":"stream","name":"stdout","text":["41469 44776\n"]}],"source":["devide1 = extract_confidence(net_uc, p_label, evalloader, s_thr)"]},{"cell_type":"markdown","metadata":{"id":"5iF0B3H_K804"},"source":["##extract_metric"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N1wM7tEKK-CA"},"outputs":[],"source":["def extract_metric(net, p_label, evalloader, n_num):\n","    net.eval()\n","    feature_bank = []\n","    with torch.no_grad():\n","        for batch_idx, (inputs1 , _, _, _, indexes) in enumerate(evalloader):\n","            out = net(inputs1.cuda())\n","            feature_bank.append(out)\n","        feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()\n","        sim_indices_list = []\n","        for batch_idx, (inputs1 , _, _, _, indexes) in enumerate(evalloader):\n","            out = net(inputs1.cuda(non_blocking=True))\n","            sim_matrix = torch.mm(out, feature_bank)\n","            _, sim_indices = sim_matrix.topk(k=n_num, dim=-1)\n","            sim_indices_list.append(sim_indices)\n","        feature_labels = p_label.cuda()\n","        first = True\n","        count = 0\n","        clean_num = 0\n","        correct_num = 0\n","        for batch_idx, (inputs1 , _, _, targets, indexes) in enumerate(evalloader):\n","            labels = p_label[indexes].cuda().long()\n","            sim_indices = sim_indices_list[count].cuda()\n","            sim_labels = torch.gather(feature_labels.expand(inputs1.size(0), -1), dim=-1, index=sim_indices)\n","            # counts for each class\n","            one_hot_label = torch.zeros(inputs1.size(0) * sim_indices.size(1), 10).cuda()\n","            one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1).long(), value=1.0)\n","            pred_scores = torch.sum(one_hot_label.view(inputs1.size(0), -1, 10), dim=1)\n","            count += 1\n","            pred_labels = pred_scores.argsort(dim=-1, descending=True)\n","            prob, _ = torch.max(F.softmax(pred_scores, dim=-1), 1)\n","            # Check whether prediction and current label are same\n","            noisy_label = labels\n","            s_idx1 = (pred_labels[:, :1].float() == labels.unsqueeze(dim=-1).float()).any(dim=-1).float()\n","            s_idx = (s_idx1 == 1.0)\n","            clean_num += labels[s_idx].shape[0]\n","            correct_num += torch.sum((labels[s_idx].float() == targets[s_idx.to(targets.device)].cuda().float())).item()\n","\n","            if first:\n","                prob_set = prob\n","                pred_same_label_set = s_idx\n","                first = False\n","            else:\n","                prob_set = torch.cat((prob_set, prob), dim = 0)\n","                pred_same_label_set = torch.cat((pred_same_label_set, s_idx), dim = 0)\n","\n","        print(correct_num, clean_num)\n","        return pred_same_label_set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31065,"status":"ok","timestamp":1699568638568,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"},"user_tz":300},"id":"WS8Jx6SPKhl3","outputId":"b23d03b7-2d83-4b04-e8c1-74cf467f61d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["41338 45116\n"]}],"source":["devide2 = extract_metric(net_embd, p_label, evalloader, n_num)"]},{"cell_type":"markdown","metadata":{"id":"5spJ_yJraRsG"},"source":["##extract_hybrid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K6SG22DoaSo0"},"outputs":[],"source":["def extract_hybrid(devide1, devide2, p_label, evalloader):\n","    devide = (devide1.float() + devide2.float() == 2)\n","    clean_num = 0\n","    correct_num = 0\n","    for batch_idx, (inputs1, _, _, targets, indexes) in enumerate(evalloader):\n","        inputs1, targets = inputs1.cuda(), targets.cuda().float()\n","        labels = p_label[indexes].float()\n","        mask = devide[indexes]\n","        s_idx = (mask == 1)\n","        clean_num += labels[s_idx].shape[0]\n","        correct_num += torch.sum((labels[s_idx] == targets[s_idx])).item()\n","\n","    print(correct_num, clean_num)\n","    return devide"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10567,"status":"ok","timestamp":1699568649132,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"},"user_tz":300},"id":"Ixn65rgzaLiI","outputId":"0b434c1d-7ef9-477f-b605-700bc45f35b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["39579 42235\n"]}],"source":["devide = extract_hybrid(devide1, devide2, p_label, evalloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"79xfSsyRa4TK"},"outputs":[],"source":["conf1 =  torch.zeros(50000)\n","conf2 =  torch.zeros(50000)"]},{"cell_type":"markdown","metadata":{"id":"jCj_QtWltvXs"},"source":["##LabelSmoothLoss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jkA8wUbGt3E6"},"outputs":[],"source":["class LabelSmoothLoss(nn.Module):\n","\n","    def __init__(self, smoothing=0.0):\n","        super(LabelSmoothLoss, self).__init__()\n","        self.smoothing = smoothing\n","\n","    def forward(self, input, target):\n","        log_prob = F.log_softmax(input, dim=-1)\n","        weight = input.new_ones(input.size()) * \\\n","            self.smoothing / (input.size(-1) - 1.)\n","        weight.scatter_(-1, target.unsqueeze(-1).long(), (1. - self.smoothing))\n","        loss = (-weight * log_prob).sum(dim=-1).mean()\n","        return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"POHnsp4Ixnbi"},"outputs":[],"source":["# LSloss = LabelSmoothLoss(nn.Module)\n","LSloss = LabelSmoothLoss(smoothing=0.1)"]},{"cell_type":"markdown","metadata":{"id":"z7RuGjzscVG9"},"source":["##train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Papqf42JeFNK"},"outputs":[],"source":["def adjust_learning_rate(lr, epochs, optimizer, epoch):\n","    # cosine learning rate schedule\n","    lr = lr\n","    lr *= 0.5 * (1. + math.cos(math.pi * epoch / epochs))\n","\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5PZr2RJcTkq"},"outputs":[],"source":["def train(epoch, net, net2, trainloader, optimizer, criterion_rb, devide, p_label, conf, batch_size):\n","    train_loss = AverageMeter()\n","    net.train()\n","    net2.train()\n","\n","    num_iter = (len(trainloader.dataset)//batch_size)+1\n","    # adjust learning rate\n","    adjust_learning_rate(lr, epochs, optimizer, epoch)\n","    optimizer.zero_grad()\n","    correct_u = 0\n","    unsupervised = 0\n","    conf_self = torch.zeros(50000)\n","    for batch_idx, (inputs1 , inputs2, inputs3, inputs4, targets, indexes) in enumerate(trainloader):\n","        inputs1, inputs2, inputs3, inputs4, targets = inputs1.float().cuda(), inputs2.float().cuda(), inputs3.float().cuda(), inputs4.float().cuda(), targets.cuda().long()\n","        s_idx = (devide[indexes] == 1)\n","        u_idx = (devide[indexes] == 0)\n","        labels = p_label[indexes].cuda().long()\n","        labels_x = torch.tensor(p_label[indexes][s_idx]).squeeze().long().cpu()\n","        target_x = torch.zeros(labels_x.shape[0], 10).scatter_(1, labels_x.view(-1,1), 1).float().cuda()\n","\n","        logit_o, logit_w1, logit_w2, logit_s = net(inputs1), net(inputs2), net(inputs3), net(inputs4)\n","        logit_s = logit_s[s_idx]\n","        max_probs, _ = torch.max(torch.softmax(logit_o, dim=1), dim=-1)\n","        conf_self[indexes] = max_probs.detach().cpu()\n","        optimizer.zero_grad()\n","\n","\n","        with torch.no_grad():\n","            # compute guessed labels of unlabel samples\n","            outputs_u11 = logit_w1[u_idx]\n","            outputs_u21  = logit_w2[u_idx]\n","            logit_o2 = net2(inputs1)\n","            logit_w12 = net2(inputs2)\n","            logit_w22 = net2(inputs3)\n","            outputs_u12 = logit_w12[u_idx]\n","            outputs_u22  = logit_w22[u_idx]\n","            pu = (torch.softmax(outputs_u11, dim=1) + torch.softmax(outputs_u21, dim=1) + torch.softmax(outputs_u12, dim=1) + torch.softmax(outputs_u22, dim=1)) / 4\n","            ptu = pu**(1/0.5) # temparature sharpening\n","            target_u = ptu / ptu.sum(dim=1, keepdim=True) # normalize\n","            target_u = target_u.detach().float()\n","\n","            px = torch.softmax(logit_o2[s_idx], dim=1) #+ torch.softmax(logit_w22[s_idx], dim=1)) / 2\n","\n","            # print('indexes value: ', indexes)\n","            indexes = indexes.cuda()\n","            # print(indexes.device)\n","            # print('conf value: ', conf)\n","            conf = conf.cuda()\n","            # print(conf.device)\n","            w_x = conf[indexes][s_idx]\n","\n","\n","\n","            w_x = w_x.view(-1,1).float().cuda()\n","            px = (1-w_x)*target_x + w_x*px\n","            ptx = px**(1/0.5) # temparature sharpening\n","            target_x = ptx / ptx.sum(dim=1, keepdim=True) # normalize\n","            target_x = target_x.detach().float()\n","\n","            if logit_o2[u_idx].shape[0] > 0:\n","                max_probs, targets_u1 = torch.max(torch.softmax(logit_o[u_idx], dim=1), dim=-1)\n","                mask_u = max_probs.ge(0.99).float()\n","                u_idx2 = (mask_u == 1)\n","                unsupervised += torch.sum(mask_u).item()\n","                correct_u += torch.sum((targets_u1[u_idx2] == targets[u_idx][u_idx2])).item()\n","                update = indexes[u_idx][u_idx2]\n","                devide[update] = True\n","                p_label[update] = targets_u1[u_idx2].float()\n","\n","\n","        l = np.random.beta(4.0, 4.0)\n","        l = max(l, 1-l)\n","\n","        all_inputs = torch.cat([inputs2[s_idx], inputs3[s_idx], inputs2[u_idx], inputs3[u_idx]],dim=0)\n","        all_targets = torch.cat([target_x, target_x, target_u, target_u], dim=0)\n","        idx = torch.randperm(all_inputs.size(0))\n","\n","        input_a, input_b = all_inputs, all_inputs[idx]\n","        target_a, target_b = all_targets, all_targets[idx]\n","\n","        mixed_input = l * input_a + (1 - l) * input_b\n","        mixed_target = l * target_a + (1 - l) * target_b\n","\n","        logits = net(mixed_input)\n","        batch_size = target_x.shape[0]\n","\n","        Lx, Lu = criterion_rb(logits[:batch_size*2], mixed_target[:batch_size*2], logits[batch_size*2:], mixed_target[batch_size*2:], epoch+batch_idx/num_iter)\n","        total_loss = Lx + Lu + LSloss(logit_s, labels_x.cuda())\n","\n","        total_loss.backward()\n","        train_loss.update(total_loss.item(), inputs2.size(0))\n","        optimizer.step()\n","\n","        if batch_idx % 100 == 0:\n","            print('Epoch: [{epoch}][{elps_iters}/{tot_iters}] '\n","                  'Train loss: {train_loss.val:.4f} ({train_loss.avg:.4f}) '.format(\n","                      epoch=epoch, elps_iters=batch_idx,tot_iters=len(trainloader),\n","                      train_loss=train_loss))\n","    conf_self = (conf_self - conf_self.min()) / (conf_self.max() - conf_self.min())\n","\n","    return train_loss.avg, devide, p_label, conf_self"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TDwG6G3EbBnz","executionInfo":{"status":"ok","timestamp":1699581996446,"user_tz":300,"elapsed":13347324,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}},"outputId":"5ba7592e-d567-47fa-ade6-a93a11fefc97"},"outputs":[{"output_type":"stream","name":"stdout","text":["== Train RUC ==\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-44-71431a0d55ab>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels_x = torch.tensor(p_label[indexes][s_idx]).squeeze().long().cpu()\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [0][0/200] Train loss: 4.6494 (4.6494) \n","Epoch: [0][100/200] Train loss: 2.6825 (2.8252) \n","Epoch: [0][0/200] Train loss: 4.6876 (4.6876) \n","Epoch: [0][100/200] Train loss: 2.1146 (2.8028) \n","accuracy: [87.378, 87.78, 87.874]\n","\n","== Train RUC ==\n","Epoch: [1][0/200] Train loss: 2.2741 (2.2741) \n","Epoch: [1][100/200] Train loss: 2.5727 (2.4463) \n","Epoch: [1][0/200] Train loss: 2.6317 (2.6317) \n","Epoch: [1][100/200] Train loss: 2.5795 (2.4587) \n","accuracy: [87.658, 87.768, 87.922]\n","\n","== Train RUC ==\n","Epoch: [2][0/200] Train loss: 1.9196 (1.9196) \n","Epoch: [2][100/200] Train loss: 2.2242 (2.3714) \n","Epoch: [2][0/200] Train loss: 2.5552 (2.5552) \n","Epoch: [2][100/200] Train loss: 2.4030 (2.4149) \n","accuracy: [87.764, 87.874, 88.01]\n","\n","== Train RUC ==\n","Epoch: [3][0/200] Train loss: 2.4753 (2.4753) \n","Epoch: [3][100/200] Train loss: 2.4501 (2.3189) \n","Epoch: [3][0/200] Train loss: 1.9092 (1.9092) \n","Epoch: [3][100/200] Train loss: 2.5604 (2.3299) \n","accuracy: [87.726, 87.888, 87.976]\n","\n","== Train RUC ==\n","Epoch: [4][0/200] Train loss: 2.4512 (2.4512) \n","Epoch: [4][100/200] Train loss: 1.6545 (2.3325) \n","Epoch: [4][0/200] Train loss: 2.3669 (2.3669) \n","Epoch: [4][100/200] Train loss: 2.1121 (2.3099) \n","accuracy: [87.872, 87.97, 88.16000000000001]\n","\n","== Train RUC ==\n","Epoch: [5][0/200] Train loss: 2.3570 (2.3570) \n","Epoch: [5][100/200] Train loss: 2.3820 (2.2858) \n","Epoch: [5][0/200] Train loss: 2.4510 (2.4510) \n","Epoch: [5][100/200] Train loss: 2.2586 (2.2726) \n","accuracy: [87.75, 87.942, 88.056]\n","\n","== Train RUC ==\n","Epoch: [6][0/200] Train loss: 2.5085 (2.5085) \n","Epoch: [6][100/200] Train loss: 2.3362 (2.3033) \n","Epoch: [6][0/200] Train loss: 2.0532 (2.0532) \n","Epoch: [6][100/200] Train loss: 2.4508 (2.2348) \n","accuracy: [87.97399999999999, 88.128, 88.28200000000001]\n","\n","== Train RUC ==\n","Epoch: [7][0/200] Train loss: 2.2385 (2.2385) \n","Epoch: [7][100/200] Train loss: 2.1730 (2.2756) \n","Epoch: [7][0/200] Train loss: 2.3981 (2.3981) \n","Epoch: [7][100/200] Train loss: 2.4144 (2.2540) \n","accuracy: [87.938, 88.242, 88.294]\n","\n","== Train RUC ==\n","Epoch: [8][0/200] Train loss: 2.3871 (2.3871) \n","Epoch: [8][100/200] Train loss: 2.4008 (2.2633) \n","Epoch: [8][0/200] Train loss: 1.9161 (1.9161) \n","Epoch: [8][100/200] Train loss: 2.3135 (2.2103) \n","accuracy: [87.89, 88.138, 88.28200000000001]\n","\n","== Train RUC ==\n","Epoch: [9][0/200] Train loss: 2.4490 (2.4490) \n","Epoch: [9][100/200] Train loss: 1.6832 (2.2676) \n","Epoch: [9][0/200] Train loss: 2.3471 (2.3471) \n","Epoch: [9][100/200] Train loss: 2.2766 (2.2371) \n","accuracy: [88.006, 88.036, 88.244]\n","\n","== Train RUC ==\n","Epoch: [10][0/200] Train loss: 1.7917 (1.7917) \n","Epoch: [10][100/200] Train loss: 2.5008 (2.2006) \n","Epoch: [10][0/200] Train loss: 2.1225 (2.1225) \n","Epoch: [10][100/200] Train loss: 2.2897 (2.2030) \n","accuracy: [87.85, 88.25, 88.348]\n","\n","== Train RUC ==\n","Epoch: [11][0/200] Train loss: 2.2750 (2.2750) \n","Epoch: [11][100/200] Train loss: 1.7924 (2.2039) \n","Epoch: [11][0/200] Train loss: 2.3699 (2.3699) \n","Epoch: [11][100/200] Train loss: 2.1148 (2.2360) \n","accuracy: [88.034, 88.318, 88.398]\n","\n","== Train RUC ==\n","Epoch: [12][0/200] Train loss: 2.1609 (2.1609) \n","Epoch: [12][100/200] Train loss: 2.3174 (2.2319) \n","Epoch: [12][0/200] Train loss: 2.3128 (2.3128) \n","Epoch: [12][100/200] Train loss: 2.3822 (2.2179) \n","accuracy: [87.964, 88.114, 88.32799999999999]\n","\n","== Train RUC ==\n","Epoch: [13][0/200] Train loss: 2.1323 (2.1323) \n","Epoch: [13][100/200] Train loss: 2.0941 (2.1961) \n","Epoch: [13][0/200] Train loss: 2.2531 (2.2531) \n","Epoch: [13][100/200] Train loss: 2.3667 (2.2102) \n","accuracy: [87.824, 88.356, 88.322]\n","\n","== Train RUC ==\n","Epoch: [14][0/200] Train loss: 2.2366 (2.2366) \n","Epoch: [14][100/200] Train loss: 2.2134 (2.2074) \n","Epoch: [14][0/200] Train loss: 2.4082 (2.4082) \n","Epoch: [14][100/200] Train loss: 2.3104 (2.2266) \n","accuracy: [88.248, 88.152, 88.44800000000001]\n","\n","== Train RUC ==\n","Epoch: [15][0/200] Train loss: 2.2258 (2.2258) \n","Epoch: [15][100/200] Train loss: 2.0772 (2.2266) \n","Epoch: [15][0/200] Train loss: 2.1858 (2.1858) \n","Epoch: [15][100/200] Train loss: 2.2322 (2.1448) \n","accuracy: [88.004, 88.17399999999999, 88.346]\n","\n","== Train RUC ==\n","Epoch: [16][0/200] Train loss: 1.9952 (1.9952) \n","Epoch: [16][100/200] Train loss: 2.2477 (2.1985) \n","Epoch: [16][0/200] Train loss: 1.8615 (1.8615) \n","Epoch: [16][100/200] Train loss: 2.0998 (2.1971) \n","accuracy: [88.03999999999999, 88.44999999999999, 88.476]\n","\n","== Train RUC ==\n","Epoch: [17][0/200] Train loss: 2.3188 (2.3188) \n","Epoch: [17][100/200] Train loss: 2.1638 (2.1955) \n","Epoch: [17][0/200] Train loss: 2.1977 (2.1977) \n","Epoch: [17][100/200] Train loss: 2.2601 (2.1930) \n","accuracy: [88.268, 88.124, 88.374]\n","\n","== Train RUC ==\n","Epoch: [18][0/200] Train loss: 2.1721 (2.1721) \n","Epoch: [18][100/200] Train loss: 2.1266 (2.1909) \n","Epoch: [18][0/200] Train loss: 2.3172 (2.3172) \n","Epoch: [18][100/200] Train loss: 1.7419 (2.1532) \n","accuracy: [88.01400000000001, 88.34, 88.456]\n","\n","== Train RUC ==\n","Epoch: [19][0/200] Train loss: 2.2267 (2.2267) \n","Epoch: [19][100/200] Train loss: 2.3604 (2.1767) \n","Epoch: [19][0/200] Train loss: 2.1549 (2.1549) \n","Epoch: [19][100/200] Train loss: 1.8606 (2.1645) \n","accuracy: [88.08200000000001, 88.31, 88.482]\n","\n","== Train RUC ==\n","Epoch: [20][0/200] Train loss: 2.0920 (2.0920) \n","Epoch: [20][100/200] Train loss: 2.3149 (2.1597) \n","Epoch: [20][0/200] Train loss: 2.1727 (2.1727) \n","Epoch: [20][100/200] Train loss: 2.2409 (2.1735) \n","accuracy: [88.358, 88.386, 88.63799999999999]\n","\n","== Train RUC ==\n","Epoch: [21][0/200] Train loss: 2.4780 (2.4780) \n","Epoch: [21][100/200] Train loss: 2.2283 (2.1774) \n","Epoch: [21][0/200] Train loss: 2.3487 (2.3487) \n","Epoch: [21][100/200] Train loss: 2.2500 (2.1520) \n","accuracy: [88.176, 88.498, 88.568]\n","\n","== Train RUC ==\n","Epoch: [22][0/200] Train loss: 2.0970 (2.0970) \n","Epoch: [22][100/200] Train loss: 1.8267 (2.1874) \n","Epoch: [22][0/200] Train loss: 2.1882 (2.1882) \n","Epoch: [22][100/200] Train loss: 2.2697 (2.1684) \n","accuracy: [88.372, 88.53, 88.698]\n","\n","== Train RUC ==\n","Epoch: [23][0/200] Train loss: 1.9937 (1.9937) \n","Epoch: [23][100/200] Train loss: 2.2515 (2.1660) \n","Epoch: [23][0/200] Train loss: 2.2886 (2.2886) \n","Epoch: [23][100/200] Train loss: 1.8640 (2.1701) \n","accuracy: [88.434, 88.426, 88.714]\n","\n","== Train RUC ==\n","Epoch: [24][0/200] Train loss: 2.2979 (2.2979) \n","Epoch: [24][100/200] Train loss: 2.1903 (2.1435) \n","Epoch: [24][0/200] Train loss: 2.0632 (2.0632) \n","Epoch: [24][100/200] Train loss: 2.0925 (2.1630) \n","accuracy: [88.508, 88.626, 88.79]\n","\n","== Train RUC ==\n","Epoch: [25][0/200] Train loss: 2.1514 (2.1514) \n","Epoch: [25][100/200] Train loss: 2.0611 (2.1817) \n","Epoch: [25][0/200] Train loss: 2.2260 (2.2260) \n","Epoch: [25][100/200] Train loss: 2.2623 (2.1939) \n","accuracy: [88.49000000000001, 88.39200000000001, 88.71]\n","\n","== Train RUC ==\n","Epoch: [26][0/200] Train loss: 2.1888 (2.1888) \n","Epoch: [26][100/200] Train loss: 2.2077 (2.1485) \n","Epoch: [26][0/200] Train loss: 2.2581 (2.2581) \n","Epoch: [26][100/200] Train loss: 2.2787 (2.1650) \n","accuracy: [88.38000000000001, 88.424, 88.714]\n","\n","== Train RUC ==\n","Epoch: [27][0/200] Train loss: 2.1628 (2.1628) \n","Epoch: [27][100/200] Train loss: 2.0212 (2.1703) \n","Epoch: [27][0/200] Train loss: 2.1803 (2.1803) \n","Epoch: [27][100/200] Train loss: 2.3104 (2.1904) \n","accuracy: [88.55, 88.51, 88.762]\n","\n","== Train RUC ==\n","Epoch: [28][0/200] Train loss: 2.2615 (2.2615) \n","Epoch: [28][100/200] Train loss: 2.0603 (2.1032) \n","Epoch: [28][0/200] Train loss: 2.2422 (2.2422) \n","Epoch: [28][100/200] Train loss: 2.1455 (2.1659) \n","accuracy: [88.21799999999999, 88.424, 88.668]\n","\n","== Train RUC ==\n","Epoch: [29][0/200] Train loss: 2.0022 (2.0022) \n","Epoch: [29][100/200] Train loss: 2.2054 (2.1109) \n","Epoch: [29][0/200] Train loss: 2.2211 (2.2211) \n","Epoch: [29][100/200] Train loss: 2.2329 (2.1409) \n","accuracy: [88.548, 88.492, 88.792]\n","\n","== Train RUC ==\n","Epoch: [30][0/200] Train loss: 2.2351 (2.2351) \n","Epoch: [30][100/200] Train loss: 2.1196 (2.1383) \n","Epoch: [30][0/200] Train loss: 2.2550 (2.2550) \n","Epoch: [30][100/200] Train loss: 2.2579 (2.1620) \n","accuracy: [88.488, 88.336, 88.564]\n","\n","== Train RUC ==\n","Epoch: [31][0/200] Train loss: 2.1498 (2.1498) \n","Epoch: [31][100/200] Train loss: 2.3057 (2.1698) \n","Epoch: [31][0/200] Train loss: 2.3434 (2.3434) \n","Epoch: [31][100/200] Train loss: 1.9989 (2.1607) \n","accuracy: [88.608, 88.542, 88.928]\n","\n","== Train RUC ==\n","Epoch: [32][0/200] Train loss: 2.1743 (2.1743) \n","Epoch: [32][100/200] Train loss: 2.1839 (2.1499) \n","Epoch: [32][0/200] Train loss: 2.2450 (2.2450) \n","Epoch: [32][100/200] Train loss: 2.0090 (2.1778) \n","accuracy: [88.34, 88.39, 88.644]\n","\n","== Train RUC ==\n","Epoch: [33][0/200] Train loss: 2.1536 (2.1536) \n","Epoch: [33][100/200] Train loss: 2.2151 (2.1563) \n","Epoch: [33][0/200] Train loss: 1.7195 (1.7195) \n","Epoch: [33][100/200] Train loss: 2.2461 (2.1433) \n","accuracy: [88.49000000000001, 88.724, 88.83]\n","\n","== Train RUC ==\n","Epoch: [34][0/200] Train loss: 2.2325 (2.2325) \n","Epoch: [34][100/200] Train loss: 2.1317 (2.1605) \n","Epoch: [34][0/200] Train loss: 2.2895 (2.2895) \n","Epoch: [34][100/200] Train loss: 2.2112 (2.1543) \n","accuracy: [88.39, 88.304, 88.63799999999999]\n","\n","== Train RUC ==\n","Epoch: [35][0/200] Train loss: 2.2719 (2.2719) \n","Epoch: [35][100/200] Train loss: 2.1911 (2.1346) \n","Epoch: [35][0/200] Train loss: 1.8084 (1.8084) \n","Epoch: [35][100/200] Train loss: 1.8496 (2.1760) \n","accuracy: [88.5, 88.536, 88.77199999999999]\n","\n","== Train RUC ==\n","Epoch: [36][0/200] Train loss: 2.2605 (2.2605) \n","Epoch: [36][100/200] Train loss: 1.9470 (2.1411) \n","Epoch: [36][0/200] Train loss: 2.1592 (2.1592) \n","Epoch: [36][100/200] Train loss: 1.7064 (2.1331) \n","accuracy: [88.506, 88.478, 88.72]\n","\n","== Train RUC ==\n","Epoch: [37][0/200] Train loss: 2.2237 (2.2237) \n","Epoch: [37][100/200] Train loss: 2.1325 (2.1309) \n","Epoch: [37][0/200] Train loss: 2.2709 (2.2709) \n","Epoch: [37][100/200] Train loss: 2.3107 (2.1402) \n","accuracy: [88.456, 88.57000000000001, 88.792]\n","\n","== Train RUC ==\n","Epoch: [38][0/200] Train loss: 2.3150 (2.3150) \n","Epoch: [38][100/200] Train loss: 2.1023 (2.1310) \n","Epoch: [38][0/200] Train loss: 2.2045 (2.2045) \n","Epoch: [38][100/200] Train loss: 2.2006 (2.1443) \n","accuracy: [88.506, 88.726, 88.866]\n","\n","== Train RUC ==\n","Epoch: [39][0/200] Train loss: 2.2268 (2.2268) \n","Epoch: [39][100/200] Train loss: 2.3067 (2.1509) \n","Epoch: [39][0/200] Train loss: 2.2153 (2.2153) \n","Epoch: [39][100/200] Train loss: 2.1143 (2.1294) \n","accuracy: [88.602, 88.6, 88.858]\n","\n","== Train RUC ==\n","Epoch: [40][0/200] Train loss: 2.0665 (2.0665) \n","Epoch: [40][100/200] Train loss: 2.0363 (2.1306) \n","Epoch: [40][0/200] Train loss: 1.8887 (1.8887) \n","Epoch: [40][100/200] Train loss: 2.1517 (2.1431) \n","accuracy: [88.778, 88.768, 89.044]\n","\n","== Train RUC ==\n","Epoch: [41][0/200] Train loss: 2.2621 (2.2621) \n","Epoch: [41][100/200] Train loss: 2.1188 (2.1384) \n","Epoch: [41][0/200] Train loss: 2.2304 (2.2304) \n","Epoch: [41][100/200] Train loss: 2.3025 (2.1640) \n","accuracy: [88.654, 88.59, 88.888]\n","\n","== Train RUC ==\n","Epoch: [42][0/200] Train loss: 2.2229 (2.2229) \n","Epoch: [42][100/200] Train loss: 2.1717 (2.1295) \n","Epoch: [42][0/200] Train loss: 2.2248 (2.2248) \n","Epoch: [42][100/200] Train loss: 2.3312 (2.1539) \n","accuracy: [88.756, 88.998, 89.12400000000001]\n","\n","== Train RUC ==\n","Epoch: [43][0/200] Train loss: 2.2216 (2.2216) \n","Epoch: [43][100/200] Train loss: 2.2699 (2.1457) \n","Epoch: [43][0/200] Train loss: 2.2324 (2.2324) \n","Epoch: [43][100/200] Train loss: 2.0140 (2.1499) \n","accuracy: [88.82, 88.66199999999999, 88.998]\n","\n","== Train RUC ==\n","Epoch: [44][0/200] Train loss: 1.6358 (1.6358) \n","Epoch: [44][100/200] Train loss: 2.2417 (2.1321) \n","Epoch: [44][0/200] Train loss: 2.1164 (2.1164) \n","Epoch: [44][100/200] Train loss: 2.1653 (2.1581) \n","accuracy: [88.696, 88.672, 88.924]\n","\n","== Train RUC ==\n","Epoch: [45][0/200] Train loss: 2.0345 (2.0345) \n","Epoch: [45][100/200] Train loss: 2.1695 (2.1424) \n","Epoch: [45][0/200] Train loss: 2.0962 (2.0962) \n","Epoch: [45][100/200] Train loss: 2.1277 (2.1424) \n","accuracy: [88.78200000000001, 88.774, 89.074]\n","\n","== Train RUC ==\n","Epoch: [46][0/200] Train loss: 2.1514 (2.1514) \n","Epoch: [46][100/200] Train loss: 1.6533 (2.1652) \n","Epoch: [46][0/200] Train loss: 2.0596 (2.0596) \n","Epoch: [46][100/200] Train loss: 2.0281 (2.1000) \n","accuracy: [88.72, 88.854, 89.062]\n","\n","== Train RUC ==\n","Epoch: [47][0/200] Train loss: 2.1995 (2.1995) \n","Epoch: [47][100/200] Train loss: 2.3109 (2.1490) \n","Epoch: [47][0/200] Train loss: 2.1436 (2.1436) \n","Epoch: [47][100/200] Train loss: 2.2787 (2.1092) \n","accuracy: [88.9, 88.682, 89.018]\n","\n","== Train RUC ==\n","Epoch: [48][0/200] Train loss: 2.2720 (2.2720) \n","Epoch: [48][100/200] Train loss: 2.3925 (2.1630) \n","Epoch: [48][0/200] Train loss: 2.1973 (2.1973) \n","Epoch: [48][100/200] Train loss: 2.3417 (2.1554) \n","accuracy: [88.916, 88.73400000000001, 89.122]\n","\n","== Train RUC ==\n","Epoch: [49][0/200] Train loss: 1.9141 (1.9141) \n","Epoch: [49][100/200] Train loss: 2.2602 (2.1471) \n","Epoch: [49][0/200] Train loss: 1.6892 (1.6892) \n","Epoch: [49][100/200] Train loss: 2.2074 (2.1592) \n","accuracy: [88.866, 88.80799999999999, 89.11200000000001]\n","\n","== Train RUC ==\n","Epoch: [50][0/200] Train loss: 2.1820 (2.1820) \n","Epoch: [50][100/200] Train loss: 2.4079 (2.1438) \n","Epoch: [50][0/200] Train loss: 2.2491 (2.2491) \n","Epoch: [50][100/200] Train loss: 1.8213 (2.1448) \n","accuracy: [88.75, 88.884, 89.00200000000001]\n","\n","== Train RUC ==\n","Epoch: [51][0/200] Train loss: 2.2156 (2.2156) \n","Epoch: [51][100/200] Train loss: 2.1412 (2.1450) \n","Epoch: [51][0/200] Train loss: 2.1461 (2.1461) \n","Epoch: [51][100/200] Train loss: 2.2321 (2.1605) \n","accuracy: [88.802, 88.672, 88.97]\n","\n","== Train RUC ==\n","Epoch: [52][0/200] Train loss: 2.1618 (2.1618) \n","Epoch: [52][100/200] Train loss: 2.2402 (2.1514) \n","Epoch: [52][0/200] Train loss: 2.2847 (2.2847) \n","Epoch: [52][100/200] Train loss: 2.1488 (2.1498) \n","accuracy: [88.84, 88.832, 89.048]\n","\n","== Train RUC ==\n","Epoch: [53][0/200] Train loss: 2.2083 (2.2083) \n","Epoch: [53][100/200] Train loss: 2.3658 (2.1832) \n","Epoch: [53][0/200] Train loss: 2.1597 (2.1597) \n","Epoch: [53][100/200] Train loss: 2.1084 (2.1581) \n","accuracy: [88.84, 88.92999999999999, 89.114]\n","\n","== Train RUC ==\n","Epoch: [54][0/200] Train loss: 2.1954 (2.1954) \n","Epoch: [54][100/200] Train loss: 2.3612 (2.1381) \n","Epoch: [54][0/200] Train loss: 2.2474 (2.2474) \n","Epoch: [54][100/200] Train loss: 2.1854 (2.1310) \n","accuracy: [88.78399999999999, 88.812, 88.974]\n","\n","== Train RUC ==\n","Epoch: [55][0/200] Train loss: 2.1863 (2.1863) \n","Epoch: [55][100/200] Train loss: 2.2688 (2.1596) \n","Epoch: [55][0/200] Train loss: 2.2935 (2.2935) \n","Epoch: [55][100/200] Train loss: 2.3484 (2.1782) \n","accuracy: [88.88000000000001, 88.748, 89.032]\n","\n","== Train RUC ==\n","Epoch: [56][0/200] Train loss: 2.2489 (2.2489) \n","Epoch: [56][100/200] Train loss: 2.0590 (2.1548) \n","Epoch: [56][0/200] Train loss: 1.7803 (1.7803) \n","Epoch: [56][100/200] Train loss: 2.2405 (2.1455) \n","accuracy: [88.858, 88.922, 89.088]\n","\n","== Train RUC ==\n","Epoch: [57][0/200] Train loss: 2.0812 (2.0812) \n","Epoch: [57][100/200] Train loss: 1.7432 (2.1387) \n","Epoch: [57][0/200] Train loss: 2.1445 (2.1445) \n","Epoch: [57][100/200] Train loss: 2.1807 (2.1405) \n","accuracy: [88.964, 88.962, 89.184]\n","\n","== Train RUC ==\n","Epoch: [58][0/200] Train loss: 2.2460 (2.2460) \n","Epoch: [58][100/200] Train loss: 2.0336 (2.1421) \n","Epoch: [58][0/200] Train loss: 2.3846 (2.3846) \n","Epoch: [58][100/200] Train loss: 2.2086 (2.1589) \n","accuracy: [89.00200000000001, 89.05999999999999, 89.21]\n","\n","== Train RUC ==\n","Epoch: [59][0/200] Train loss: 2.2918 (2.2918) \n","Epoch: [59][100/200] Train loss: 1.4529 (2.1628) \n","Epoch: [59][0/200] Train loss: 2.0494 (2.0494) \n","Epoch: [59][100/200] Train loss: 2.0422 (2.1603) \n","accuracy: [88.776, 89.142, 89.19399999999999]\n","\n","== Train RUC ==\n","Epoch: [60][0/200] Train loss: 1.7872 (1.7872) \n","Epoch: [60][100/200] Train loss: 2.0520 (2.1535) \n","Epoch: [60][0/200] Train loss: 1.9911 (1.9911) \n","Epoch: [60][100/200] Train loss: 2.2370 (2.1353) \n","accuracy: [88.94999999999999, 89.13799999999999, 89.27199999999999]\n","\n","== Train RUC ==\n","Epoch: [61][0/200] Train loss: 2.3086 (2.3086) \n","Epoch: [61][100/200] Train loss: 2.1442 (2.1575) \n","Epoch: [61][0/200] Train loss: 2.1830 (2.1830) \n","Epoch: [61][100/200] Train loss: 2.1906 (2.1635) \n","accuracy: [88.884, 88.966, 89.176]\n","\n","== Train RUC ==\n","Epoch: [62][0/200] Train loss: 2.1263 (2.1263) \n","Epoch: [62][100/200] Train loss: 2.3313 (2.1460) \n","Epoch: [62][0/200] Train loss: 1.6452 (1.6452) \n","Epoch: [62][100/200] Train loss: 1.9351 (2.1471) \n","accuracy: [89.048, 89.096, 89.27199999999999]\n","\n","== Train RUC ==\n","Epoch: [63][0/200] Train loss: 2.2719 (2.2719) \n","Epoch: [63][100/200] Train loss: 2.2926 (2.1450) \n","Epoch: [63][0/200] Train loss: 2.3271 (2.3271) \n","Epoch: [63][100/200] Train loss: 1.9529 (2.1461) \n","accuracy: [89.03, 89.23, 89.384]\n","\n","== Train RUC ==\n","Epoch: [64][0/200] Train loss: 2.1304 (2.1304) \n","Epoch: [64][100/200] Train loss: 2.3785 (2.1692) \n","Epoch: [64][0/200] Train loss: 2.1736 (2.1736) \n","Epoch: [64][100/200] Train loss: 2.0752 (2.1543) \n","accuracy: [88.992, 89.032, 89.224]\n","\n","== Train RUC ==\n","Epoch: [65][0/200] Train loss: 2.1883 (2.1883) \n","Epoch: [65][100/200] Train loss: 2.1637 (2.1619) \n","Epoch: [65][0/200] Train loss: 2.2001 (2.2001) \n","Epoch: [65][100/200] Train loss: 2.0310 (2.1591) \n","accuracy: [89.114, 88.946, 89.28399999999999]\n","\n","== Train RUC ==\n","Epoch: [66][0/200] Train loss: 1.9053 (1.9053) \n","Epoch: [66][100/200] Train loss: 2.2279 (2.1640) \n","Epoch: [66][0/200] Train loss: 2.1704 (2.1704) \n","Epoch: [66][100/200] Train loss: 2.0155 (2.1401) \n","accuracy: [88.982, 89.066, 89.24600000000001]\n","\n","== Train RUC ==\n","Epoch: [67][0/200] Train loss: 2.2948 (2.2948) \n","Epoch: [67][100/200] Train loss: 2.2590 (2.1889) \n","Epoch: [67][0/200] Train loss: 2.3109 (2.3109) \n","Epoch: [67][100/200] Train loss: 2.3069 (2.1407) \n","accuracy: [89.068, 89.106, 89.3]\n","\n","== Train RUC ==\n","Epoch: [68][0/200] Train loss: 1.6710 (1.6710) \n","Epoch: [68][100/200] Train loss: 2.2631 (2.1365) \n","Epoch: [68][0/200] Train loss: 2.1377 (2.1377) \n","Epoch: [68][100/200] Train loss: 1.4813 (2.1534) \n","accuracy: [89.206, 89.228, 89.416]\n","\n","== Train RUC ==\n","Epoch: [69][0/200] Train loss: 2.2599 (2.2599) \n","Epoch: [69][100/200] Train loss: 2.2412 (2.1375) \n","Epoch: [69][0/200] Train loss: 2.2449 (2.2449) \n","Epoch: [69][100/200] Train loss: 2.2448 (2.1355) \n","accuracy: [89.142, 89.078, 89.312]\n","\n","== Train RUC ==\n","Epoch: [70][0/200] Train loss: 2.1715 (2.1715) \n","Epoch: [70][100/200] Train loss: 2.1188 (2.1565) \n","Epoch: [70][0/200] Train loss: 2.2220 (2.2220) \n","Epoch: [70][100/200] Train loss: 2.2424 (2.1808) \n","accuracy: [89.19200000000001, 89.068, 89.344]\n","\n","== Train RUC ==\n","Epoch: [71][0/200] Train loss: 2.1667 (2.1667) \n","Epoch: [71][100/200] Train loss: 2.2415 (2.1787) \n","Epoch: [71][0/200] Train loss: 1.8442 (1.8442) \n","Epoch: [71][100/200] Train loss: 2.1454 (2.1631) \n","accuracy: [89.03, 89.144, 89.306]\n","\n","== Train RUC ==\n","Epoch: [72][0/200] Train loss: 2.2383 (2.2383) \n","Epoch: [72][100/200] Train loss: 2.3842 (2.1538) \n","Epoch: [72][0/200] Train loss: 2.2871 (2.2871) \n","Epoch: [72][100/200] Train loss: 2.3355 (2.1572) \n","accuracy: [89.142, 89.092, 89.322]\n","\n","== Train RUC ==\n","Epoch: [73][0/200] Train loss: 2.2580 (2.2580) \n","Epoch: [73][100/200] Train loss: 2.3178 (2.1698) \n","Epoch: [73][0/200] Train loss: 2.2234 (2.2234) \n","Epoch: [73][100/200] Train loss: 2.0969 (2.1622) \n","accuracy: [89.328, 89.24, 89.442]\n","\n","== Train RUC ==\n","Epoch: [74][0/200] Train loss: 2.1646 (2.1646) \n","Epoch: [74][100/200] Train loss: 2.1593 (2.1800) \n","Epoch: [74][0/200] Train loss: 2.1498 (2.1498) \n","Epoch: [74][100/200] Train loss: 2.1457 (2.1576) \n","accuracy: [89.122, 88.964, 89.27000000000001]\n","\n","== Train RUC ==\n","Epoch: [75][0/200] Train loss: 1.8373 (1.8373) \n","Epoch: [75][100/200] Train loss: 1.9589 (2.1593) \n","Epoch: [75][0/200] Train loss: 2.1896 (2.1896) \n","Epoch: [75][100/200] Train loss: 2.0140 (2.1528) \n","accuracy: [89.13799999999999, 89.084, 89.322]\n","\n","== Train RUC ==\n","Epoch: [76][0/200] Train loss: 2.2854 (2.2854) \n","Epoch: [76][100/200] Train loss: 1.9667 (2.1478) \n","Epoch: [76][0/200] Train loss: 2.0729 (2.0729) \n","Epoch: [76][100/200] Train loss: 2.0654 (2.1476) \n","accuracy: [89.31400000000001, 89.24600000000001, 89.45]\n","\n","== Train RUC ==\n","Epoch: [77][0/200] Train loss: 2.1911 (2.1911) \n","Epoch: [77][100/200] Train loss: 2.1268 (2.1961) \n","Epoch: [77][0/200] Train loss: 1.9917 (1.9917) \n","Epoch: [77][100/200] Train loss: 2.1642 (2.1454) \n","accuracy: [89.274, 89.312, 89.51400000000001]\n","\n","== Train RUC ==\n","Epoch: [78][0/200] Train loss: 2.3063 (2.3063) \n","Epoch: [78][100/200] Train loss: 2.2349 (2.1749) \n","Epoch: [78][0/200] Train loss: 2.3462 (2.3462) \n","Epoch: [78][100/200] Train loss: 1.9167 (2.1393) \n","accuracy: [88.986, 89.256, 89.352]\n","\n","== Train RUC ==\n","Epoch: [79][0/200] Train loss: 2.1295 (2.1295) \n","Epoch: [79][100/200] Train loss: 2.1161 (2.1441) \n","Epoch: [79][0/200] Train loss: 2.1810 (2.1810) \n","Epoch: [79][100/200] Train loss: 2.3140 (2.1607) \n","accuracy: [89.084, 88.994, 89.29599999999999]\n","\n","== Train RUC ==\n","Epoch: [80][0/200] Train loss: 1.8750 (1.8750) \n","Epoch: [80][100/200] Train loss: 2.3339 (2.1763) \n","Epoch: [80][0/200] Train loss: 2.1646 (2.1646) \n","Epoch: [80][100/200] Train loss: 1.8866 (2.1611) \n","accuracy: [89.248, 89.08, 89.376]\n","\n","== Train RUC ==\n","Epoch: [81][0/200] Train loss: 2.2478 (2.2478) \n","Epoch: [81][100/200] Train loss: 2.1553 (2.1760) \n","Epoch: [81][0/200] Train loss: 2.3273 (2.3273) \n","Epoch: [81][100/200] Train loss: 2.2024 (2.1721) \n","accuracy: [89.216, 89.23, 89.432]\n","\n","== Train RUC ==\n","Epoch: [82][0/200] Train loss: 2.0137 (2.0137) \n","Epoch: [82][100/200] Train loss: 2.0867 (2.1605) \n","Epoch: [82][0/200] Train loss: 2.3869 (2.3869) \n","Epoch: [82][100/200] Train loss: 2.1335 (2.1526) \n","accuracy: [89.298, 89.132, 89.398]\n","\n","== Train RUC ==\n","Epoch: [83][0/200] Train loss: 2.0850 (2.0850) \n","Epoch: [83][100/200] Train loss: 2.1732 (2.1705) \n","Epoch: [83][0/200] Train loss: 2.3070 (2.3070) \n","Epoch: [83][100/200] Train loss: 2.2916 (2.1782) \n","accuracy: [89.33, 89.268, 89.408]\n","\n","== Train RUC ==\n","Epoch: [84][0/200] Train loss: 1.7112 (1.7112) \n","Epoch: [84][100/200] Train loss: 2.2237 (2.1646) \n","Epoch: [84][0/200] Train loss: 2.1786 (2.1786) \n","Epoch: [84][100/200] Train loss: 2.2641 (2.1634) \n","accuracy: [89.408, 89.14, 89.46600000000001]\n","\n","== Train RUC ==\n","Epoch: [85][0/200] Train loss: 1.6837 (1.6837) \n","Epoch: [85][100/200] Train loss: 2.2264 (2.1574) \n","Epoch: [85][0/200] Train loss: 2.2075 (2.2075) \n","Epoch: [85][100/200] Train loss: 2.1135 (2.1277) \n","accuracy: [89.4, 89.28399999999999, 89.468]\n","\n","== Train RUC ==\n","Epoch: [86][0/200] Train loss: 2.2524 (2.2524) \n","Epoch: [86][100/200] Train loss: 2.2588 (2.1875) \n","Epoch: [86][0/200] Train loss: 2.1407 (2.1407) \n","Epoch: [86][100/200] Train loss: 2.2325 (2.1725) \n","accuracy: [89.47800000000001, 89.47, 89.606]\n","\n","== Train RUC ==\n","Epoch: [87][0/200] Train loss: 2.1699 (2.1699) \n","Epoch: [87][100/200] Train loss: 2.1562 (2.1744) \n","Epoch: [87][0/200] Train loss: 2.2707 (2.2707) \n","Epoch: [87][100/200] Train loss: 2.0876 (2.1570) \n","accuracy: [89.30799999999999, 89.42, 89.548]\n","\n","== Train RUC ==\n","Epoch: [88][0/200] Train loss: 2.2397 (2.2397) \n","Epoch: [88][100/200] Train loss: 1.6759 (2.1618) \n","Epoch: [88][0/200] Train loss: 2.3414 (2.3414) \n","Epoch: [88][100/200] Train loss: 2.3057 (2.1640) \n","accuracy: [89.458, 89.53999999999999, 89.618]\n","\n","== Train RUC ==\n","Epoch: [89][0/200] Train loss: 2.1370 (2.1370) \n","Epoch: [89][100/200] Train loss: 2.2697 (2.1595) \n","Epoch: [89][0/200] Train loss: 1.5577 (1.5577) \n","Epoch: [89][100/200] Train loss: 2.0843 (2.1490) \n","accuracy: [89.404, 89.436, 89.56]\n","\n","== Train RUC ==\n","Epoch: [90][0/200] Train loss: 2.2572 (2.2572) \n","Epoch: [90][100/200] Train loss: 2.2429 (2.1426) \n","Epoch: [90][0/200] Train loss: 2.2524 (2.2524) \n","Epoch: [90][100/200] Train loss: 2.2178 (2.1619) \n","accuracy: [89.452, 89.462, 89.614]\n","\n","== Train RUC ==\n","Epoch: [91][0/200] Train loss: 2.1187 (2.1187) \n","Epoch: [91][100/200] Train loss: 2.1208 (2.1435) \n","Epoch: [91][0/200] Train loss: 2.2184 (2.2184) \n","Epoch: [91][100/200] Train loss: 2.2830 (2.1505) \n","accuracy: [89.358, 89.446, 89.544]\n","\n","== Train RUC ==\n","Epoch: [92][0/200] Train loss: 2.1045 (2.1045) \n","Epoch: [92][100/200] Train loss: 2.2569 (2.1766) \n","Epoch: [92][0/200] Train loss: 1.8320 (1.8320) \n","Epoch: [92][100/200] Train loss: 2.0420 (2.1603) \n","accuracy: [89.434, 89.55199999999999, 89.652]\n","\n","== Train RUC ==\n","Epoch: [93][0/200] Train loss: 2.1864 (2.1864) \n","Epoch: [93][100/200] Train loss: 2.1187 (2.1642) \n","Epoch: [93][0/200] Train loss: 2.1752 (2.1752) \n","Epoch: [93][100/200] Train loss: 2.2519 (2.1630) \n","accuracy: [89.38199999999999, 89.496, 89.566]\n","\n","== Train RUC ==\n","Epoch: [94][0/200] Train loss: 1.9258 (1.9258) \n","Epoch: [94][100/200] Train loss: 2.0755 (2.1548) \n","Epoch: [94][0/200] Train loss: 2.2567 (2.2567) \n","Epoch: [94][100/200] Train loss: 2.1226 (2.1786) \n","accuracy: [89.47, 89.574, 89.60000000000001]\n","\n","== Train RUC ==\n","Epoch: [95][0/200] Train loss: 1.8342 (1.8342) \n","Epoch: [95][100/200] Train loss: 2.2183 (2.1451) \n","Epoch: [95][0/200] Train loss: 2.1129 (2.1129) \n","Epoch: [95][100/200] Train loss: 2.3038 (2.1762) \n","accuracy: [89.462, 89.57000000000001, 89.642]\n","\n","== Train RUC ==\n","Epoch: [96][0/200] Train loss: 2.2855 (2.2855) \n","Epoch: [96][100/200] Train loss: 2.1958 (2.2085) \n","Epoch: [96][0/200] Train loss: 2.3297 (2.3297) \n","Epoch: [96][100/200] Train loss: 2.2839 (2.1975) \n","accuracy: [89.488, 89.542, 89.658]\n","\n","== Train RUC ==\n","Epoch: [97][0/200] Train loss: 2.3039 (2.3039) \n","Epoch: [97][100/200] Train loss: 2.1482 (2.1485) \n","Epoch: [97][0/200] Train loss: 2.0685 (2.0685) \n","Epoch: [97][100/200] Train loss: 2.3848 (2.1925) \n","accuracy: [89.598, 89.526, 89.694]\n","\n","== Train RUC ==\n","Epoch: [98][0/200] Train loss: 2.2880 (2.2880) \n","Epoch: [98][100/200] Train loss: 2.3005 (2.1667) \n","Epoch: [98][0/200] Train loss: 2.2805 (2.2805) \n","Epoch: [98][100/200] Train loss: 2.2230 (2.1839) \n","accuracy: [89.62, 89.63, 89.732]\n","\n","== Train RUC ==\n","Epoch: [99][0/200] Train loss: 2.2122 (2.2122) \n","Epoch: [99][100/200] Train loss: 2.0506 (2.1608) \n","Epoch: [99][0/200] Train loss: 2.2253 (2.2253) \n","Epoch: [99][100/200] Train loss: 1.7987 (2.1790) \n","accuracy: [89.628, 89.652, 89.788]\n","\n","== Train RUC ==\n","Epoch: [100][0/200] Train loss: 2.1420 (2.1420) \n","Epoch: [100][100/200] Train loss: 2.0937 (2.1749) \n","Epoch: [100][0/200] Train loss: 2.2107 (2.2107) \n","Epoch: [100][100/200] Train loss: 2.3275 (2.1653) \n","accuracy: [89.61200000000001, 89.61, 89.73400000000001]\n","\n","== Train RUC ==\n","Epoch: [101][0/200] Train loss: 2.2908 (2.2908) \n","Epoch: [101][100/200] Train loss: 1.9663 (2.1653) \n","Epoch: [101][0/200] Train loss: 1.6740 (1.6740) \n","Epoch: [101][100/200] Train loss: 1.6942 (2.1578) \n","accuracy: [89.61, 89.57000000000001, 89.736]\n","\n","== Train RUC ==\n","Epoch: [102][0/200] Train loss: 2.2179 (2.2179) \n","Epoch: [102][100/200] Train loss: 2.2757 (2.1624) \n","Epoch: [102][0/200] Train loss: 2.0686 (2.0686) \n","Epoch: [102][100/200] Train loss: 2.3450 (2.1609) \n","accuracy: [89.386, 89.5, 89.562]\n","\n","== Train RUC ==\n","Epoch: [103][0/200] Train loss: 2.3125 (2.3125) \n","Epoch: [103][100/200] Train loss: 2.2785 (2.1894) \n","Epoch: [103][0/200] Train loss: 2.0548 (2.0548) \n","Epoch: [103][100/200] Train loss: 2.1966 (2.1912) \n","accuracy: [89.542, 89.534, 89.68]\n","\n","== Train RUC ==\n","Epoch: [104][0/200] Train loss: 1.9203 (1.9203) \n","Epoch: [104][100/200] Train loss: 1.5207 (2.1837) \n","Epoch: [104][0/200] Train loss: 2.2640 (2.2640) \n","Epoch: [104][100/200] Train loss: 2.2219 (2.1443) \n","accuracy: [89.666, 89.72200000000001, 89.754]\n","\n","== Train RUC ==\n","Epoch: [105][0/200] Train loss: 2.3284 (2.3284) \n","Epoch: [105][100/200] Train loss: 2.2272 (2.1455) \n","Epoch: [105][0/200] Train loss: 2.2459 (2.2459) \n","Epoch: [105][100/200] Train loss: 2.1911 (2.2042) \n","accuracy: [89.518, 89.69, 89.726]\n","\n","== Train RUC ==\n","Epoch: [106][0/200] Train loss: 1.4888 (1.4888) \n","Epoch: [106][100/200] Train loss: 2.3645 (2.1952) \n","Epoch: [106][0/200] Train loss: 2.1664 (2.1664) \n","Epoch: [106][100/200] Train loss: 2.3362 (2.1476) \n","accuracy: [89.69, 89.64, 89.786]\n","\n","== Train RUC ==\n","Epoch: [107][0/200] Train loss: 2.3200 (2.3200) \n","Epoch: [107][100/200] Train loss: 1.6222 (2.1291) \n","Epoch: [107][0/200] Train loss: 1.9006 (1.9006) \n","Epoch: [107][100/200] Train loss: 1.7445 (2.1475) \n","accuracy: [89.69200000000001, 89.644, 89.83]\n","\n","== Train RUC ==\n","Epoch: [108][0/200] Train loss: 2.1628 (2.1628) \n","Epoch: [108][100/200] Train loss: 2.2713 (2.1344) \n","Epoch: [108][0/200] Train loss: 2.2841 (2.2841) \n","Epoch: [108][100/200] Train loss: 2.2326 (2.1682) \n","accuracy: [89.61, 89.74600000000001, 89.816]\n","\n","== Train RUC ==\n","Epoch: [109][0/200] Train loss: 2.0607 (2.0607) \n","Epoch: [109][100/200] Train loss: 2.2369 (2.1862) \n","Epoch: [109][0/200] Train loss: 2.2739 (2.2739) \n","Epoch: [109][100/200] Train loss: 2.2892 (2.1579) \n","accuracy: [89.706, 89.708, 89.836]\n","\n","== Train RUC ==\n","Epoch: [110][0/200] Train loss: 2.1639 (2.1639) \n","Epoch: [110][100/200] Train loss: 2.2867 (2.2034) \n","Epoch: [110][0/200] Train loss: 2.2577 (2.2577) \n","Epoch: [110][100/200] Train loss: 2.1664 (2.1659) \n","accuracy: [89.64800000000001, 89.724, 89.818]\n","\n","== Train RUC ==\n","Epoch: [111][0/200] Train loss: 1.9701 (1.9701) \n","Epoch: [111][100/200] Train loss: 2.2267 (2.1747) \n","Epoch: [111][0/200] Train loss: 2.3231 (2.3231) \n","Epoch: [111][100/200] Train loss: 2.2766 (2.1847) \n","accuracy: [89.56, 89.74, 89.794]\n","\n","== Train RUC ==\n","Epoch: [112][0/200] Train loss: 2.1764 (2.1764) \n","Epoch: [112][100/200] Train loss: 2.1967 (2.1845) \n","Epoch: [112][0/200] Train loss: 2.2971 (2.2971) \n","Epoch: [112][100/200] Train loss: 1.9880 (2.1658) \n","accuracy: [89.776, 89.728, 89.896]\n","\n","== Train RUC ==\n","Epoch: [113][0/200] Train loss: 2.2004 (2.2004) \n","Epoch: [113][100/200] Train loss: 1.7918 (2.1603) \n","Epoch: [113][0/200] Train loss: 2.3439 (2.3439) \n","Epoch: [113][100/200] Train loss: 2.2822 (2.1800) \n","accuracy: [89.74, 89.68599999999999, 89.828]\n","\n","== Train RUC ==\n","Epoch: [114][0/200] Train loss: 2.0019 (2.0019) \n","Epoch: [114][100/200] Train loss: 2.2335 (2.1506) \n","Epoch: [114][0/200] Train loss: 2.2266 (2.2266) \n","Epoch: [114][100/200] Train loss: 2.2016 (2.1781) \n","accuracy: [89.61200000000001, 89.67399999999999, 89.776]\n","\n","== Train RUC ==\n","Epoch: [115][0/200] Train loss: 2.1839 (2.1839) \n","Epoch: [115][100/200] Train loss: 1.9505 (2.1353) \n","Epoch: [115][0/200] Train loss: 2.2088 (2.2088) \n","Epoch: [115][100/200] Train loss: 2.2464 (2.1679) \n","accuracy: [89.68599999999999, 89.90599999999999, 89.902]\n","\n","== Train RUC ==\n","Epoch: [116][0/200] Train loss: 2.3814 (2.3814) \n","Epoch: [116][100/200] Train loss: 1.9336 (2.1813) \n","Epoch: [116][0/200] Train loss: 2.2051 (2.2051) \n","Epoch: [116][100/200] Train loss: 2.1378 (2.1838) \n","accuracy: [89.86, 89.782, 89.90599999999999]\n","\n","== Train RUC ==\n","Epoch: [117][0/200] Train loss: 2.0682 (2.0682) \n","Epoch: [117][100/200] Train loss: 2.2933 (2.1819) \n","Epoch: [117][0/200] Train loss: 2.3795 (2.3795) \n","Epoch: [117][100/200] Train loss: 2.1953 (2.1534) \n","accuracy: [89.768, 89.754, 89.79]\n","\n","== Train RUC ==\n","Epoch: [118][0/200] Train loss: 2.3307 (2.3307) \n","Epoch: [118][100/200] Train loss: 2.2215 (2.1931) \n","Epoch: [118][0/200] Train loss: 2.2489 (2.2489) \n","Epoch: [118][100/200] Train loss: 2.2349 (2.1675) \n","accuracy: [89.716, 89.794, 89.858]\n","\n","== Train RUC ==\n","Epoch: [119][0/200] Train loss: 2.0038 (2.0038) \n","Epoch: [119][100/200] Train loss: 2.2601 (2.1546) \n","Epoch: [119][0/200] Train loss: 2.3372 (2.3372) \n","Epoch: [119][100/200] Train loss: 2.3091 (2.1488) \n","accuracy: [89.81, 89.788, 89.886]\n","\n","== Train RUC ==\n","Epoch: [120][0/200] Train loss: 2.2888 (2.2888) \n","Epoch: [120][100/200] Train loss: 2.2890 (2.1571) \n","Epoch: [120][0/200] Train loss: 2.2604 (2.2604) \n","Epoch: [120][100/200] Train loss: 2.2452 (2.1779) \n","accuracy: [89.81, 89.7, 89.842]\n","\n","== Train RUC ==\n","Epoch: [121][0/200] Train loss: 2.0370 (2.0370) \n","Epoch: [121][100/200] Train loss: 1.8812 (2.1426) \n","Epoch: [121][0/200] Train loss: 1.9243 (1.9243) \n","Epoch: [121][100/200] Train loss: 1.9945 (2.2008) \n","accuracy: [89.792, 89.828, 89.874]\n","\n","== Train RUC ==\n","Epoch: [122][0/200] Train loss: 1.9659 (1.9659) \n","Epoch: [122][100/200] Train loss: 1.8676 (2.1803) \n","Epoch: [122][0/200] Train loss: 2.2087 (2.2087) \n","Epoch: [122][100/200] Train loss: 2.1974 (2.1766) \n","accuracy: [89.78399999999999, 89.852, 89.876]\n","\n","== Train RUC ==\n","Epoch: [123][0/200] Train loss: 1.9095 (1.9095) \n","Epoch: [123][100/200] Train loss: 2.2902 (2.1926) \n","Epoch: [123][0/200] Train loss: 2.1273 (2.1273) \n","Epoch: [123][100/200] Train loss: 2.0804 (2.1790) \n","accuracy: [89.726, 89.75999999999999, 89.854]\n","\n","== Train RUC ==\n","Epoch: [124][0/200] Train loss: 2.1319 (2.1319) \n","Epoch: [124][100/200] Train loss: 2.1778 (2.1879) \n","Epoch: [124][0/200] Train loss: 2.4416 (2.4416) \n","Epoch: [124][100/200] Train loss: 1.8623 (2.1760) \n","accuracy: [89.864, 89.87, 89.91]\n","\n","== Train RUC ==\n","Epoch: [125][0/200] Train loss: 1.8536 (1.8536) \n","Epoch: [125][100/200] Train loss: 2.3087 (2.1486) \n","Epoch: [125][0/200] Train loss: 2.2319 (2.2319) \n","Epoch: [125][100/200] Train loss: 2.3810 (2.1722) \n","accuracy: [89.842, 89.858, 89.92599999999999]\n","\n","== Train RUC ==\n","Epoch: [126][0/200] Train loss: 2.3125 (2.3125) \n","Epoch: [126][100/200] Train loss: 2.0832 (2.1644) \n","Epoch: [126][0/200] Train loss: 2.2424 (2.2424) \n","Epoch: [126][100/200] Train loss: 2.2452 (2.1579) \n","accuracy: [89.834, 89.86, 89.928]\n","\n","== Train RUC ==\n","Epoch: [127][0/200] Train loss: 2.1001 (2.1001) \n","Epoch: [127][100/200] Train loss: 2.3383 (2.1588) \n","Epoch: [127][0/200] Train loss: 2.3746 (2.3746) \n","Epoch: [127][100/200] Train loss: 2.2493 (2.1485) \n","accuracy: [89.90599999999999, 89.972, 89.994]\n","\n","== Train RUC ==\n","Epoch: [128][0/200] Train loss: 1.9537 (1.9537) \n","Epoch: [128][100/200] Train loss: 2.1569 (2.2009) \n","Epoch: [128][0/200] Train loss: 2.2142 (2.2142) \n","Epoch: [128][100/200] Train loss: 2.1709 (2.1987) \n","accuracy: [89.948, 89.874, 89.998]\n","\n","== Train RUC ==\n","Epoch: [129][0/200] Train loss: 1.6081 (1.6081) \n","Epoch: [129][100/200] Train loss: 2.3132 (2.1964) \n","Epoch: [129][0/200] Train loss: 2.2563 (2.2563) \n","Epoch: [129][100/200] Train loss: 2.3184 (2.1907) \n","accuracy: [89.912, 89.916, 90.0]\n","\n","== Train RUC ==\n","Epoch: [130][0/200] Train loss: 2.2474 (2.2474) \n","Epoch: [130][100/200] Train loss: 1.9819 (2.1592) \n","Epoch: [130][0/200] Train loss: 2.1909 (2.1909) \n","Epoch: [130][100/200] Train loss: 2.1405 (2.1579) \n","accuracy: [89.91799999999999, 89.902, 89.968]\n","\n","== Train RUC ==\n","Epoch: [131][0/200] Train loss: 2.1167 (2.1167) \n","Epoch: [131][100/200] Train loss: 2.0525 (2.1905) \n","Epoch: [131][0/200] Train loss: 2.3280 (2.3280) \n","Epoch: [131][100/200] Train loss: 1.9337 (2.1854) \n","accuracy: [89.976, 89.98, 90.062]\n","\n","== Train RUC ==\n","Epoch: [132][0/200] Train loss: 2.3766 (2.3766) \n","Epoch: [132][100/200] Train loss: 1.7829 (2.1801) \n","Epoch: [132][0/200] Train loss: 2.3153 (2.3153) \n","Epoch: [132][100/200] Train loss: 2.2968 (2.1927) \n","accuracy: [89.96, 89.946, 90.006]\n","\n","== Train RUC ==\n","Epoch: [133][0/200] Train loss: 2.1444 (2.1444) \n","Epoch: [133][100/200] Train loss: 1.4494 (2.1943) \n","Epoch: [133][0/200] Train loss: 2.3028 (2.3028) \n","Epoch: [133][100/200] Train loss: 2.1929 (2.1879) \n","accuracy: [89.932, 89.932, 89.996]\n","\n","== Train RUC ==\n","Epoch: [134][0/200] Train loss: 1.4297 (1.4297) \n","Epoch: [134][100/200] Train loss: 1.9231 (2.1697) \n","Epoch: [134][0/200] Train loss: 2.1802 (2.1802) \n","Epoch: [134][100/200] Train loss: 2.3423 (2.1636) \n","accuracy: [90.008, 90.008, 90.042]\n","\n","== Train RUC ==\n","Epoch: [135][0/200] Train loss: 2.3365 (2.3365) \n","Epoch: [135][100/200] Train loss: 2.2235 (2.1786) \n","Epoch: [135][0/200] Train loss: 2.0757 (2.0757) \n","Epoch: [135][100/200] Train loss: 2.1245 (2.1654) \n","accuracy: [89.92999999999999, 89.938, 90.0]\n","\n","== Train RUC ==\n","Epoch: [136][0/200] Train loss: 2.1936 (2.1936) \n","Epoch: [136][100/200] Train loss: 1.9151 (2.1703) \n","Epoch: [136][0/200] Train loss: 2.2870 (2.2870) \n","Epoch: [136][100/200] Train loss: 2.0493 (2.1767) \n","accuracy: [89.92599999999999, 89.96, 89.92999999999999]\n","\n","== Train RUC ==\n","Epoch: [137][0/200] Train loss: 2.1638 (2.1638) \n","Epoch: [137][100/200] Train loss: 2.1976 (2.1723) \n","Epoch: [137][0/200] Train loss: 2.2864 (2.2864) \n","Epoch: [137][100/200] Train loss: 2.0664 (2.1583) \n","accuracy: [89.84400000000001, 89.866, 89.904]\n","\n","== Train RUC ==\n","Epoch: [138][0/200] Train loss: 2.4080 (2.4080) \n","Epoch: [138][100/200] Train loss: 2.1000 (2.1551) \n","Epoch: [138][0/200] Train loss: 2.3081 (2.3081) \n","Epoch: [138][100/200] Train loss: 2.0205 (2.1701) \n","accuracy: [89.874, 89.976, 90.0]\n","\n","== Train RUC ==\n","Epoch: [139][0/200] Train loss: 1.8453 (1.8453) \n","Epoch: [139][100/200] Train loss: 2.3523 (2.1772) \n","Epoch: [139][0/200] Train loss: 2.1280 (2.1280) \n","Epoch: [139][100/200] Train loss: 2.1831 (2.1552) \n","accuracy: [89.936, 89.91, 89.984]\n","\n","== Train RUC ==\n","Epoch: [140][0/200] Train loss: 2.1280 (2.1280) \n","Epoch: [140][100/200] Train loss: 2.2000 (2.1676) \n","Epoch: [140][0/200] Train loss: 2.1714 (2.1714) \n","Epoch: [140][100/200] Train loss: 2.3180 (2.1669) \n","accuracy: [89.896, 90.01, 89.998]\n","\n","== Train RUC ==\n","Epoch: [141][0/200] Train loss: 2.2763 (2.2763) \n","Epoch: [141][100/200] Train loss: 2.2419 (2.2001) \n","Epoch: [141][0/200] Train loss: 1.9266 (1.9266) \n","Epoch: [141][100/200] Train loss: 1.7597 (2.1492) \n","accuracy: [89.942, 89.91, 89.998]\n","\n","== Train RUC ==\n","Epoch: [142][0/200] Train loss: 2.2582 (2.2582) \n","Epoch: [142][100/200] Train loss: 1.8945 (2.1788) \n","Epoch: [142][0/200] Train loss: 2.1875 (2.1875) \n","Epoch: [142][100/200] Train loss: 2.1189 (2.1925) \n","accuracy: [90.05, 89.864, 90.022]\n","\n","== Train RUC ==\n","Epoch: [143][0/200] Train loss: 1.8695 (1.8695) \n","Epoch: [143][100/200] Train loss: 2.2553 (2.1647) \n","Epoch: [143][0/200] Train loss: 2.2253 (2.2253) \n","Epoch: [143][100/200] Train loss: 2.0198 (2.1739) \n","accuracy: [89.962, 90.032, 90.034]\n","\n","== Train RUC ==\n","Epoch: [144][0/200] Train loss: 2.1123 (2.1123) \n","Epoch: [144][100/200] Train loss: 2.1323 (2.1917) \n","Epoch: [144][0/200] Train loss: 2.0366 (2.0366) \n","Epoch: [144][100/200] Train loss: 2.2671 (2.1676) \n","accuracy: [89.952, 90.032, 90.062]\n","\n","== Train RUC ==\n","Epoch: [145][0/200] Train loss: 2.3637 (2.3637) \n","Epoch: [145][100/200] Train loss: 2.3082 (2.1588) \n","Epoch: [145][0/200] Train loss: 2.1221 (2.1221) \n","Epoch: [145][100/200] Train loss: 2.3377 (2.1602) \n","accuracy: [89.984, 90.042, 90.072]\n","\n","== Train RUC ==\n","Epoch: [146][0/200] Train loss: 2.3074 (2.3074) \n","Epoch: [146][100/200] Train loss: 2.2756 (2.1526) \n","Epoch: [146][0/200] Train loss: 2.2829 (2.2829) \n","Epoch: [146][100/200] Train loss: 2.3502 (2.1788) \n","accuracy: [90.02799999999999, 90.02, 90.042]\n","\n","== Train RUC ==\n","Epoch: [147][0/200] Train loss: 2.1766 (2.1766) \n","Epoch: [147][100/200] Train loss: 2.1296 (2.2122) \n","Epoch: [147][0/200] Train loss: 2.0102 (2.0102) \n","Epoch: [147][100/200] Train loss: 2.3151 (2.1625) \n","accuracy: [90.03, 89.956, 90.048]\n","\n","== Train RUC ==\n","Epoch: [148][0/200] Train loss: 2.2426 (2.2426) \n","Epoch: [148][100/200] Train loss: 1.8629 (2.1838) \n","Epoch: [148][0/200] Train loss: 2.2087 (2.2087) \n","Epoch: [148][100/200] Train loss: 2.1684 (2.1808) \n","accuracy: [89.976, 90.032, 90.05]\n","\n","== Train RUC ==\n","Epoch: [149][0/200] Train loss: 2.2073 (2.2073) \n","Epoch: [149][100/200] Train loss: 2.3771 (2.1690) \n","Epoch: [149][0/200] Train loss: 2.2031 (2.2031) \n","Epoch: [149][100/200] Train loss: 2.2116 (2.1834) \n","accuracy: [90.032, 90.00200000000001, 90.082]\n","\n","== Train RUC ==\n","Epoch: [150][0/200] Train loss: 2.1274 (2.1274) \n","Epoch: [150][100/200] Train loss: 1.9799 (2.1740) \n","Epoch: [150][0/200] Train loss: 2.2004 (2.2004) \n","Epoch: [150][100/200] Train loss: 2.2368 (2.1452) \n","accuracy: [90.042, 90.086, 90.058]\n","\n","== Train RUC ==\n","Epoch: [151][0/200] Train loss: 2.3311 (2.3311) \n","Epoch: [151][100/200] Train loss: 2.3693 (2.1708) \n","Epoch: [151][0/200] Train loss: 2.3069 (2.3069) \n","Epoch: [151][100/200] Train loss: 2.3356 (2.1456) \n","accuracy: [90.058, 90.00399999999999, 90.02799999999999]\n","\n","== Train RUC ==\n","Epoch: [152][0/200] Train loss: 2.2743 (2.2743) \n","Epoch: [152][100/200] Train loss: 1.8287 (2.1617) \n","Epoch: [152][0/200] Train loss: 2.2767 (2.2767) \n","Epoch: [152][100/200] Train loss: 2.2813 (2.1594) \n","accuracy: [90.048, 89.98, 90.078]\n","\n","== Train RUC ==\n","Epoch: [153][0/200] Train loss: 2.2629 (2.2629) \n","Epoch: [153][100/200] Train loss: 2.0122 (2.1519) \n","Epoch: [153][0/200] Train loss: 2.2961 (2.2961) \n","Epoch: [153][100/200] Train loss: 2.2749 (2.1583) \n","accuracy: [89.974, 90.05, 90.078]\n","\n","== Train RUC ==\n","Epoch: [154][0/200] Train loss: 2.2092 (2.2092) \n","Epoch: [154][100/200] Train loss: 2.2217 (2.1716) \n","Epoch: [154][0/200] Train loss: 2.0803 (2.0803) \n","Epoch: [154][100/200] Train loss: 2.2084 (2.1555) \n","accuracy: [90.00399999999999, 90.044, 90.076]\n","\n","== Train RUC ==\n","Epoch: [155][0/200] Train loss: 2.1250 (2.1250) \n","Epoch: [155][100/200] Train loss: 1.7856 (2.1563) \n","Epoch: [155][0/200] Train loss: 2.2088 (2.2088) \n","Epoch: [155][100/200] Train loss: 2.2035 (2.1726) \n","accuracy: [90.096, 90.042, 90.102]\n","\n","== Train RUC ==\n","Epoch: [156][0/200] Train loss: 2.2397 (2.2397) \n","Epoch: [156][100/200] Train loss: 2.0602 (2.1822) \n","Epoch: [156][0/200] Train loss: 2.4010 (2.4010) \n","Epoch: [156][100/200] Train loss: 2.0752 (2.1893) \n","accuracy: [90.08, 90.034, 90.096]\n","\n","== Train RUC ==\n","Epoch: [157][0/200] Train loss: 2.2162 (2.2162) \n","Epoch: [157][100/200] Train loss: 2.2507 (2.1503) \n","Epoch: [157][0/200] Train loss: 1.9826 (1.9826) \n","Epoch: [157][100/200] Train loss: 2.3062 (2.2155) \n","accuracy: [90.12599999999999, 90.048, 90.08]\n","\n","== Train RUC ==\n","Epoch: [158][0/200] Train loss: 2.2723 (2.2723) \n","Epoch: [158][100/200] Train loss: 1.7566 (2.1624) \n","Epoch: [158][0/200] Train loss: 2.3208 (2.3208) \n","Epoch: [158][100/200] Train loss: 2.2892 (2.1717) \n","accuracy: [90.066, 90.066, 90.106]\n","\n","== Train RUC ==\n","Epoch: [159][0/200] Train loss: 2.2735 (2.2735) \n","Epoch: [159][100/200] Train loss: 2.1967 (2.1668) \n","Epoch: [159][0/200] Train loss: 2.3603 (2.3603) \n","Epoch: [159][100/200] Train loss: 2.2065 (2.1545) \n","accuracy: [90.086, 90.094, 90.12599999999999]\n","\n","== Train RUC ==\n","Epoch: [160][0/200] Train loss: 2.3576 (2.3576) \n","Epoch: [160][100/200] Train loss: 1.8129 (2.1605) \n","Epoch: [160][0/200] Train loss: 2.0544 (2.0544) \n","Epoch: [160][100/200] Train loss: 2.0839 (2.1872) \n","accuracy: [90.042, 90.008, 90.068]\n","\n","== Train RUC ==\n","Epoch: [161][0/200] Train loss: 2.1869 (2.1869) \n","Epoch: [161][100/200] Train loss: 2.3212 (2.1628) \n","Epoch: [161][0/200] Train loss: 2.1144 (2.1144) \n","Epoch: [161][100/200] Train loss: 2.2764 (2.1439) \n","accuracy: [90.11, 90.048, 90.10000000000001]\n","\n","== Train RUC ==\n","Epoch: [162][0/200] Train loss: 2.2878 (2.2878) \n","Epoch: [162][100/200] Train loss: 2.0979 (2.1803) \n","Epoch: [162][0/200] Train loss: 2.2777 (2.2777) \n","Epoch: [162][100/200] Train loss: 2.2018 (2.1803) \n","accuracy: [90.09, 90.02, 90.09]\n","\n","== Train RUC ==\n","Epoch: [163][0/200] Train loss: 2.1441 (2.1441) \n","Epoch: [163][100/200] Train loss: 2.3255 (2.1633) \n","Epoch: [163][0/200] Train loss: 2.2378 (2.2378) \n","Epoch: [163][100/200] Train loss: 2.1630 (2.1347) \n","accuracy: [90.11, 90.09, 90.114]\n","\n","== Train RUC ==\n","Epoch: [164][0/200] Train loss: 2.1960 (2.1960) \n","Epoch: [164][100/200] Train loss: 1.8845 (2.1692) \n","Epoch: [164][0/200] Train loss: 1.9879 (1.9879) \n","Epoch: [164][100/200] Train loss: 2.2657 (2.1825) \n","accuracy: [90.142, 90.05, 90.102]\n","\n","== Train RUC ==\n","Epoch: [165][0/200] Train loss: 2.3441 (2.3441) \n","Epoch: [165][100/200] Train loss: 1.9088 (2.1330) \n","Epoch: [165][0/200] Train loss: 2.0901 (2.0901) \n","Epoch: [165][100/200] Train loss: 2.2699 (2.1525) \n","accuracy: [90.08800000000001, 90.10000000000001, 90.106]\n","\n","== Train RUC ==\n","Epoch: [166][0/200] Train loss: 2.3003 (2.3003) \n","Epoch: [166][100/200] Train loss: 1.9863 (2.1584) \n","Epoch: [166][0/200] Train loss: 2.2837 (2.2837) \n","Epoch: [166][100/200] Train loss: 2.2877 (2.1510) \n","accuracy: [90.094, 90.048, 90.09]\n","\n","== Train RUC ==\n","Epoch: [167][0/200] Train loss: 1.9580 (1.9580) \n","Epoch: [167][100/200] Train loss: 2.4524 (2.1519) \n","Epoch: [167][0/200] Train loss: 2.2351 (2.2351) \n","Epoch: [167][100/200] Train loss: 2.2285 (2.1447) \n","accuracy: [90.096, 90.104, 90.142]\n","\n","== Train RUC ==\n","Epoch: [168][0/200] Train loss: 2.0349 (2.0349) \n","Epoch: [168][100/200] Train loss: 2.2160 (2.1703) \n","Epoch: [168][0/200] Train loss: 2.1811 (2.1811) \n","Epoch: [168][100/200] Train loss: 2.0626 (2.1549) \n","accuracy: [90.086, 90.044, 90.07]\n","\n","== Train RUC ==\n","Epoch: [169][0/200] Train loss: 1.3336 (1.3336) \n","Epoch: [169][100/200] Train loss: 2.0886 (2.1477) \n","Epoch: [169][0/200] Train loss: 2.1078 (2.1078) \n","Epoch: [169][100/200] Train loss: 1.5500 (2.1726) \n","accuracy: [90.086, 90.106, 90.13]\n","\n","== Train RUC ==\n","Epoch: [170][0/200] Train loss: 2.3320 (2.3320) \n","Epoch: [170][100/200] Train loss: 2.3266 (2.1649) \n","Epoch: [170][0/200] Train loss: 2.2478 (2.2478) \n","Epoch: [170][100/200] Train loss: 2.0697 (2.1840) \n","accuracy: [90.098, 90.074, 90.104]\n","\n","== Train RUC ==\n","Epoch: [171][0/200] Train loss: 2.4024 (2.4024) \n","Epoch: [171][100/200] Train loss: 2.1769 (2.1506) \n","Epoch: [171][0/200] Train loss: 2.2219 (2.2219) \n","Epoch: [171][100/200] Train loss: 2.2234 (2.1497) \n","accuracy: [90.08800000000001, 90.11, 90.128]\n","\n","== Train RUC ==\n","Epoch: [172][0/200] Train loss: 2.2316 (2.2316) \n","Epoch: [172][100/200] Train loss: 2.1733 (2.1826) \n","Epoch: [172][0/200] Train loss: 2.1366 (2.1366) \n","Epoch: [172][100/200] Train loss: 2.0637 (2.1830) \n","accuracy: [90.104, 90.104, 90.12]\n","\n","== Train RUC ==\n","Epoch: [173][0/200] Train loss: 2.2503 (2.2503) \n","Epoch: [173][100/200] Train loss: 2.1263 (2.1743) \n","Epoch: [173][0/200] Train loss: 1.6697 (1.6697) \n","Epoch: [173][100/200] Train loss: 1.7964 (2.1491) \n","accuracy: [90.118, 90.106, 90.132]\n","\n","== Train RUC ==\n","Epoch: [174][0/200] Train loss: 1.9153 (1.9153) \n","Epoch: [174][100/200] Train loss: 2.2738 (2.1239) \n","Epoch: [174][0/200] Train loss: 2.3485 (2.3485) \n","Epoch: [174][100/200] Train loss: 2.2852 (2.1641) \n","accuracy: [90.114, 90.092, 90.12599999999999]\n","\n","== Train RUC ==\n","Epoch: [175][0/200] Train loss: 2.2380 (2.2380) \n","Epoch: [175][100/200] Train loss: 2.1843 (2.1485) \n","Epoch: [175][0/200] Train loss: 1.7260 (1.7260) \n","Epoch: [175][100/200] Train loss: 2.2286 (2.1323) \n","accuracy: [90.098, 90.102, 90.12400000000001]\n","\n","== Train RUC ==\n","Epoch: [176][0/200] Train loss: 1.9485 (1.9485) \n","Epoch: [176][100/200] Train loss: 2.0860 (2.1804) \n","Epoch: [176][0/200] Train loss: 1.8291 (1.8291) \n","Epoch: [176][100/200] Train loss: 2.2358 (2.1791) \n","accuracy: [90.132, 90.114, 90.132]\n","\n","== Train RUC ==\n","Epoch: [177][0/200] Train loss: 2.1954 (2.1954) \n","Epoch: [177][100/200] Train loss: 2.1333 (2.1458) \n","Epoch: [177][0/200] Train loss: 1.7547 (1.7547) \n","Epoch: [177][100/200] Train loss: 2.1669 (2.1814) \n","accuracy: [90.152, 90.10000000000001, 90.134]\n","\n","== Train RUC ==\n","Epoch: [178][0/200] Train loss: 1.5446 (1.5446) \n","Epoch: [178][100/200] Train loss: 2.2102 (2.1650) \n","Epoch: [178][0/200] Train loss: 2.1908 (2.1908) \n","Epoch: [178][100/200] Train loss: 2.2412 (2.1912) \n","accuracy: [90.12400000000001, 90.116, 90.12400000000001]\n","\n","== Train RUC ==\n","Epoch: [179][0/200] Train loss: 2.2986 (2.2986) \n","Epoch: [179][100/200] Train loss: 2.4164 (2.1941) \n","Epoch: [179][0/200] Train loss: 2.2428 (2.2428) \n","Epoch: [179][100/200] Train loss: 2.2079 (2.2016) \n","accuracy: [90.13, 90.102, 90.13]\n","\n","== Train RUC ==\n","Epoch: [180][0/200] Train loss: 2.1645 (2.1645) \n","Epoch: [180][100/200] Train loss: 2.2227 (2.1753) \n","Epoch: [180][0/200] Train loss: 2.3991 (2.3991) \n","Epoch: [180][100/200] Train loss: 2.2122 (2.1683) \n","accuracy: [90.106, 90.078, 90.114]\n","\n","== Train RUC ==\n","Epoch: [181][0/200] Train loss: 2.3885 (2.3885) \n","Epoch: [181][100/200] Train loss: 2.2199 (2.1653) \n","Epoch: [181][0/200] Train loss: 2.2959 (2.2959) \n","Epoch: [181][100/200] Train loss: 2.1631 (2.1608) \n","accuracy: [90.114, 90.092, 90.116]\n","\n","== Train RUC ==\n","Epoch: [182][0/200] Train loss: 2.1567 (2.1567) \n","Epoch: [182][100/200] Train loss: 2.1275 (2.1775) \n","Epoch: [182][0/200] Train loss: 1.8955 (1.8955) \n","Epoch: [182][100/200] Train loss: 2.2778 (2.1973) \n","accuracy: [90.108, 90.104, 90.104]\n","\n","== Train RUC ==\n","Epoch: [183][0/200] Train loss: 2.2813 (2.2813) \n","Epoch: [183][100/200] Train loss: 2.3157 (2.1626) \n","Epoch: [183][0/200] Train loss: 2.2995 (2.2995) \n","Epoch: [183][100/200] Train loss: 2.2877 (2.1894) \n","accuracy: [90.118, 90.108, 90.118]\n","\n","== Train RUC ==\n","Epoch: [184][0/200] Train loss: 2.3499 (2.3499) \n","Epoch: [184][100/200] Train loss: 2.2753 (2.1954) \n","Epoch: [184][0/200] Train loss: 2.2436 (2.2436) \n","Epoch: [184][100/200] Train loss: 2.1610 (2.1559) \n","accuracy: [90.102, 90.08800000000001, 90.13]\n","\n","== Train RUC ==\n","Epoch: [185][0/200] Train loss: 2.3470 (2.3470) \n","Epoch: [185][100/200] Train loss: 1.7708 (2.1444) \n","Epoch: [185][0/200] Train loss: 2.3095 (2.3095) \n","Epoch: [185][100/200] Train loss: 2.2446 (2.1976) \n","accuracy: [90.106, 90.092, 90.128]\n","\n","== Train RUC ==\n","Epoch: [186][0/200] Train loss: 2.0259 (2.0259) \n","Epoch: [186][100/200] Train loss: 2.1967 (2.1901) \n","Epoch: [186][0/200] Train loss: 2.3696 (2.3696) \n","Epoch: [186][100/200] Train loss: 2.1311 (2.1761) \n","accuracy: [90.128, 90.116, 90.134]\n","\n","== Train RUC ==\n","Epoch: [187][0/200] Train loss: 1.8304 (1.8304) \n","Epoch: [187][100/200] Train loss: 2.2985 (2.1922) \n","Epoch: [187][0/200] Train loss: 2.2362 (2.2362) \n","Epoch: [187][100/200] Train loss: 2.2281 (2.1557) \n","accuracy: [90.122, 90.096, 90.142]\n","\n","== Train RUC ==\n","Epoch: [188][0/200] Train loss: 2.3581 (2.3581) \n","Epoch: [188][100/200] Train loss: 2.3693 (2.1694) \n","Epoch: [188][0/200] Train loss: 1.9522 (1.9522) \n","Epoch: [188][100/200] Train loss: 2.2433 (2.1653) \n","accuracy: [90.118, 90.10000000000001, 90.12400000000001]\n","\n","== Train RUC ==\n","Epoch: [189][0/200] Train loss: 2.3044 (2.3044) \n","Epoch: [189][100/200] Train loss: 2.3346 (2.1641) \n","Epoch: [189][0/200] Train loss: 2.2565 (2.2565) \n","Epoch: [189][100/200] Train loss: 2.3400 (2.1998) \n","accuracy: [90.128, 90.108, 90.13]\n","\n","== Train RUC ==\n","Epoch: [190][0/200] Train loss: 2.2981 (2.2981) \n","Epoch: [190][100/200] Train loss: 2.2562 (2.1709) \n","Epoch: [190][0/200] Train loss: 2.1920 (2.1920) \n","Epoch: [190][100/200] Train loss: 1.8149 (2.2061) \n","accuracy: [90.11200000000001, 90.098, 90.11]\n","\n","== Train RUC ==\n","Epoch: [191][0/200] Train loss: 2.2233 (2.2233) \n","Epoch: [191][100/200] Train loss: 2.2581 (2.1966) \n","Epoch: [191][0/200] Train loss: 2.2062 (2.2062) \n","Epoch: [191][100/200] Train loss: 2.2880 (2.1743) \n","accuracy: [90.11, 90.086, 90.12599999999999]\n","\n","== Train RUC ==\n","Epoch: [192][0/200] Train loss: 2.1797 (2.1797) \n","Epoch: [192][100/200] Train loss: 2.2326 (2.1863) \n","Epoch: [192][0/200] Train loss: 2.3759 (2.3759) \n","Epoch: [192][100/200] Train loss: 1.3043 (2.1762) \n","accuracy: [90.11, 90.106, 90.128]\n","\n","== Train RUC ==\n","Epoch: [193][0/200] Train loss: 2.2750 (2.2750) \n","Epoch: [193][100/200] Train loss: 2.3591 (2.1602) \n","Epoch: [193][0/200] Train loss: 2.3430 (2.3430) \n","Epoch: [193][100/200] Train loss: 2.3253 (2.1634) \n","accuracy: [90.106, 90.076, 90.128]\n","\n","== Train RUC ==\n","Epoch: [194][0/200] Train loss: 2.3696 (2.3696) \n","Epoch: [194][100/200] Train loss: 1.9604 (2.1765) \n","Epoch: [194][0/200] Train loss: 2.3324 (2.3324) \n","Epoch: [194][100/200] Train loss: 2.3611 (2.2033) \n","accuracy: [90.118, 90.11200000000001, 90.148]\n","\n","== Train RUC ==\n","Epoch: [195][0/200] Train loss: 2.3716 (2.3716) \n","Epoch: [195][100/200] Train loss: 2.3606 (2.1729) \n","Epoch: [195][0/200] Train loss: 2.0877 (2.0877) \n","Epoch: [195][100/200] Train loss: 2.2771 (2.2186) \n","accuracy: [90.104, 90.092, 90.12599999999999]\n","\n","== Train RUC ==\n","Epoch: [196][0/200] Train loss: 2.0599 (2.0599) \n","Epoch: [196][100/200] Train loss: 2.2661 (2.2092) \n","Epoch: [196][0/200] Train loss: 1.9548 (1.9548) \n","Epoch: [196][100/200] Train loss: 2.3585 (2.2115) \n","accuracy: [90.108, 90.09, 90.11]\n","\n","== Train RUC ==\n","Epoch: [197][0/200] Train loss: 1.8154 (1.8154) \n","Epoch: [197][100/200] Train loss: 2.3142 (2.2075) \n","Epoch: [197][0/200] Train loss: 2.2561 (2.2561) \n","Epoch: [197][100/200] Train loss: 2.3212 (2.1907) \n","accuracy: [90.11200000000001, 90.11, 90.128]\n","\n","== Train RUC ==\n","Epoch: [198][0/200] Train loss: 2.2812 (2.2812) \n","Epoch: [198][100/200] Train loss: 1.5379 (2.1694) \n","Epoch: [198][0/200] Train loss: 1.9464 (1.9464) \n","Epoch: [198][100/200] Train loss: 2.2471 (2.1540) \n","accuracy: [90.108, 90.106, 90.14]\n","\n","== Train RUC ==\n","Epoch: [199][0/200] Train loss: 2.2782 (2.2782) \n","Epoch: [199][100/200] Train loss: 2.3733 (2.1872) \n","Epoch: [199][0/200] Train loss: 2.0191 (2.0191) \n","Epoch: [199][100/200] Train loss: 2.2799 (2.1992) \n","accuracy: [90.122, 90.11200000000001, 90.114]\n","\n"]}],"source":["for epoch in range(epochs):\n","        print(\"== Train RUC ==\")\n","        loss, devide, p_label, conf1 = train(epoch, net, net2, trainloader, optimizer1, criterion, devide, p_label, conf2, batch_size)\n","        loss, devide, p_label, conf2 = train(epoch, net2, net, trainloader, optimizer2, criterion, devide, p_label, conf1, batch_size)\n","        acc, p_list = test_ruc(net, net2, evalloader, device, class_num)\n","        print(\"accuracy: {}\\n\".format(acc))\n","\n","        state = {'net1': net.state_dict(),\n","                 'net2': net2.state_dict()}\n","        torch.save(state, './checkpoint/ruc_cifar10.t7')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["RfhVE17sOKlw","1Tuu5vTVUogt","bUvNXWRqfs18","uV7EVUbGf3Uu","S4JDNm9-fOFf","m_DJzeqVgem2","ojLykrplgo2y","vh3fmXfSgymA","lSepSnpLhuf5","JjuwT-GcjlZm","FGuyouNcjvcR","VQ_aY6poj4ci","UE9JsjtrcLBy","lfL9n_Q2UVR3","B2qhRZOOJ_JG","baW5f_SpHqX2"],"machine_shape":"hm","provenance":[],"gpuType":"A100","toc_visible":true,"authorship_tag":"ABX9TyNpBNESTQefB+U2ckXnGwg3"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}