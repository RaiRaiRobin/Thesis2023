{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["2VkakOvBs7LC"],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1nQArdks6fXiT206iIJqZVD1XzD9a9NF6","authorship_tag":"ABX9TyOhzWbS5r0MERSs+7qVMS0w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#Import packages"],"metadata":{"id":"_ugUoN73sBiu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"I6GGMyazqEoV"},"outputs":[],"source":["# Import packages\n","import torch\n","import numpy as np\n","import torchvision.transforms as transforms\n","import random\n","import PIL\n","import PIL.ImageOps\n","import PIL.ImageEnhance\n","import PIL.ImageDraw\n","from PIL import Image\n","import torchvision.datasets as torchdivision_datasets\n","import pickle\n","import os\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import copy\n","import torch.backends.cudnn as cudnn\n","from scipy.optimize import linear_sum_assignment as linear_assignment\n","import math"]},{"cell_type":"markdown","source":["#Change the working directory"],"metadata":{"id":"1xv24Yhqs023"}},{"cell_type":"code","source":["import os\n","\n","# Change the current working directory to a directory in Google Drive\n","new_directory_path = \"/content/drive/My Drive/Mercy college/Thesis\"\n","os.chdir(new_directory_path)"],"metadata":{"id":"RrhAK76Js3By"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify the current working directory\n","current_directory = os.getcwd()\n","print(\"Current Working Directory:\", current_directory)"],"metadata":{"id":"5Q0t3kDbs4TP","executionInfo":{"status":"ok","timestamp":1700161987015,"user_tz":300,"elapsed":9,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"426754a6-8959-4f57-cc6b-5d48ca779832"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Current Working Directory: /content/drive/My Drive/Mercy college/Thesis\n"]}]},{"cell_type":"markdown","source":["#Common codes"],"metadata":{"id":"2VkakOvBs7LC"}},{"cell_type":"markdown","source":["##BasicBlock"],"metadata":{"id":"usJtK7Sys_51"}},{"cell_type":"code","source":["class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1, is_last=False):\n","        super(BasicBlock, self).__init__()\n","        self.is_last = is_last\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion * planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion * planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        preact = out\n","        out = F.relu(out)\n","        if self.is_last:\n","            return out, preact\n","        else:\n","            return out"],"metadata":{"id":"QbzxuVUEs9Od"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Bottleneck"],"metadata":{"id":"sqee0l64tF8z"}},{"cell_type":"code","source":["class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1, is_last=False):\n","        super(Bottleneck, self).__init__()\n","        self.is_last = is_last\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion * planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion * planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        preact = out\n","        out = F.relu(out)\n","        if self.is_last:\n","            return out, preact\n","        else:\n","            return out"],"metadata":{"id":"jfyN_TfPtIKY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##ResNet"],"metadata":{"id":"DUiroe1qtMNk"}},{"cell_type":"code","source":["class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, in_channel=3, zero_init_residual=False):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1,bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","        # Zero-initialize the last BN in each residual branch,\n","        # so that the residual branch starts with zeros, and each residual block behaves\n","        # like an identity. This improves the model by 0.2~0.3% according to:\n","        # https://arxiv.org/abs/1706.02677\n","        if zero_init_residual:\n","            for m in self.modules():\n","                if isinstance(m, Bottleneck):\n","                    nn.init.constant_(m.bn3.weight, 0)\n","                elif isinstance(m, BasicBlock):\n","                    nn.init.constant_(m.bn2.weight, 0)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1] * (num_blocks - 1)\n","        layers = []\n","        for i in range(num_blocks):\n","            stride = strides[i]\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = self.avgpool(out)\n","        out = torch.flatten(out, 1)\n","        return out\n"],"metadata":{"id":"MejtTbIItLSh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Resnet_CIFAR"],"metadata":{"id":"QrCTbAgatRBv"}},{"cell_type":"code","source":["def Resnet_CIFAR():\n","    return ResNet(BasicBlock, [2, 2, 2, 2])"],"metadata":{"id":"6NO3ohEdtS68"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##ContrastiveModel"],"metadata":{"id":"nahDGR9HtVXR"}},{"cell_type":"code","source":["class ContrastiveModel(nn.Module):\n","    def __init__(self, backbone, head='mlp', features_dim=128):\n","        super(ContrastiveModel, self).__init__()\n","        self.backbone = backbone\n","        self.backbone_dim = 512\n","        self.head = head\n","\n","        if head == 'linear':\n","            self.contrastive_head = nn.Linear(self.backbone_dim, features_dim)\n","\n","        elif head == 'mlp':\n","            self.contrastive_head = nn.Sequential(\n","                    nn.Linear(self.backbone_dim, self.backbone_dim),\n","                    nn.ReLU(), nn.Linear(self.backbone_dim, features_dim))\n","\n","        else:\n","            raise ValueError('Invalid head {}'.format(head))\n","\n","    def forward(self, x):\n","        features = self.contrastive_head(self.backbone(x))\n","        features = F.normalize(features, dim = 1)\n","        return features\n"],"metadata":{"id":"0Ke-BSQ4tW9g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##ClusteringModel"],"metadata":{"id":"2xxEoarPtaEt"}},{"cell_type":"code","source":["class ClusteringModel(nn.Module):\n","    def __init__(self, backbone, class_num):\n","        super(ClusteringModel, self).__init__()\n","        self.backbone = backbone\n","        self.cluster_head = nn.ModuleList([nn.Linear(512, class_num) for _ in range(1)])\n","\n","    def forward(self, x):\n","        features = self.backbone(x)\n","        out = [cluster_head(features) for cluster_head in self.cluster_head]\n","\n","        return out[0]"],"metadata":{"id":"3Uwrl9qXtbW2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##RandAugmentMC"],"metadata":{"id":"pxFSgyEEtdUy"}},{"cell_type":"code","source":["PARAMETER_MAX = 10\n","\n","def AutoContrast(img, **kwarg):\n","    return PIL.ImageOps.autocontrast(img)\n","\n","\n","def Brightness(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    return PIL.ImageEnhance.Brightness(img).enhance(v)\n","\n","\n","def Color(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    return PIL.ImageEnhance.Color(img).enhance(v)\n","\n","\n","def Contrast(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    return PIL.ImageEnhance.Contrast(img).enhance(v)\n","\n","\n","def Cutout(img, v, max_v, bias=0):\n","    if v == 0:\n","        return img\n","    v = _float_parameter(v, max_v) + bias\n","    v = int(v * min(img.size))\n","    return CutoutAbs(img, v)\n","\n","\n","def CutoutAbs(img, v, **kwarg):\n","    w, h = img.size\n","    x0 = np.random.uniform(0, w)\n","    y0 = np.random.uniform(0, h)\n","    x0 = int(max(0, x0 - v / 2.))\n","    y0 = int(max(0, y0 - v / 2.))\n","    x1 = int(min(w, x0 + v))\n","    y1 = int(min(h, y0 + v))\n","    xy = (x0, y0, x1, y1)\n","    # gray\n","    color = (127, 127, 127)\n","    img = img.copy()\n","    PIL.ImageDraw.Draw(img).rectangle(xy, color)\n","    return img\n","\n","\n","def Equalize(img, **kwarg):\n","    return PIL.ImageOps.equalize(img)\n","\n","\n","def Identity(img, **kwarg):\n","    return img\n","\n","\n","def Invert(img, **kwarg):\n","    return PIL.ImageOps.invert(img)\n","\n","\n","def Posterize(img, v, max_v, bias=0):\n","    v = _int_parameter(v, max_v) + bias\n","    return PIL.ImageOps.posterize(img, v)\n","\n","\n","def Rotate(img, v, max_v, bias=0):\n","    v = _int_parameter(v, max_v) + bias\n","    if random.random() < 0.5:\n","        v = -v\n","    return img.rotate(v)\n","\n","\n","def Sharpness(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    return PIL.ImageEnhance.Sharpness(img).enhance(v)\n","\n","\n","def ShearX(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    if random.random() < 0.5:\n","        v = -v\n","    return img.transform(img.size, PIL.Image.AFFINE, (1, v, 0, 0, 1, 0))\n","\n","\n","def ShearY(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    if random.random() < 0.5:\n","        v = -v\n","    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, v, 1, 0))\n","\n","\n","def Solarize(img, v, max_v, bias=0):\n","    v = _int_parameter(v, max_v) + bias\n","    return PIL.ImageOps.solarize(img, 256 - v)\n","\n","\n","def SolarizeAdd(img, v, max_v, bias=0, threshold=128):\n","    v = _int_parameter(v, max_v) + bias\n","    if random.random() < 0.5:\n","        v = -v\n","    img_np = np.array(img).astype(np.int)\n","    img_np = img_np + v\n","    img_np = np.clip(img_np, 0, 255)\n","    img_np = img_np.astype(np.uint8)\n","    img = Image.fromarray(img_np)\n","    return PIL.ImageOps.solarize(img, threshold)\n","\n","\n","def TranslateX(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    if random.random() < 0.5:\n","        v = -v\n","    v = int(v * img.size[0])\n","    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n","\n","\n","def TranslateY(img, v, max_v, bias=0):\n","    v = _float_parameter(v, max_v) + bias\n","    if random.random() < 0.5:\n","        v = -v\n","    v = int(v * img.size[1])\n","    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n","\n","\n","def _float_parameter(v, max_v):\n","    return float(v) * max_v / PARAMETER_MAX\n","\n","\n","def _int_parameter(v, max_v):\n","    return int(v * max_v / PARAMETER_MAX)\n","\n","\n","def augment_pool():\n","    # FixMatch paper\n","    augs = [(AutoContrast, None, None),\n","            (Brightness, 0.99, 0.01),\n","            (Color, 0.99, 0.01),\n","            (Contrast, 0.99, 0.01),\n","            (Equalize, None, None),\n","            (Identity, None, None),\n","            (Posterize, 1, 8),\n","            (Rotate, 45, -45),\n","            (Sharpness, 0.99, 0.01),\n","            (ShearX, 0.3, -0.3),\n","            (ShearY, 0.3, -0.3),\n","            (Solarize, 256, 0),\n","            (TranslateX, 0.3, -0.3),\n","            (TranslateY, 0.3, -0.3)]\n","    return augs\n","\n","\n","class RandAugmentMC(object):\n","    def __init__(self, n, m):\n","        assert n >= 1\n","        assert 1 <= m <= 10\n","        self.n = n\n","        self.m = m\n","        self.augment_pool = augment_pool()\n","\n","    def __call__(self, img):\n","        ops = random.sample(self.augment_pool, k=self.n)\n","        for op, max_v, bias in ops:\n","            v = np.random.randint(1, self.m)\n","            if random.random() < 0.5:\n","                img = op(img, v=v, max_v=max_v, bias=bias)\n","        img = CutoutAbs(img, 16)\n","        return img"],"metadata":{"id":"_-tnNpkgteOv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##AverageMeter"],"metadata":{"id":"09EZGFbQtiPl"}},{"cell_type":"code","source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"metadata":{"id":"rnIM0n_ItjTW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##_hungarian_match"],"metadata":{"id":"RUMx6tTutk7H"}},{"cell_type":"code","source":["def _hungarian_match(flat_preds, flat_targets, num_samples, class_num):\n","    num_k = class_num\n","    num_correct = np.zeros((num_k, num_k))\n","\n","    for c1 in range(0, num_k):\n","        for c2 in range(0, num_k):\n","        # elementwise, so each sample contributes once\n","            votes = int(((flat_preds == c1) * (flat_targets == c2)).sum())\n","            num_correct[c1, c2] = votes\n","\n","    # num_correct is small\n","    match = linear_assignment(num_samples - num_correct)\n","\n","    # return as list of tuples, out_c to gt_c\n","    res = []\n","    for i in range(len(match[0])):\n","        out_c = match[0][i]\n","        gt_c = match[1][i]\n","        res.append((out_c, gt_c))\n","\n","    return res"],"metadata":{"id":"okAYHrsAtmXy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##test"],"metadata":{"id":"_PuLHG6ktorv"}},{"cell_type":"code","source":["def test(net, testloader,device, class_num):\n","    net.eval()\n","    predicted_all = []\n","    targets_all = []\n","    for batch_idx, (inputs, _,_, targets, indexes) in enumerate(testloader):\n","        batchSize = inputs.size(0)\n","        targets, inputs = targets.to(device), inputs.to(device)\n","        output = net(inputs)\n","        predicted = torch.argmax(output, 1)\n","        predicted_all.append(predicted)\n","        targets_all.append(targets)\n","\n","\n","    flat_predict = torch.cat(predicted_all).to(device)\n","    flat_target = torch.cat(targets_all).to(device)\n","    num_samples = flat_predict.shape[0]\n","    match = _hungarian_match(flat_predict, flat_target, num_samples, class_num)\n","    reordered_preds = torch.zeros(num_samples).to(device)\n","\n","    for pred_i, target_i in match:\n","        reordered_preds[flat_predict == pred_i] = int(target_i)\n","\n","    acc = int((reordered_preds == flat_target.float()).sum()) / float(num_samples) * 100\n","\n","    return acc, reordered_preds"],"metadata":{"id":"QYmCOW2YtpPW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##test_ruc"],"metadata":{"id":"hIn2Iw8htsjj"}},{"cell_type":"code","source":["def test_ruc(net, net2, testloader, device, class_num):\n","    net.eval()\n","    net2.eval()\n","\n","    predicted_all = [[] for i in range(0,3)]\n","    targets_all = []\n","    acc_list = []\n","    p_label_list = []\n","\n","    for batch_idx, (inputs, _, _, targets, indexes) in enumerate(testloader):\n","        batchSize = inputs.size(0)\n","        targets, inputs = targets.to(device), inputs.to(device)\n","        logit = net(inputs)\n","        logit2 = net2(inputs)\n","        _, predicted = torch.max(logit, 1)\n","        _, predicted2 = torch.max(logit2, 1)\n","        _, predicted3 = torch.max(logit + logit2, 1)\n","\n","        predicted_all[0].append(predicted)\n","        predicted_all[1].append(predicted2)\n","        predicted_all[2].append(predicted3)\n","        targets_all.append(targets)\n","\n","    for i in range(0, 3):\n","        flat_predict = torch.cat(predicted_all[i]).to(device)\n","        flat_target = torch.cat(targets_all).to(device)\n","        num_samples = flat_predict.shape[0]\n","        acc = int((flat_predict.float() == flat_target.float()).sum()) / float(num_samples) * 100\n","        acc_list.append(acc)\n","        p_label_list.append(flat_predict)\n","\n","    return acc_list, p_label_list"],"metadata":{"id":"NBIPB3-ttt9z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#CIFAR20"],"metadata":{"id":"8ri0Onoft3Mf"}},{"cell_type":"markdown","source":["##Default variables"],"metadata":{"id":"t1DodTuZt8cz"}},{"cell_type":"code","source":["# Default variables\n","lr = 0.01\n","momentum = 0.9\n","weight_decay = 5e-4\n","epochs = 200\n","batch_size = 250\n","s_thr = 0.99\n","n_num = 100\n","o_model = 'checkpoint/selflabel_cifar-20.pth.tar'\n","e_model = 'checkpoint/simclr_cifar-20.pth.tar'\n","seed = 1567010775"],"metadata":{"id":"m8rAk01Wt9Xs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","np.random.seed(seed)"],"metadata":{"id":"4a3k4_jnuAOV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Check if the GPU is available"],"metadata":{"id":"jatO4oRYuBiw"}},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"JsOAF3XRuCn8","executionInfo":{"status":"ok","timestamp":1700161987015,"user_tz":300,"elapsed":5,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}},"outputId":"7e2f51cc-07ba-44bf-9780-9bffe15d3247"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["##cifar100_to_cifar20"],"metadata":{"id":"EaOhESK9uFx_"}},{"cell_type":"code","source":["def cifar100_to_cifar20(target):\n","    \"\"\"\n","    CIFAR100 to CIFAR 20 dictionary.\n","    This function is from IIC github.\n","    \"\"\"\n","\n","    class_dict = {0: 4,\n","     1: 1,\n","     2: 14,\n","     3: 8,\n","     4: 0,\n","     5: 6,\n","     6: 7,\n","     7: 7,\n","     8: 18,\n","     9: 3,\n","     10: 3,\n","     11: 14,\n","     12: 9,\n","     13: 18,\n","     14: 7,\n","     15: 11,\n","     16: 3,\n","     17: 9,\n","     18: 7,\n","     19: 11,\n","     20: 6,\n","     21: 11,\n","     22: 5,\n","     23: 10,\n","     24: 7,\n","     25: 6,\n","     26: 13,\n","     27: 15,\n","     28: 3,\n","     29: 15,\n","     30: 0,\n","     31: 11,\n","     32: 1,\n","     33: 10,\n","     34: 12,\n","     35: 14,\n","     36: 16,\n","     37: 9,\n","     38: 11,\n","     39: 5,\n","     40: 5,\n","     41: 19,\n","     42: 8,\n","     43: 8,\n","     44: 15,\n","     45: 13,\n","     46: 14,\n","     47: 17,\n","     48: 18,\n","     49: 10,\n","     50: 16,\n","     51: 4,\n","     52: 17,\n","     53: 4,\n","     54: 2,\n","     55: 0,\n","     56: 17,\n","     57: 4,\n","     58: 18,\n","     59: 17,\n","     60: 10,\n","     61: 3,\n","     62: 2,\n","     63: 12,\n","     64: 12,\n","     65: 16,\n","     66: 12,\n","     67: 1,\n","     68: 9,\n","     69: 19,\n","     70: 2,\n","     71: 10,\n","     72: 0,\n","     73: 1,\n","     74: 16,\n","     75: 12,\n","     76: 9,\n","     77: 13,\n","     78: 15,\n","     79: 13,\n","     80: 16,\n","     81: 19,\n","     82: 2,\n","     83: 4,\n","     84: 6,\n","     85: 19,\n","     86: 5,\n","     87: 5,\n","     88: 8,\n","     89: 19,\n","     90: 18,\n","     91: 1,\n","     92: 2,\n","     93: 15,\n","     94: 6,\n","     95: 0,\n","     96: 17,\n","     97: 8,\n","     98: 14,\n","     99: 13}\n","\n","    return class_dict[target]"],"metadata":{"id":"pCvRpNY_uGus"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##CIFAR20RUC"],"metadata":{"id":"91IepcOouKQ3"}},{"cell_type":"code","source":["class CIFAR20RUC(torchdivision_datasets.CIFAR100):\n","    def __init__(self, root, transform, transform2, transform3, transform4=None, target_transform=None,train=True, download = False):\n","        self.root = root\n","        self.train = train  # training set or test set\n","        self.transform = transform\n","        self.transform2 = transform2\n","        self.transform3 = transform3\n","        self.transform4 = transform4\n","\n","        if download:\n","            self.download()\n","\n","        if not self._check_integrity():\n","            raise RuntimeError('Dataset not found or corrupted.' +\n","                               ' You can use download=True to download it')\n","\n","        if self.train:\n","            downloaded_list = self.train_list\n","        else:\n","            downloaded_list = self.test_list\n","\n","        self.data = []\n","        self.targets = []\n","\n","        # now load the picked numpy arrays\n","        for file_name, checksum in downloaded_list:\n","            file_path = os.path.join(self.root, self.base_folder, file_name)\n","            with open(file_path, 'rb') as f:\n","                entry = pickle.load(f, encoding='latin1')\n","                self.data.append(entry['data'])\n","                if 'labels' in entry:\n","                    self.targets.extend(entry['labels'])\n","                else:\n","                    self.targets.extend(entry['fine_labels'])\n","\n","        self.data = np.vstack(self.data).reshape(-1, 3, 32, 32)\n","        self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n","        self._load_meta()\n","\n","    def __getitem__(self, index) :\n","        img, target = self.data[index], cifar100_to_cifar20(self.targets[index])\n","        img = Image.fromarray(img)\n","\n","        if self.transform is not None:\n","            img1 = self.transform(img)\n","            img2 = self.transform2(img)\n","            img3 = self.transform3(img)\n","\n","        if self.transform4 != None:\n","            img4 = self.transform4(img)\n","            return img1, img2, img3, img4, target, index\n","        else:\n","            return img1, img2, img3, target, index\n","\n","        return img1, img2, img3, target, index"],"metadata":{"id":"hDcp_UDJuLnJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##preprocess"],"metadata":{"id":"13dZB8TSuPLi"}},{"cell_type":"code","source":["def preprocess():\n","    mean = [0.5071, 0.4867, 0.4408]\n","    std = [0.2675, 0.2565, 0.2761]\n","    transform_train = transforms.Compose([\n","        transforms.RandomResizedCrop(size=32, scale=(0.2,1.)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=mean, std=std),\n","    ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.Resize(32),\n","        transforms.CenterCrop(32),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=mean, std=std),\n","    ])\n","\n","    transform_strong = transforms.Compose([\n","            transforms.RandomResizedCrop(size=32, scale=(0.2,1.)),\n","            transforms.RandomHorizontalFlip(),\n","            RandAugmentMC(n=2, m=2),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=mean, std=std)])\n","\n","    trainset = CIFAR20RUC(root=\"./data/cifar-20\", transform=transform_test, transform2 = transform_train, transform3 = transform_train, transform4 = transform_strong, download=True)\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=False)\n","    testset = CIFAR20RUC(root=\"./data/cifar-20\",transform=transform_test, transform2 = transform_test, transform3 = transform_test,  download=False)\n","    evalloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=4)\n","\n","    return trainset, trainloader, testset, evalloader, 20"],"metadata":{"id":"N8pm285zuQWN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Download and split dataset"],"metadata":{"id":"D_2ZzBINuTjn"}},{"cell_type":"code","source":["trainset, trainloader, testset, evalloader, class_num  = preprocess()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kzHcwbRAuUd0","executionInfo":{"status":"ok","timestamp":1700161996951,"user_tz":300,"elapsed":9940,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}},"outputId":"5d755584-043a-410e-fa44-65376ebd3977"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["net = ClusteringModel(Resnet_CIFAR(), class_num)"],"metadata":{"id":"-NqcWqt5unNS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net2 = copy.deepcopy(net)"],"metadata":{"id":"Wcuy3M1Iuovw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net_uc = copy.deepcopy(net)"],"metadata":{"id":"ZwJsNxlsuqIf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net_embd = ContrastiveModel(Resnet_CIFAR())"],"metadata":{"id":"XXJZqfIGurVu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["try:\n","    state_dict = torch.load(o_model)\n","    state_dict2 = torch.load(e_model)\n","    net_uc.load_state_dict(state_dict)\n","    net_embd.load_state_dict(state_dict2, strict = True)\n","    net.load_state_dict(state_dict, strict = False)\n","    net2.load_state_dict(state_dict, strict = False)\n","    net.cluster_head = nn.ModuleList([nn.Linear(512, class_num) for _ in range(1)])\n","    net2.cluster_head = nn.ModuleList([nn.Linear(512, class_num) for _ in range(1)])\n","except:\n","    print(\"Check Model Directory!\")\n","    # exit(0)"],"metadata":{"id":"9dxOvAY3ut1n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(device)\n","if device == 'cuda':\n","  net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n","  net2 = torch.nn.DataParallel(net2, device_ids=range(torch.cuda.device_count()))\n","  net_uc = torch.nn.DataParallel(net_uc, device_ids=range(torch.cuda.device_count()))\n","  net_embd = torch.nn.DataParallel(net_uc, device_ids=range(torch.cuda.device_count()))\n","  cudnn.benchmark = True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yqOSiBmcu_eN","executionInfo":{"status":"ok","timestamp":1700162006565,"user_tz":300,"elapsed":3,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}},"outputId":"ce02223a-ad65-4c36-fb49-ac8a07fd4602"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["net.to(device)\n","net2.to(device)\n","net_uc.to(device)\n","net_embd.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0r0Jw0IVvyzT","executionInfo":{"status":"ok","timestamp":1700162006921,"user_tz":300,"elapsed":358,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}},"outputId":"ca66348d-6393-4cf8-85f0-272b5cb87beb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataParallel(\n","  (module): DataParallel(\n","    (module): ClusteringModel(\n","      (backbone): ResNet(\n","        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (layer1): Sequential(\n","          (0): BasicBlock(\n","            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (shortcut): Sequential()\n","          )\n","          (1): BasicBlock(\n","            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (shortcut): Sequential()\n","          )\n","        )\n","        (layer2): Sequential(\n","          (0): BasicBlock(\n","            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (shortcut): Sequential(\n","              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (1): BasicBlock(\n","            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (shortcut): Sequential()\n","          )\n","        )\n","        (layer3): Sequential(\n","          (0): BasicBlock(\n","            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (shortcut): Sequential(\n","              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (1): BasicBlock(\n","            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (shortcut): Sequential()\n","          )\n","        )\n","        (layer4): Sequential(\n","          (0): BasicBlock(\n","            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (shortcut): Sequential(\n","              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (1): BasicBlock(\n","            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (shortcut): Sequential()\n","          )\n","        )\n","        (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","      )\n","      (cluster_head): ModuleList(\n","        (0): Linear(in_features=512, out_features=20, bias=True)\n","      )\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["##linear_rampup"],"metadata":{"id":"MCna5-UmxYkD"}},{"cell_type":"code","source":["def linear_rampup(current, rampup_length=200):\n","  if rampup_length == 0:\n","    return 1.0\n","  else:\n","    current = np.clip((current) / rampup_length, 0.1, 1.0)\n","    return float(current)"],"metadata":{"id":"vhFRv1zgxZnk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##criterion_rb"],"metadata":{"id":"IJywSCoQwstE"}},{"cell_type":"code","source":["class criterion_rb(object):\n","  def __call__(self, outputs_x, targets_x, outputs_u, targets_u, epoch):\n","    # Clean sample Loss\n","    probs_u = torch.softmax(outputs_u, dim=1)\n","    Lx = -torch.mean(torch.sum(F.log_softmax(outputs_x, dim=1) * targets_x, dim=1))\n","    # Pseudo Label Loss\n","    Lu = 100*torch.mean((probs_u - targets_u)**2)\n","    Lu = linear_rampup(epoch) * Lu\n","    # Total Loss\n","    return Lx, Lu"],"metadata":{"id":"-bNJshAcwtqw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer1 = torch.optim.SGD(net.parameters(), lr, momentum=momentum, weight_decay=weight_decay, nesterov=True)\n","optimizer2 = torch.optim.SGD(net2.parameters(), lr, momentum=momentum, weight_decay=weight_decay, nesterov=True)\n","criterion = criterion_rb()"],"metadata":{"id":"t207ywWAv_3p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##extract_confidence"],"metadata":{"id":"5Jr0RG4pyM6i"}},{"cell_type":"code","source":["def extract_confidence(net, p_label, evalloader, threshold):\n","  net.eval()\n","  devide = torch.tensor([]).cuda()\n","  clean_num = 0\n","  correct_num = 0\n","  for batch_idx, (inputs1, _, _, targets, indexes) in enumerate(evalloader):\n","    inputs1, targets = inputs1.cuda(), targets.cuda().float()\n","    labels = p_label[indexes].float()\n","    logits = net(inputs1)\n","    prob = torch.softmax(logits.detach_(), dim=-1)\n","    max_probs, _ = torch.max(prob, dim=-1)\n","    mask = max_probs.ge(threshold).float()\n","    devide = torch.cat([devide, mask])\n","    s_idx = (mask == 1)\n","    clean_num += labels[s_idx].shape[0]\n","    correct_num += torch.sum((labels[s_idx] == targets[s_idx])).item()\n","\n","  print(correct_num, clean_num)\n","  return devide"],"metadata":{"id":"d3t-40DWySKa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##extract_metric"],"metadata":{"id":"ZHfsgC0S0IWa"}},{"cell_type":"code","source":["def extract_metric(net, p_label, evalloader, n_num):\n","    net.eval()\n","    feature_bank = []\n","    with torch.no_grad():\n","        for batch_idx, (inputs1 , _, _, _, indexes) in enumerate(evalloader):\n","            out = net(inputs1.cuda())\n","            feature_bank.append(out)\n","        feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()\n","        sim_indices_list = []\n","        for batch_idx, (inputs1 , _, _, _, indexes) in enumerate(evalloader):\n","            out = net(inputs1.cuda(non_blocking=True))\n","            sim_matrix = torch.mm(out, feature_bank)\n","            _, sim_indices = sim_matrix.topk(k=n_num, dim=-1)\n","            sim_indices_list.append(sim_indices)\n","        feature_labels = p_label.cuda()\n","        first = True\n","        count = 0\n","        clean_num = 0\n","        correct_num = 0\n","        for batch_idx, (inputs1 , _, _, targets, indexes) in enumerate(evalloader):\n","            # labels = p_label[indexes].cuda().long()\n","            labels = p_label[indexes.to(device)].long()\n","\n","            sim_indices = sim_indices_list[count]\n","            sim_labels = torch.gather(feature_labels.expand(inputs1.size(0), -1), dim=-1, index=sim_indices)\n","            # counts for each class\n","            one_hot_label = torch.zeros(inputs1.size(0) * sim_indices.size(1), 20).cuda()\n","            one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1).long(), value=1.0)\n","            pred_scores = torch.sum(one_hot_label.view(inputs1.size(0), -1, 20), dim=1)\n","            count += 1\n","            pred_labels = pred_scores.argsort(dim=-1, descending=True)\n","            prob, _ = torch.max(F.softmax(pred_scores, dim=-1), 1)\n","            # Check whether prediction and current label are same\n","            noisy_label = labels\n","            s_idx1 = (pred_labels[:, :1].float() == labels.unsqueeze(dim=-1).float()).any(dim=-1).float()\n","            s_idx = (s_idx1 == 1.0)\n","            clean_num += labels[s_idx].shape[0]\n","            # correct_num += torch.sum((labels[s_idx].float() == targets[s_idx].cuda().float())).item()\n","\n","            correct_num += torch.sum((labels[s_idx].float() == targets[s_idx.to(targets.device)].cuda().float())).item()\n","\n","\n","\n","            if first:\n","                prob_set = prob\n","                pred_same_label_set = s_idx\n","                first = False\n","            else:\n","                prob_set = torch.cat((prob_set, prob), dim = 0)\n","                pred_same_label_set = torch.cat((pred_same_label_set, s_idx), dim = 0)\n","\n","        print(correct_num, clean_num)\n","        return pred_same_label_set"],"metadata":{"id":"CrIdFPmW0JI9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##extract_hybrid"],"metadata":{"id":"pqJhHhv30vBM"}},{"cell_type":"code","source":["def extract_hybrid(devide1, devide2, p_label, evalloader):\n","    devide = (devide1.float() + devide2.float() == 2)\n","    clean_num = 0\n","    correct_num = 0\n","    for batch_idx, (inputs1, _, _, targets, indexes) in enumerate(evalloader):\n","        inputs1, targets = inputs1.cuda(), targets.cuda().float()\n","        labels = p_label[indexes].float()\n","        mask = devide[indexes]\n","        s_idx = (mask == 1)\n","        clean_num += labels[s_idx].shape[0]\n","        correct_num += torch.sum((labels[s_idx] == targets[s_idx])).item()\n","\n","    print(correct_num, clean_num)\n","    return devide"],"metadata":{"id":"_Mz0rMZO0vwU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract Pseudo Label\n","acc_uc, p_label = test(net_uc, evalloader, device, class_num)\n","print(acc_uc)\n","devide1 = extract_confidence(net_uc, p_label, evalloader, s_thr)\n","devide2 = extract_metric(net_embd, p_label, evalloader, n_num)\n","devide = extract_hybrid(devide1, devide2, p_label, evalloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N7sgKrUNx3Vk","executionInfo":{"status":"ok","timestamp":1700162075949,"user_tz":300,"elapsed":69029,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}},"outputId":"5c852e5e-6df3-4cb2-b06a-df380b3e3d08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["50.605999999999995\n","21029 35331\n","24247 45335\n","21007 35225\n"]}]},{"cell_type":"code","source":["conf1 =  torch.zeros(50000)\n","conf2 =  torch.zeros(50000)"],"metadata":{"id":"yG5lvM0FAGV9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##LabelSmoothLoss"],"metadata":{"id":"xTHNHVRIeslV"}},{"cell_type":"code","source":["class LabelSmoothLoss(nn.Module):\n","\n","    def __init__(self, smoothing=0.0):\n","        super(LabelSmoothLoss, self).__init__()\n","        self.smoothing = smoothing\n","\n","    def forward(self, input, target):\n","        log_prob = F.log_softmax(input, dim=-1)\n","        weight = input.new_ones(input.size()) * \\\n","            self.smoothing / (input.size(-1) - 1.)\n","        weight.scatter_(-1, target.unsqueeze(-1).long(), (1. - self.smoothing))\n","        loss = (-weight * log_prob).sum(dim=-1).mean()\n","        return loss"],"metadata":{"id":"BhXUdCgheq-M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["LSloss = LabelSmoothLoss(0.5)"],"metadata":{"id":"VT24EPoeeqYM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Train"],"metadata":{"id":"wGtWPgy4A_-u"}},{"cell_type":"code","source":["def adjust_learning_rate(lr, epochs, optimizer, epoch):\n","    # cosine learning rate schedule\n","    lr = lr\n","    lr *= 0.5 * (1. + math.cos(math.pi * epoch / epochs))\n","\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr"],"metadata":{"id":"mum2SK61ce_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_threshold(current):\n","    return 0.9 + 0.02*int(current / 40)"],"metadata":{"id":"MuASpnvTd0RV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(epoch, net, net2, trainloader, optimizer, criterion_rb, devide, p_label, conf, batch_size):\n","    train_loss = AverageMeter()\n","    net.train()\n","    net2.train()\n","\n","    num_iter = (len(trainloader.dataset)//batch_size)+1\n","    # adjust learning rate\n","    adjust_learning_rate(lr, epochs, optimizer, epoch)\n","    optimizer.zero_grad()\n","    correct_u = 0\n","    unsupervised = 0\n","    conf_self = torch.zeros(50000)\n","    for batch_idx, (inputs1 , inputs2, inputs3, inputs4, targets, indexes) in enumerate(trainloader):\n","        inputs1, inputs2, inputs3, inputs4, targets = inputs1.float().cuda(), inputs2.float().cuda(), inputs3.float().cuda(), inputs4.float().cuda(), targets.cuda().long()\n","        s_idx = (devide[indexes] == 1)\n","        u_idx = (devide[indexes] == 0)\n","        labels = p_label[indexes].cuda().long()\n","        labels_x = torch.tensor(p_label[indexes][s_idx]).squeeze().long().cpu()\n","        target_x = torch.zeros(labels_x.shape[0], 20).scatter_(1, labels_x.view(-1,1), 1).float().cuda()\n","\n","        logit_o, logit_w1, logit_w2, logit_s = net(inputs1), net(inputs2), net(inputs3), net(inputs4)\n","        logit_s = logit_s[s_idx]\n","        max_probs, _ = torch.max(torch.softmax(logit_o, dim=1), dim=-1)\n","        conf_self[indexes] = max_probs.detach().cpu()\n","        optimizer.zero_grad()\n","\n","        with torch.no_grad():\n","            # compute guessed labels of unlabel samples\n","            outputs_u11 = logit_w1[u_idx]\n","            outputs_u21  = logit_w2[u_idx]\n","            logit_o2 = net2(inputs1)\n","            logit_w12 = net2(inputs2)\n","            logit_w22 = net2(inputs3)\n","            outputs_u12 = logit_w12[u_idx]\n","            outputs_u22  = logit_w22[u_idx]\n","            pu = (torch.softmax(outputs_u11, dim=1) + torch.softmax(outputs_u21, dim=1) + torch.softmax(outputs_u12, dim=1) + torch.softmax(outputs_u22, dim=1)) / 4\n","            ptu = pu**(1/0.5) # temparature sharpening\n","            target_u = ptu / ptu.sum(dim=1, keepdim=True) # normalize\n","            target_u = target_u.detach().float()\n","\n","            px = torch.softmax(logit_o2[s_idx], dim=1)\n","            indexes = indexes.cuda()\n","            conf = conf.cuda()\n","            w_x = conf[indexes][s_idx]\n","            w_x = w_x.view(-1,1).float().cuda()\n","            px = (1-w_x)*target_x + w_x*px\n","            ptx = px**(1/0.5) # temparature sharpening\n","            target_x = ptx / ptx.sum(dim=1, keepdim=True) # normalize\n","            target_x = target_x.detach().float()\n","\n","            if logit_o[u_idx].shape[0] > 0:\n","                max_probs, targets_u1 = torch.max(torch.softmax(logit_o[u_idx], dim=1), dim=-1)\n","                thr = get_threshold(epoch)\n","                mask_u = max_probs.ge(thr).float()\n","                u_idx2 = (mask_u == 1)\n","                unsupervised += torch.sum(mask_u).item()\n","                correct_u += torch.sum((targets_u1[u_idx2] == targets[u_idx][u_idx2])).item()\n","                update = indexes[u_idx][u_idx2]\n","                devide[update] = True\n","                p_label[update] = targets_u1[u_idx2].float()\n","\n","\n","        l = np.random.beta(4.0, 4.0)\n","        l = max(l, 1-l)\n","\n","        all_inputs = torch.cat([inputs2[s_idx], inputs3[s_idx], inputs2[u_idx], inputs3[u_idx]],dim=0)\n","        all_targets = torch.cat([target_x, target_x, target_u, target_u], dim=0)\n","        idx = torch.randperm(all_inputs.size(0))\n","\n","        input_a, input_b = all_inputs, all_inputs[idx]\n","        target_a, target_b = all_targets, all_targets[idx]\n","\n","        mixed_input = l * input_a + (1 - l) * input_b\n","        mixed_target = l * target_a + (1 - l) * target_b\n","\n","        logits = net(mixed_input)\n","        batch_size = target_x.shape[0]\n","\n","        Lx, Lu = criterion_rb(logits[:batch_size*2], mixed_target[:batch_size*2], logits[batch_size*2:], mixed_target[batch_size*2:], epoch+batch_idx/num_iter)\n","        total_loss = Lx + Lu + LSloss(logit_s, labels_x.cuda())\n","\n","        total_loss.backward()\n","        train_loss.update(total_loss.item(), inputs2.size(0))\n","        optimizer.step()\n","\n","        if batch_idx % 100 == 0:\n","            print('Epoch: [{epoch}][{elps_iters}/{tot_iters}] '\n","                  'Train loss: {train_loss.val:.4f} ({train_loss.avg:.4f}) '.format(\n","                      epoch=epoch, elps_iters=batch_idx,tot_iters=len(trainloader),\n","                      train_loss=train_loss))\n","    conf_self = (conf_self - conf_self.min()) / (conf_self.max() - conf_self.min())\n","    return train_loss.avg, devide, p_label, conf_self"],"metadata":{"id":"ozBzTHFjBB_l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(epochs):\n","  print(\"== Train RUC ==\")\n","  loss, devide, p_label, conf1 = train(epoch, net, net2, trainloader, optimizer1, criterion, devide, p_label, conf2, batch_size)\n","  loss, devide, p_label, conf2 = train(epoch, net2, net, trainloader, optimizer2, criterion, devide, p_label, conf1, batch_size)\n","  acc, p_list = test_ruc(net, net2, evalloader, device, class_num)\n","  print(\"accuracy: {}\\n\".format(acc))\n","\n","  state = {'net1': net.state_dict(),\n","            'net2': net2.state_dict() }\n","  torch.save(state, './checkpoint/ruc_cifar20.t7')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WYl6sfQhALOO","executionInfo":{"status":"ok","timestamp":1700175836458,"user_tz":300,"elapsed":13364546,"user":{"displayName":"Robin Rai","userId":"05210411039795673539"}},"outputId":"e2019f58-a668-4e14-eb84-da4c16a2db1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["== Train RUC ==\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-43-2753c2780110>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels_x = torch.tensor(p_label[indexes][s_idx]).squeeze().long().cpu()\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [0][0/200] Train loss: 6.1148 (6.1148) \n","Epoch: [0][100/200] Train loss: 4.7774 (4.8717) \n","Epoch: [0][0/200] Train loss: 6.1383 (6.1383) \n","Epoch: [0][100/200] Train loss: 4.1738 (4.8568) \n","accuracy: [51.562, 51.518, 51.742]\n","\n","== Train RUC ==\n","Epoch: [1][0/200] Train loss: 4.1537 (4.1537) \n","Epoch: [1][100/200] Train loss: 4.6410 (4.5229) \n","Epoch: [1][0/200] Train loss: 4.7681 (4.7681) \n","Epoch: [1][100/200] Train loss: 4.6218 (4.5474) \n","accuracy: [51.653999999999996, 51.172, 51.57000000000001]\n","\n","== Train RUC ==\n","Epoch: [2][0/200] Train loss: 3.9441 (3.9441) \n","Epoch: [2][100/200] Train loss: 4.1757 (4.4641) \n","Epoch: [2][0/200] Train loss: 4.7928 (4.7928) \n","Epoch: [2][100/200] Train loss: 4.6518 (4.5313) \n","accuracy: [51.605999999999995, 51.902, 51.912000000000006]\n","\n","== Train RUC ==\n","Epoch: [3][0/200] Train loss: 4.7140 (4.7140) \n","Epoch: [3][100/200] Train loss: 4.6651 (4.4295) \n","Epoch: [3][0/200] Train loss: 3.8848 (3.8848) \n","Epoch: [3][100/200] Train loss: 4.6252 (4.4238) \n","accuracy: [51.57000000000001, 51.92, 51.914]\n","\n","== Train RUC ==\n","Epoch: [4][0/200] Train loss: 4.5182 (4.5182) \n","Epoch: [4][100/200] Train loss: 3.7583 (4.4552) \n","Epoch: [4][0/200] Train loss: 4.4560 (4.4560) \n","Epoch: [4][100/200] Train loss: 4.2788 (4.3951) \n","accuracy: [51.734, 51.896, 51.99]\n","\n","== Train RUC ==\n","Epoch: [5][0/200] Train loss: 4.5759 (4.5759) \n","Epoch: [5][100/200] Train loss: 4.5205 (4.3849) \n","Epoch: [5][0/200] Train loss: 4.7732 (4.7732) \n","Epoch: [5][100/200] Train loss: 4.3959 (4.3632) \n","accuracy: [51.832, 52.062, 52.071999999999996]\n","\n","== Train RUC ==\n","Epoch: [6][0/200] Train loss: 4.6399 (4.6399) \n","Epoch: [6][100/200] Train loss: 4.4780 (4.4182) \n","Epoch: [6][0/200] Train loss: 4.0713 (4.0713) \n","Epoch: [6][100/200] Train loss: 4.5782 (4.3166) \n","accuracy: [52.004, 52.174, 52.276]\n","\n","== Train RUC ==\n","Epoch: [7][0/200] Train loss: 4.3297 (4.3297) \n","Epoch: [7][100/200] Train loss: 4.4193 (4.3759) \n","Epoch: [7][0/200] Train loss: 4.5444 (4.5444) \n","Epoch: [7][100/200] Train loss: 4.5622 (4.3485) \n","accuracy: [51.81400000000001, 52.002, 52.056000000000004]\n","\n","== Train RUC ==\n","Epoch: [8][0/200] Train loss: 4.5305 (4.5305) \n","Epoch: [8][100/200] Train loss: 4.6537 (4.3644) \n","Epoch: [8][0/200] Train loss: 3.8968 (3.8968) \n","Epoch: [8][100/200] Train loss: 4.5635 (4.3151) \n","accuracy: [51.94800000000001, 52.198, 52.222]\n","\n","== Train RUC ==\n","Epoch: [9][0/200] Train loss: 4.6040 (4.6040) \n","Epoch: [9][100/200] Train loss: 3.5685 (4.3792) \n","Epoch: [9][0/200] Train loss: 4.3155 (4.3155) \n","Epoch: [9][100/200] Train loss: 4.5513 (4.3284) \n","accuracy: [52.002, 52.128, 52.215999999999994]\n","\n","== Train RUC ==\n","Epoch: [10][0/200] Train loss: 3.8031 (3.8031) \n","Epoch: [10][100/200] Train loss: 4.5616 (4.2672) \n","Epoch: [10][0/200] Train loss: 4.2638 (4.2638) \n","Epoch: [10][100/200] Train loss: 4.5327 (4.2815) \n","accuracy: [51.944, 52.144, 52.205999999999996]\n","\n","== Train RUC ==\n","Epoch: [11][0/200] Train loss: 4.4935 (4.4935) \n","Epoch: [11][100/200] Train loss: 3.7152 (4.2847) \n","Epoch: [11][0/200] Train loss: 4.5757 (4.5757) \n","Epoch: [11][100/200] Train loss: 4.1237 (4.3134) \n","accuracy: [51.77, 52.22599999999999, 52.132]\n","\n","== Train RUC ==\n","Epoch: [12][0/200] Train loss: 4.2374 (4.2374) \n","Epoch: [12][100/200] Train loss: 4.3349 (4.3316) \n","Epoch: [12][0/200] Train loss: 4.4804 (4.4804) \n","Epoch: [12][100/200] Train loss: 4.4912 (4.3216) \n","accuracy: [52.308, 52.134, 52.356]\n","\n","== Train RUC ==\n","Epoch: [13][0/200] Train loss: 4.0743 (4.0743) \n","Epoch: [13][100/200] Train loss: 4.1611 (4.2751) \n","Epoch: [13][0/200] Train loss: 4.3596 (4.3596) \n","Epoch: [13][100/200] Train loss: 4.5268 (4.2991) \n","accuracy: [52.198, 51.944, 52.290000000000006]\n","\n","== Train RUC ==\n","Epoch: [14][0/200] Train loss: 4.3299 (4.3299) \n","Epoch: [14][100/200] Train loss: 4.3064 (4.2803) \n","Epoch: [14][0/200] Train loss: 4.4031 (4.4031) \n","Epoch: [14][100/200] Train loss: 4.4545 (4.3184) \n","accuracy: [52.176, 52.272, 52.418]\n","\n","== Train RUC ==\n","Epoch: [15][0/200] Train loss: 4.4439 (4.4439) \n","Epoch: [15][100/200] Train loss: 4.0782 (4.3059) \n","Epoch: [15][0/200] Train loss: 4.2176 (4.2176) \n","Epoch: [15][100/200] Train loss: 4.2179 (4.1958) \n","accuracy: [52.474, 52.11599999999999, 52.459999999999994]\n","\n","== Train RUC ==\n","Epoch: [16][0/200] Train loss: 3.8337 (3.8337) \n","Epoch: [16][100/200] Train loss: 4.3380 (4.2763) \n","Epoch: [16][0/200] Train loss: 3.7449 (3.7449) \n","Epoch: [16][100/200] Train loss: 4.1352 (4.2541) \n","accuracy: [52.34799999999999, 52.188, 52.418]\n","\n","== Train RUC ==\n","Epoch: [17][0/200] Train loss: 4.4327 (4.4327) \n","Epoch: [17][100/200] Train loss: 4.0341 (4.2654) \n","Epoch: [17][0/200] Train loss: 4.4090 (4.4090) \n","Epoch: [17][100/200] Train loss: 4.4224 (4.2666) \n","accuracy: [52.205999999999996, 52.382, 52.474]\n","\n","== Train RUC ==\n","Epoch: [18][0/200] Train loss: 4.1938 (4.1938) \n","Epoch: [18][100/200] Train loss: 4.1587 (4.2600) \n","Epoch: [18][0/200] Train loss: 4.3443 (4.3443) \n","Epoch: [18][100/200] Train loss: 3.6930 (4.2149) \n","accuracy: [52.276, 52.408, 52.476]\n","\n","== Train RUC ==\n","Epoch: [19][0/200] Train loss: 4.4212 (4.4212) \n","Epoch: [19][100/200] Train loss: 4.4142 (4.2572) \n","Epoch: [19][0/200] Train loss: 4.1330 (4.1330) \n","Epoch: [19][100/200] Train loss: 3.8674 (4.2346) \n","accuracy: [52.256, 52.256, 52.442]\n","\n","== Train RUC ==\n","Epoch: [20][0/200] Train loss: 4.0039 (4.0039) \n","Epoch: [20][100/200] Train loss: 4.4165 (4.2263) \n","Epoch: [20][0/200] Train loss: 4.2481 (4.2481) \n","Epoch: [20][100/200] Train loss: 4.3890 (4.2238) \n","accuracy: [52.332, 52.332, 52.48199999999999]\n","\n","== Train RUC ==\n","Epoch: [21][0/200] Train loss: 4.3833 (4.3833) \n","Epoch: [21][100/200] Train loss: 4.1981 (4.2602) \n","Epoch: [21][0/200] Train loss: 4.4016 (4.4016) \n","Epoch: [21][100/200] Train loss: 4.4224 (4.2099) \n","accuracy: [52.536, 52.322, 52.598]\n","\n","== Train RUC ==\n","Epoch: [22][0/200] Train loss: 4.2791 (4.2791) \n","Epoch: [22][100/200] Train loss: 3.7740 (4.2569) \n","Epoch: [22][0/200] Train loss: 4.2016 (4.2016) \n","Epoch: [22][100/200] Train loss: 4.4422 (4.2478) \n","accuracy: [52.224000000000004, 52.198, 52.444]\n","\n","== Train RUC ==\n","Epoch: [23][0/200] Train loss: 4.0505 (4.0505) \n","Epoch: [23][100/200] Train loss: 4.5197 (4.2247) \n","Epoch: [23][0/200] Train loss: 4.4046 (4.4046) \n","Epoch: [23][100/200] Train loss: 3.8746 (4.2226) \n","accuracy: [52.410000000000004, 52.349999999999994, 52.522000000000006]\n","\n","== Train RUC ==\n","Epoch: [24][0/200] Train loss: 4.1942 (4.1942) \n","Epoch: [24][100/200] Train loss: 4.3591 (4.2139) \n","Epoch: [24][0/200] Train loss: 4.2142 (4.2142) \n","Epoch: [24][100/200] Train loss: 4.1860 (4.2095) \n","accuracy: [52.361999999999995, 52.52, 52.592000000000006]\n","\n","== Train RUC ==\n","Epoch: [25][0/200] Train loss: 4.3399 (4.3399) \n","Epoch: [25][100/200] Train loss: 4.0430 (4.2600) \n","Epoch: [25][0/200] Train loss: 4.2992 (4.2992) \n","Epoch: [25][100/200] Train loss: 4.3446 (4.2725) \n","accuracy: [52.446000000000005, 52.364, 52.581999999999994]\n","\n","== Train RUC ==\n","Epoch: [26][0/200] Train loss: 4.4042 (4.4042) \n","Epoch: [26][100/200] Train loss: 4.2346 (4.2223) \n","Epoch: [26][0/200] Train loss: 4.3633 (4.3633) \n","Epoch: [26][100/200] Train loss: 4.2042 (4.2246) \n","accuracy: [52.416, 52.437999999999995, 52.61]\n","\n","== Train RUC ==\n","Epoch: [27][0/200] Train loss: 4.1514 (4.1514) \n","Epoch: [27][100/200] Train loss: 4.0530 (4.2378) \n","Epoch: [27][0/200] Train loss: 4.3241 (4.3241) \n","Epoch: [27][100/200] Train loss: 4.3696 (4.2536) \n","accuracy: [52.246, 52.332, 52.483999999999995]\n","\n","== Train RUC ==\n","Epoch: [28][0/200] Train loss: 4.2324 (4.2324) \n","Epoch: [28][100/200] Train loss: 3.9692 (4.1593) \n","Epoch: [28][0/200] Train loss: 4.3269 (4.3269) \n","Epoch: [28][100/200] Train loss: 4.2655 (4.2336) \n","accuracy: [52.5, 52.286, 52.6]\n","\n","== Train RUC ==\n","Epoch: [29][0/200] Train loss: 4.0243 (4.0243) \n","Epoch: [29][100/200] Train loss: 4.5273 (4.1754) \n","Epoch: [29][0/200] Train loss: 4.2811 (4.2811) \n","Epoch: [29][100/200] Train loss: 4.3939 (4.1868) \n","accuracy: [52.114000000000004, 52.690000000000005, 52.581999999999994]\n","\n","== Train RUC ==\n","Epoch: [30][0/200] Train loss: 4.3540 (4.3540) \n","Epoch: [30][100/200] Train loss: 4.2266 (4.2101) \n","Epoch: [30][0/200] Train loss: 4.3205 (4.3205) \n","Epoch: [30][100/200] Train loss: 4.3640 (4.2305) \n","accuracy: [52.43600000000001, 52.42400000000001, 52.652]\n","\n","== Train RUC ==\n","Epoch: [31][0/200] Train loss: 4.2174 (4.2174) \n","Epoch: [31][100/200] Train loss: 4.3813 (4.2563) \n","Epoch: [31][0/200] Train loss: 4.2216 (4.2216) \n","Epoch: [31][100/200] Train loss: 4.0250 (4.2166) \n","accuracy: [52.608, 52.727999999999994, 52.796]\n","\n","== Train RUC ==\n","Epoch: [32][0/200] Train loss: 4.3638 (4.3638) \n","Epoch: [32][100/200] Train loss: 4.3049 (4.2258) \n","Epoch: [32][0/200] Train loss: 4.4738 (4.4738) \n","Epoch: [32][100/200] Train loss: 3.9125 (4.2459) \n","accuracy: [52.66, 52.617999999999995, 52.858000000000004]\n","\n","== Train RUC ==\n","Epoch: [33][0/200] Train loss: 4.3486 (4.3486) \n","Epoch: [33][100/200] Train loss: 4.3256 (4.2305) \n","Epoch: [33][0/200] Train loss: 3.6382 (3.6382) \n","Epoch: [33][100/200] Train loss: 4.3340 (4.2128) \n","accuracy: [52.459999999999994, 52.480000000000004, 52.628]\n","\n","== Train RUC ==\n","Epoch: [34][0/200] Train loss: 4.3249 (4.3249) \n","Epoch: [34][100/200] Train loss: 4.2276 (4.2382) \n","Epoch: [34][0/200] Train loss: 4.4018 (4.4018) \n","Epoch: [34][100/200] Train loss: 4.1679 (4.2266) \n","accuracy: [52.532000000000004, 52.669999999999995, 52.796]\n","\n","== Train RUC ==\n","Epoch: [35][0/200] Train loss: 4.3328 (4.3328) \n","Epoch: [35][100/200] Train loss: 4.2850 (4.2197) \n","Epoch: [35][0/200] Train loss: 3.8491 (3.8491) \n","Epoch: [35][100/200] Train loss: 3.6996 (4.2543) \n","accuracy: [52.488, 52.562, 52.78]\n","\n","== Train RUC ==\n","Epoch: [36][0/200] Train loss: 4.2890 (4.2890) \n","Epoch: [36][100/200] Train loss: 3.9560 (4.2216) \n","Epoch: [36][0/200] Train loss: 4.3905 (4.3905) \n","Epoch: [36][100/200] Train loss: 3.7446 (4.2064) \n","accuracy: [52.636, 52.514, 52.800000000000004]\n","\n","== Train RUC ==\n","Epoch: [37][0/200] Train loss: 4.4627 (4.4627) \n","Epoch: [37][100/200] Train loss: 4.1679 (4.1987) \n","Epoch: [37][0/200] Train loss: 4.4187 (4.4187) \n","Epoch: [37][100/200] Train loss: 4.3693 (4.1966) \n","accuracy: [52.524, 52.416, 52.69200000000001]\n","\n","== Train RUC ==\n","Epoch: [38][0/200] Train loss: 4.4774 (4.4774) \n","Epoch: [38][100/200] Train loss: 4.2361 (4.2159) \n","Epoch: [38][0/200] Train loss: 4.3442 (4.3442) \n","Epoch: [38][100/200] Train loss: 4.2974 (4.2093) \n","accuracy: [52.55800000000001, 52.588, 52.800000000000004]\n","\n","== Train RUC ==\n","Epoch: [39][0/200] Train loss: 4.4679 (4.4679) \n","Epoch: [39][100/200] Train loss: 4.4957 (4.2326) \n","Epoch: [39][0/200] Train loss: 4.2175 (4.2175) \n","Epoch: [39][100/200] Train loss: 4.1307 (4.1928) \n","accuracy: [52.554, 52.86, 52.878]\n","\n","== Train RUC ==\n","Epoch: [40][0/200] Train loss: 4.1034 (4.1034) \n","Epoch: [40][100/200] Train loss: 4.1090 (4.2267) \n","Epoch: [40][0/200] Train loss: 3.7866 (3.7866) \n","Epoch: [40][100/200] Train loss: 4.2144 (4.2226) \n","accuracy: [52.571999999999996, 52.994, 52.978]\n","\n","== Train RUC ==\n","Epoch: [41][0/200] Train loss: 4.2759 (4.2759) \n","Epoch: [41][100/200] Train loss: 4.0981 (4.2345) \n","Epoch: [41][0/200] Train loss: 4.2555 (4.2555) \n","Epoch: [41][100/200] Train loss: 4.4906 (4.2352) \n","accuracy: [52.564, 52.614000000000004, 52.834]\n","\n","== Train RUC ==\n","Epoch: [42][0/200] Train loss: 4.4659 (4.4659) \n","Epoch: [42][100/200] Train loss: 4.3760 (4.2244) \n","Epoch: [42][0/200] Train loss: 4.2704 (4.2704) \n","Epoch: [42][100/200] Train loss: 4.4691 (4.2457) \n","accuracy: [52.571999999999996, 52.796, 52.93]\n","\n","== Train RUC ==\n","Epoch: [43][0/200] Train loss: 4.2336 (4.2336) \n","Epoch: [43][100/200] Train loss: 4.5630 (4.2630) \n","Epoch: [43][0/200] Train loss: 4.4347 (4.4347) \n","Epoch: [43][100/200] Train loss: 3.9544 (4.2251) \n","accuracy: [52.864, 52.746, 53.018]\n","\n","== Train RUC ==\n","Epoch: [44][0/200] Train loss: 3.4926 (3.4926) \n","Epoch: [44][100/200] Train loss: 4.3992 (4.2241) \n","Epoch: [44][0/200] Train loss: 4.2229 (4.2229) \n","Epoch: [44][100/200] Train loss: 4.3161 (4.2392) \n","accuracy: [52.738, 52.712, 52.918]\n","\n","== Train RUC ==\n","Epoch: [45][0/200] Train loss: 4.0277 (4.0277) \n","Epoch: [45][100/200] Train loss: 4.2091 (4.2224) \n","Epoch: [45][0/200] Train loss: 4.1450 (4.1450) \n","Epoch: [45][100/200] Train loss: 4.2120 (4.2248) \n","accuracy: [52.666000000000004, 52.596, 52.88]\n","\n","== Train RUC ==\n","Epoch: [46][0/200] Train loss: 4.1823 (4.1823) \n","Epoch: [46][100/200] Train loss: 3.5589 (4.2643) \n","Epoch: [46][0/200] Train loss: 4.0873 (4.0873) \n","Epoch: [46][100/200] Train loss: 4.1550 (4.1923) \n","accuracy: [52.59, 52.748, 52.876]\n","\n","== Train RUC ==\n","Epoch: [47][0/200] Train loss: 4.4617 (4.4617) \n","Epoch: [47][100/200] Train loss: 4.5389 (4.2561) \n","Epoch: [47][0/200] Train loss: 4.3230 (4.3230) \n","Epoch: [47][100/200] Train loss: 4.3421 (4.2043) \n","accuracy: [52.534000000000006, 52.408, 52.72]\n","\n","== Train RUC ==\n","Epoch: [48][0/200] Train loss: 4.4622 (4.4622) \n","Epoch: [48][100/200] Train loss: 4.4859 (4.2637) \n","Epoch: [48][0/200] Train loss: 4.4437 (4.4437) \n","Epoch: [48][100/200] Train loss: 4.3728 (4.2453) \n","accuracy: [52.614000000000004, 52.73, 52.83]\n","\n","== Train RUC ==\n","Epoch: [49][0/200] Train loss: 4.0230 (4.0230) \n","Epoch: [49][100/200] Train loss: 4.5883 (4.2644) \n","Epoch: [49][0/200] Train loss: 3.7313 (3.7313) \n","Epoch: [49][100/200] Train loss: 4.3405 (4.2580) \n","accuracy: [52.786, 52.676, 52.93]\n","\n","== Train RUC ==\n","Epoch: [50][0/200] Train loss: 4.2826 (4.2826) \n","Epoch: [50][100/200] Train loss: 4.3787 (4.2575) \n","Epoch: [50][0/200] Train loss: 4.4437 (4.4437) \n","Epoch: [50][100/200] Train loss: 3.7448 (4.2514) \n","accuracy: [52.734, 52.812000000000005, 52.954]\n","\n","== Train RUC ==\n","Epoch: [51][0/200] Train loss: 4.4179 (4.4179) \n","Epoch: [51][100/200] Train loss: 4.1612 (4.2503) \n","Epoch: [51][0/200] Train loss: 4.3188 (4.3188) \n","Epoch: [51][100/200] Train loss: 4.4353 (4.2687) \n","accuracy: [52.732, 52.898, 53.028]\n","\n","== Train RUC ==\n","Epoch: [52][0/200] Train loss: 4.1966 (4.1966) \n","Epoch: [52][100/200] Train loss: 4.3268 (4.2725) \n","Epoch: [52][0/200] Train loss: 4.3697 (4.3697) \n","Epoch: [52][100/200] Train loss: 4.2224 (4.2531) \n","accuracy: [52.872, 52.798, 53.022000000000006]\n","\n","== Train RUC ==\n","Epoch: [53][0/200] Train loss: 4.3048 (4.3048) \n","Epoch: [53][100/200] Train loss: 4.3660 (4.2785) \n","Epoch: [53][0/200] Train loss: 4.1476 (4.1476) \n","Epoch: [53][100/200] Train loss: 4.0545 (4.2603) \n","accuracy: [52.891999999999996, 52.634, 52.974]\n","\n","== Train RUC ==\n","Epoch: [54][0/200] Train loss: 4.4550 (4.4550) \n","Epoch: [54][100/200] Train loss: 4.4776 (4.2370) \n","Epoch: [54][0/200] Train loss: 4.5064 (4.5064) \n","Epoch: [54][100/200] Train loss: 4.2895 (4.2378) \n","accuracy: [52.78, 52.884, 53.024]\n","\n","== Train RUC ==\n","Epoch: [55][0/200] Train loss: 4.2446 (4.2446) \n","Epoch: [55][100/200] Train loss: 4.3769 (4.2768) \n","Epoch: [55][0/200] Train loss: 4.3922 (4.3922) \n","Epoch: [55][100/200] Train loss: 4.4813 (4.2987) \n","accuracy: [52.638, 52.878, 52.932]\n","\n","== Train RUC ==\n","Epoch: [56][0/200] Train loss: 4.3268 (4.3268) \n","Epoch: [56][100/200] Train loss: 4.0961 (4.2723) \n","Epoch: [56][0/200] Train loss: 3.7371 (3.7371) \n","Epoch: [56][100/200] Train loss: 4.4466 (4.2600) \n","accuracy: [52.616, 52.82, 52.896]\n","\n","== Train RUC ==\n","Epoch: [57][0/200] Train loss: 4.3340 (4.3340) \n","Epoch: [57][100/200] Train loss: 3.8245 (4.2713) \n","Epoch: [57][0/200] Train loss: 4.3610 (4.3610) \n","Epoch: [57][100/200] Train loss: 4.1788 (4.2490) \n","accuracy: [52.88, 52.903999999999996, 53.002]\n","\n","== Train RUC ==\n","Epoch: [58][0/200] Train loss: 4.4704 (4.4704) \n","Epoch: [58][100/200] Train loss: 4.2807 (4.2483) \n","Epoch: [58][0/200] Train loss: 4.4847 (4.4847) \n","Epoch: [58][100/200] Train loss: 4.5598 (4.2852) \n","accuracy: [52.71, 52.798, 52.884]\n","\n","== Train RUC ==\n","Epoch: [59][0/200] Train loss: 4.4605 (4.4605) \n","Epoch: [59][100/200] Train loss: 3.3627 (4.2970) \n","Epoch: [59][0/200] Train loss: 4.1307 (4.1307) \n","Epoch: [59][100/200] Train loss: 4.1797 (4.2856) \n","accuracy: [52.696, 52.925999999999995, 53.04]\n","\n","== Train RUC ==\n","Epoch: [60][0/200] Train loss: 3.8203 (3.8203) \n","Epoch: [60][100/200] Train loss: 4.2099 (4.2877) \n","Epoch: [60][0/200] Train loss: 3.9530 (3.9530) \n","Epoch: [60][100/200] Train loss: 4.4131 (4.2630) \n","accuracy: [52.736000000000004, 52.662, 52.88]\n","\n","== Train RUC ==\n","Epoch: [61][0/200] Train loss: 4.5072 (4.5072) \n","Epoch: [61][100/200] Train loss: 4.3468 (4.2873) \n","Epoch: [61][0/200] Train loss: 4.5429 (4.5429) \n","Epoch: [61][100/200] Train loss: 4.2780 (4.3030) \n","accuracy: [52.786, 52.766000000000005, 52.971999999999994]\n","\n","== Train RUC ==\n","Epoch: [62][0/200] Train loss: 4.1814 (4.1814) \n","Epoch: [62][100/200] Train loss: 4.4209 (4.2645) \n","Epoch: [62][0/200] Train loss: 3.5815 (3.5815) \n","Epoch: [62][100/200] Train loss: 4.0189 (4.2807) \n","accuracy: [52.908, 52.751999999999995, 53.081999999999994]\n","\n","== Train RUC ==\n","Epoch: [63][0/200] Train loss: 4.4980 (4.4980) \n","Epoch: [63][100/200] Train loss: 4.4351 (4.2589) \n","Epoch: [63][0/200] Train loss: 4.4829 (4.4829) \n","Epoch: [63][100/200] Train loss: 4.1231 (4.2803) \n","accuracy: [52.73, 52.949999999999996, 52.99]\n","\n","== Train RUC ==\n","Epoch: [64][0/200] Train loss: 4.2915 (4.2915) \n","Epoch: [64][100/200] Train loss: 4.5331 (4.3115) \n","Epoch: [64][0/200] Train loss: 4.3442 (4.3442) \n","Epoch: [64][100/200] Train loss: 4.2785 (4.2887) \n","accuracy: [53.025999999999996, 52.876, 53.11]\n","\n","== Train RUC ==\n","Epoch: [65][0/200] Train loss: 4.3467 (4.3467) \n","Epoch: [65][100/200] Train loss: 4.1265 (4.3191) \n","Epoch: [65][0/200] Train loss: 4.3220 (4.3220) \n","Epoch: [65][100/200] Train loss: 4.1992 (4.2901) \n","accuracy: [52.788000000000004, 52.988, 53.062]\n","\n","== Train RUC ==\n","Epoch: [66][0/200] Train loss: 3.9914 (3.9914) \n","Epoch: [66][100/200] Train loss: 4.5247 (4.3049) \n","Epoch: [66][0/200] Train loss: 4.2591 (4.2591) \n","Epoch: [66][100/200] Train loss: 3.8794 (4.2653) \n","accuracy: [52.674, 52.815999999999995, 52.93]\n","\n","== Train RUC ==\n","Epoch: [67][0/200] Train loss: 4.3382 (4.3382) \n","Epoch: [67][100/200] Train loss: 4.3927 (4.3437) \n","Epoch: [67][0/200] Train loss: 4.3958 (4.3958) \n","Epoch: [67][100/200] Train loss: 4.4313 (4.2714) \n","accuracy: [53.03, 52.964, 53.20399999999999]\n","\n","== Train RUC ==\n","Epoch: [68][0/200] Train loss: 3.6479 (3.6479) \n","Epoch: [68][100/200] Train loss: 4.3236 (4.2708) \n","Epoch: [68][0/200] Train loss: 4.2717 (4.2717) \n","Epoch: [68][100/200] Train loss: 3.3421 (4.2773) \n","accuracy: [53.215999999999994, 52.944, 53.266000000000005]\n","\n","== Train RUC ==\n","Epoch: [69][0/200] Train loss: 4.4193 (4.4193) \n","Epoch: [69][100/200] Train loss: 4.5538 (4.2913) \n","Epoch: [69][0/200] Train loss: 4.4875 (4.4875) \n","Epoch: [69][100/200] Train loss: 4.4221 (4.2718) \n","accuracy: [53.128, 53.146, 53.364]\n","\n","== Train RUC ==\n","Epoch: [70][0/200] Train loss: 4.3573 (4.3573) \n","Epoch: [70][100/200] Train loss: 4.5970 (4.3045) \n","Epoch: [70][0/200] Train loss: 4.4354 (4.4354) \n","Epoch: [70][100/200] Train loss: 4.3861 (4.3406) \n","accuracy: [52.902, 52.858000000000004, 53.056000000000004]\n","\n","== Train RUC ==\n","Epoch: [71][0/200] Train loss: 4.5040 (4.5040) \n","Epoch: [71][100/200] Train loss: 4.4140 (4.3323) \n","Epoch: [71][0/200] Train loss: 3.8715 (3.8715) \n","Epoch: [71][100/200] Train loss: 4.3871 (4.3238) \n","accuracy: [52.86, 52.92, 53.09199999999999]\n","\n","== Train RUC ==\n","Epoch: [72][0/200] Train loss: 4.3438 (4.3438) \n","Epoch: [72][100/200] Train loss: 4.4738 (4.3158) \n","Epoch: [72][0/200] Train loss: 4.3610 (4.3610) \n","Epoch: [72][100/200] Train loss: 4.4865 (4.3095) \n","accuracy: [52.93600000000001, 52.954, 53.083999999999996]\n","\n","== Train RUC ==\n","Epoch: [73][0/200] Train loss: 4.4857 (4.4857) \n","Epoch: [73][100/200] Train loss: 4.4987 (4.3406) \n","Epoch: [73][0/200] Train loss: 4.1371 (4.1371) \n","Epoch: [73][100/200] Train loss: 4.2472 (4.3356) \n","accuracy: [52.978, 52.82, 53.112]\n","\n","== Train RUC ==\n","Epoch: [74][0/200] Train loss: 4.4385 (4.4385) \n","Epoch: [74][100/200] Train loss: 4.3403 (4.3370) \n","Epoch: [74][0/200] Train loss: 4.4665 (4.4665) \n","Epoch: [74][100/200] Train loss: 4.1689 (4.3252) \n","accuracy: [53.074, 52.932, 53.227999999999994]\n","\n","== Train RUC ==\n","Epoch: [75][0/200] Train loss: 3.7200 (3.7200) \n","Epoch: [75][100/200] Train loss: 4.1524 (4.3412) \n","Epoch: [75][0/200] Train loss: 4.3847 (4.3847) \n","Epoch: [75][100/200] Train loss: 4.0031 (4.3057) \n","accuracy: [53.013999999999996, 53.088, 53.190000000000005]\n","\n","== Train RUC ==\n","Epoch: [76][0/200] Train loss: 4.4784 (4.4784) \n","Epoch: [76][100/200] Train loss: 3.9326 (4.3192) \n","Epoch: [76][0/200] Train loss: 4.3287 (4.3287) \n","Epoch: [76][100/200] Train loss: 4.3076 (4.2991) \n","accuracy: [53.105999999999995, 53.232, 53.286]\n","\n","== Train RUC ==\n","Epoch: [77][0/200] Train loss: 4.4057 (4.4057) \n","Epoch: [77][100/200] Train loss: 4.2859 (4.3687) \n","Epoch: [77][0/200] Train loss: 4.1583 (4.1583) \n","Epoch: [77][100/200] Train loss: 4.3191 (4.3234) \n","accuracy: [52.922000000000004, 52.959999999999994, 53.09199999999999]\n","\n","== Train RUC ==\n","Epoch: [78][0/200] Train loss: 4.4245 (4.4245) \n","Epoch: [78][100/200] Train loss: 4.4985 (4.3394) \n","Epoch: [78][0/200] Train loss: 4.4262 (4.4262) \n","Epoch: [78][100/200] Train loss: 4.0488 (4.3106) \n","accuracy: [53.11, 53.193999999999996, 53.276]\n","\n","== Train RUC ==\n","Epoch: [79][0/200] Train loss: 4.2317 (4.2317) \n","Epoch: [79][100/200] Train loss: 4.3048 (4.3032) \n","Epoch: [79][0/200] Train loss: 4.4508 (4.4508) \n","Epoch: [79][100/200] Train loss: 4.3658 (4.3228) \n","accuracy: [53.14, 53.198, 53.28000000000001]\n","\n","== Train RUC ==\n","Epoch: [80][0/200] Train loss: 3.8444 (3.8444) \n","Epoch: [80][100/200] Train loss: 4.5578 (4.3601) \n","Epoch: [80][0/200] Train loss: 4.0529 (4.0529) \n","Epoch: [80][100/200] Train loss: 4.1132 (4.3399) \n","accuracy: [53.142, 53.112, 53.262]\n","\n","== Train RUC ==\n","Epoch: [81][0/200] Train loss: 4.5807 (4.5807) \n","Epoch: [81][100/200] Train loss: 4.4655 (4.3643) \n","Epoch: [81][0/200] Train loss: 4.4645 (4.4645) \n","Epoch: [81][100/200] Train loss: 4.3817 (4.3360) \n","accuracy: [53.32599999999999, 53.152, 53.32]\n","\n","== Train RUC ==\n","Epoch: [82][0/200] Train loss: 4.2526 (4.2526) \n","Epoch: [82][100/200] Train loss: 4.2775 (4.3519) \n","Epoch: [82][0/200] Train loss: 4.4801 (4.4801) \n","Epoch: [82][100/200] Train loss: 4.4900 (4.3282) \n","accuracy: [53.2, 53.205999999999996, 53.318]\n","\n","== Train RUC ==\n","Epoch: [83][0/200] Train loss: 4.0072 (4.0072) \n","Epoch: [83][100/200] Train loss: 4.5976 (4.3615) \n","Epoch: [83][0/200] Train loss: 4.4859 (4.4859) \n","Epoch: [83][100/200] Train loss: 4.3995 (4.3564) \n","accuracy: [52.852, 53.093999999999994, 53.093999999999994]\n","\n","== Train RUC ==\n","Epoch: [84][0/200] Train loss: 3.7807 (3.7807) \n","Epoch: [84][100/200] Train loss: 4.5014 (4.3652) \n","Epoch: [84][0/200] Train loss: 4.3473 (4.3473) \n","Epoch: [84][100/200] Train loss: 4.5753 (4.3618) \n","accuracy: [53.006, 53.080000000000005, 53.20399999999999]\n","\n","== Train RUC ==\n","Epoch: [85][0/200] Train loss: 3.7622 (3.7622) \n","Epoch: [85][100/200] Train loss: 4.5066 (4.3552) \n","Epoch: [85][0/200] Train loss: 4.1313 (4.1313) \n","Epoch: [85][100/200] Train loss: 4.6605 (4.3098) \n","accuracy: [52.932, 53.056000000000004, 53.064]\n","\n","== Train RUC ==\n","Epoch: [86][0/200] Train loss: 4.4028 (4.4028) \n","Epoch: [86][100/200] Train loss: 4.5552 (4.3785) \n","Epoch: [86][0/200] Train loss: 4.1691 (4.1691) \n","Epoch: [86][100/200] Train loss: 4.5167 (4.3915) \n","accuracy: [53.054, 53.26, 53.246]\n","\n","== Train RUC ==\n","Epoch: [87][0/200] Train loss: 4.4894 (4.4894) \n","Epoch: [87][100/200] Train loss: 4.6033 (4.3789) \n","Epoch: [87][0/200] Train loss: 4.3944 (4.3944) \n","Epoch: [87][100/200] Train loss: 4.5601 (4.3693) \n","accuracy: [52.762, 53.244, 53.188]\n","\n","== Train RUC ==\n","Epoch: [88][0/200] Train loss: 4.3338 (4.3338) \n","Epoch: [88][100/200] Train loss: 3.7565 (4.3575) \n","Epoch: [88][0/200] Train loss: 4.5228 (4.5228) \n","Epoch: [88][100/200] Train loss: 4.5735 (4.3606) \n","accuracy: [53.03, 53.16, 53.23]\n","\n","== Train RUC ==\n","Epoch: [89][0/200] Train loss: 4.2968 (4.2968) \n","Epoch: [89][100/200] Train loss: 4.5513 (4.3741) \n","Epoch: [89][0/200] Train loss: 3.6005 (3.6005) \n","Epoch: [89][100/200] Train loss: 4.2319 (4.3352) \n","accuracy: [53.068000000000005, 53.11, 53.224000000000004]\n","\n","== Train RUC ==\n","Epoch: [90][0/200] Train loss: 4.6451 (4.6451) \n","Epoch: [90][100/200] Train loss: 4.5271 (4.3480) \n","Epoch: [90][0/200] Train loss: 4.4261 (4.4261) \n","Epoch: [90][100/200] Train loss: 4.4513 (4.3965) \n","accuracy: [52.827999999999996, 53.086, 53.190000000000005]\n","\n","== Train RUC ==\n","Epoch: [91][0/200] Train loss: 4.3326 (4.3326) \n","Epoch: [91][100/200] Train loss: 4.4931 (4.3560) \n","Epoch: [91][0/200] Train loss: 4.6221 (4.6221) \n","Epoch: [91][100/200] Train loss: 4.6105 (4.3553) \n","accuracy: [52.959999999999994, 53.193999999999996, 53.282]\n","\n","== Train RUC ==\n","Epoch: [92][0/200] Train loss: 4.3095 (4.3095) \n","Epoch: [92][100/200] Train loss: 4.4092 (4.3935) \n","Epoch: [92][0/200] Train loss: 3.9107 (3.9107) \n","Epoch: [92][100/200] Train loss: 4.2605 (4.3931) \n","accuracy: [53.09199999999999, 53.074, 53.215999999999994]\n","\n","== Train RUC ==\n","Epoch: [93][0/200] Train loss: 4.3748 (4.3748) \n","Epoch: [93][100/200] Train loss: 4.3848 (4.3777) \n","Epoch: [93][0/200] Train loss: 4.3450 (4.3450) \n","Epoch: [93][100/200] Train loss: 4.5066 (4.3787) \n","accuracy: [53.138, 53.300000000000004, 53.374]\n","\n","== Train RUC ==\n","Epoch: [94][0/200] Train loss: 4.2515 (4.2515) \n","Epoch: [94][100/200] Train loss: 4.1530 (4.3786) \n","Epoch: [94][0/200] Train loss: 4.5256 (4.5256) \n","Epoch: [94][100/200] Train loss: 4.3922 (4.4138) \n","accuracy: [52.769999999999996, 53.190000000000005, 53.12]\n","\n","== Train RUC ==\n","Epoch: [95][0/200] Train loss: 3.9778 (3.9778) \n","Epoch: [95][100/200] Train loss: 4.4327 (4.3521) \n","Epoch: [95][0/200] Train loss: 4.4718 (4.4718) \n","Epoch: [95][100/200] Train loss: 4.5502 (4.4124) \n","accuracy: [53.205999999999996, 53.239999999999995, 53.366]\n","\n","== Train RUC ==\n","Epoch: [96][0/200] Train loss: 4.6742 (4.6742) \n","Epoch: [96][100/200] Train loss: 4.5213 (4.4423) \n","Epoch: [96][0/200] Train loss: 4.7551 (4.7551) \n","Epoch: [96][100/200] Train loss: 4.5392 (4.4299) \n","accuracy: [53.21, 53.19199999999999, 53.354]\n","\n","== Train RUC ==\n","Epoch: [97][0/200] Train loss: 4.6529 (4.6529) \n","Epoch: [97][100/200] Train loss: 4.4147 (4.3670) \n","Epoch: [97][0/200] Train loss: 4.2661 (4.2661) \n","Epoch: [97][100/200] Train loss: 4.7478 (4.4114) \n","accuracy: [53.128, 53.174, 53.332]\n","\n","== Train RUC ==\n","Epoch: [98][0/200] Train loss: 4.5303 (4.5303) \n","Epoch: [98][100/200] Train loss: 4.4129 (4.4048) \n","Epoch: [98][0/200] Train loss: 4.5268 (4.5268) \n","Epoch: [98][100/200] Train loss: 4.6182 (4.4321) \n","accuracy: [53.044000000000004, 52.992, 53.152]\n","\n","== Train RUC ==\n","Epoch: [99][0/200] Train loss: 4.6352 (4.6352) \n","Epoch: [99][100/200] Train loss: 4.2187 (4.3864) \n","Epoch: [99][0/200] Train loss: 4.5880 (4.5880) \n","Epoch: [99][100/200] Train loss: 3.9563 (4.4228) \n","accuracy: [52.99, 53.102000000000004, 53.164]\n","\n","== Train RUC ==\n","Epoch: [100][0/200] Train loss: 4.4167 (4.4167) \n","Epoch: [100][100/200] Train loss: 4.3160 (4.4260) \n","Epoch: [100][0/200] Train loss: 4.6194 (4.6194) \n","Epoch: [100][100/200] Train loss: 4.5981 (4.3963) \n","accuracy: [53.117999999999995, 53.112, 53.208]\n","\n","== Train RUC ==\n","Epoch: [101][0/200] Train loss: 4.5233 (4.5233) \n","Epoch: [101][100/200] Train loss: 4.1967 (4.4122) \n","Epoch: [101][0/200] Train loss: 3.5271 (3.5271) \n","Epoch: [101][100/200] Train loss: 3.9854 (4.3848) \n","accuracy: [52.983999999999995, 53.166000000000004, 53.278000000000006]\n","\n","== Train RUC ==\n","Epoch: [102][0/200] Train loss: 4.4153 (4.4153) \n","Epoch: [102][100/200] Train loss: 4.6845 (4.4140) \n","Epoch: [102][0/200] Train loss: 4.2749 (4.2749) \n","Epoch: [102][100/200] Train loss: 4.5486 (4.4059) \n","accuracy: [52.995999999999995, 53.276, 53.238]\n","\n","== Train RUC ==\n","Epoch: [103][0/200] Train loss: 4.5724 (4.5724) \n","Epoch: [103][100/200] Train loss: 4.6609 (4.4364) \n","Epoch: [103][0/200] Train loss: 3.9666 (3.9666) \n","Epoch: [103][100/200] Train loss: 4.4406 (4.4521) \n","accuracy: [52.94799999999999, 53.198, 53.234]\n","\n","== Train RUC ==\n","Epoch: [104][0/200] Train loss: 4.1403 (4.1403) \n","Epoch: [104][100/200] Train loss: 3.5580 (4.4218) \n","Epoch: [104][0/200] Train loss: 4.5887 (4.5887) \n","Epoch: [104][100/200] Train loss: 4.7239 (4.3937) \n","accuracy: [53.25, 53.384, 53.428]\n","\n","== Train RUC ==\n","Epoch: [105][0/200] Train loss: 4.7246 (4.7246) \n","Epoch: [105][100/200] Train loss: 4.6232 (4.4254) \n","Epoch: [105][0/200] Train loss: 4.6290 (4.6290) \n","Epoch: [105][100/200] Train loss: 4.2573 (4.4722) \n","accuracy: [53.176, 53.03, 53.21399999999999]\n","\n","== Train RUC ==\n","Epoch: [106][0/200] Train loss: 3.5051 (3.5051) \n","Epoch: [106][100/200] Train loss: 4.6354 (4.4383) \n","Epoch: [106][0/200] Train loss: 4.5186 (4.5186) \n","Epoch: [106][100/200] Train loss: 4.6912 (4.3968) \n","accuracy: [52.958000000000006, 53.258, 53.227999999999994]\n","\n","== Train RUC ==\n","Epoch: [107][0/200] Train loss: 4.5632 (4.5632) \n","Epoch: [107][100/200] Train loss: 3.6581 (4.3802) \n","Epoch: [107][0/200] Train loss: 4.0724 (4.0724) \n","Epoch: [107][100/200] Train loss: 3.7614 (4.3917) \n","accuracy: [53.047999999999995, 53.04, 53.144000000000005]\n","\n","== Train RUC ==\n","Epoch: [108][0/200] Train loss: 4.4976 (4.4976) \n","Epoch: [108][100/200] Train loss: 4.7197 (4.3653) \n","Epoch: [108][0/200] Train loss: 4.6333 (4.6333) \n","Epoch: [108][100/200] Train loss: 4.6050 (4.4108) \n","accuracy: [53.022000000000006, 53.09199999999999, 53.14]\n","\n","== Train RUC ==\n","Epoch: [109][0/200] Train loss: 4.3048 (4.3048) \n","Epoch: [109][100/200] Train loss: 4.5214 (4.4590) \n","Epoch: [109][0/200] Train loss: 4.5264 (4.5264) \n","Epoch: [109][100/200] Train loss: 4.6929 (4.4157) \n","accuracy: [52.717999999999996, 53.105999999999995, 53.006]\n","\n","== Train RUC ==\n","Epoch: [110][0/200] Train loss: 4.2853 (4.2853) \n","Epoch: [110][100/200] Train loss: 4.6273 (4.4569) \n","Epoch: [110][0/200] Train loss: 4.4234 (4.4234) \n","Epoch: [110][100/200] Train loss: 4.3644 (4.4319) \n","accuracy: [53.042, 53.269999999999996, 53.25]\n","\n","== Train RUC ==\n","Epoch: [111][0/200] Train loss: 3.9802 (3.9802) \n","Epoch: [111][100/200] Train loss: 4.5448 (4.4566) \n","Epoch: [111][0/200] Train loss: 4.5529 (4.5529) \n","Epoch: [111][100/200] Train loss: 4.5186 (4.4625) \n","accuracy: [52.982, 52.995999999999995, 53.132000000000005]\n","\n","== Train RUC ==\n","Epoch: [112][0/200] Train loss: 4.4864 (4.4864) \n","Epoch: [112][100/200] Train loss: 4.6041 (4.4641) \n","Epoch: [112][0/200] Train loss: 4.5809 (4.5809) \n","Epoch: [112][100/200] Train loss: 4.0072 (4.4387) \n","accuracy: [53.006, 53.002, 53.088]\n","\n","== Train RUC ==\n","Epoch: [113][0/200] Train loss: 4.3777 (4.3777) \n","Epoch: [113][100/200] Train loss: 3.8421 (4.4089) \n","Epoch: [113][0/200] Train loss: 4.6135 (4.6135) \n","Epoch: [113][100/200] Train loss: 4.6226 (4.4521) \n","accuracy: [53.028, 53.266000000000005, 53.224000000000004]\n","\n","== Train RUC ==\n","Epoch: [114][0/200] Train loss: 4.1454 (4.1454) \n","Epoch: [114][100/200] Train loss: 4.5248 (4.4274) \n","Epoch: [114][0/200] Train loss: 4.6779 (4.6779) \n","Epoch: [114][100/200] Train loss: 4.5489 (4.4513) \n","accuracy: [52.956, 53.146, 53.162]\n","\n","== Train RUC ==\n","Epoch: [115][0/200] Train loss: 4.4401 (4.4401) \n","Epoch: [115][100/200] Train loss: 4.0821 (4.4022) \n","Epoch: [115][0/200] Train loss: 4.2407 (4.2407) \n","Epoch: [115][100/200] Train loss: 4.6281 (4.4262) \n","accuracy: [52.925999999999995, 53.064, 53.09199999999999]\n","\n","== Train RUC ==\n","Epoch: [116][0/200] Train loss: 4.5957 (4.5957) \n","Epoch: [116][100/200] Train loss: 4.2240 (4.4627) \n","Epoch: [116][0/200] Train loss: 4.4555 (4.4555) \n","Epoch: [116][100/200] Train loss: 4.4706 (4.4617) \n","accuracy: [52.959999999999994, 53.044000000000004, 53.088]\n","\n","== Train RUC ==\n","Epoch: [117][0/200] Train loss: 4.3175 (4.3175) \n","Epoch: [117][100/200] Train loss: 4.6453 (4.4522) \n","Epoch: [117][0/200] Train loss: 4.7092 (4.7092) \n","Epoch: [117][100/200] Train loss: 4.5847 (4.4210) \n","accuracy: [53.176, 53.294, 53.33599999999999]\n","\n","== Train RUC ==\n","Epoch: [118][0/200] Train loss: 4.5759 (4.5759) \n","Epoch: [118][100/200] Train loss: 4.6182 (4.4800) \n","Epoch: [118][0/200] Train loss: 4.6354 (4.6354) \n","Epoch: [118][100/200] Train loss: 4.6208 (4.4414) \n","accuracy: [52.964, 53.224000000000004, 53.215999999999994]\n","\n","== Train RUC ==\n","Epoch: [119][0/200] Train loss: 4.4198 (4.4198) \n","Epoch: [119][100/200] Train loss: 4.6420 (4.4404) \n","Epoch: [119][0/200] Train loss: 4.5374 (4.5374) \n","Epoch: [119][100/200] Train loss: 4.6305 (4.4102) \n","accuracy: [52.964, 52.983999999999995, 53.1]\n","\n","== Train RUC ==\n","Epoch: [120][0/200] Train loss: 4.5049 (4.5049) \n","Epoch: [120][100/200] Train loss: 4.6962 (4.4351) \n","Epoch: [120][0/200] Train loss: 4.6588 (4.6588) \n","Epoch: [120][100/200] Train loss: 4.5046 (4.4757) \n","accuracy: [53.181999999999995, 53.222, 53.224000000000004]\n","\n","== Train RUC ==\n","Epoch: [121][0/200] Train loss: 4.2498 (4.2498) \n","Epoch: [121][100/200] Train loss: 3.8351 (4.4340) \n","Epoch: [121][0/200] Train loss: 4.1765 (4.1765) \n","Epoch: [121][100/200] Train loss: 4.2930 (4.4990) \n","accuracy: [53.11, 53.12, 53.190000000000005]\n","\n","== Train RUC ==\n","Epoch: [122][0/200] Train loss: 4.3388 (4.3388) \n","Epoch: [122][100/200] Train loss: 4.0797 (4.4844) \n","Epoch: [122][0/200] Train loss: 4.7249 (4.7249) \n","Epoch: [122][100/200] Train loss: 4.6432 (4.4910) \n","accuracy: [52.961999999999996, 53.242, 53.224000000000004]\n","\n","== Train RUC ==\n","Epoch: [123][0/200] Train loss: 4.1110 (4.1110) \n","Epoch: [123][100/200] Train loss: 4.6458 (4.5008) \n","Epoch: [123][0/200] Train loss: 4.3722 (4.3722) \n","Epoch: [123][100/200] Train loss: 4.2571 (4.4690) \n","accuracy: [52.978, 53.146, 53.146]\n","\n","== Train RUC ==\n","Epoch: [124][0/200] Train loss: 4.4208 (4.4208) \n","Epoch: [124][100/200] Train loss: 4.3030 (4.4880) \n","Epoch: [124][0/200] Train loss: 4.7421 (4.7421) \n","Epoch: [124][100/200] Train loss: 4.2242 (4.4692) \n","accuracy: [52.866, 53.215999999999994, 53.093999999999994]\n","\n","== Train RUC ==\n","Epoch: [125][0/200] Train loss: 4.2212 (4.2212) \n","Epoch: [125][100/200] Train loss: 4.6638 (4.4492) \n","Epoch: [125][0/200] Train loss: 4.5713 (4.5713) \n","Epoch: [125][100/200] Train loss: 4.5274 (4.4586) \n","accuracy: [52.94, 53.054, 53.112]\n","\n","== Train RUC ==\n","Epoch: [126][0/200] Train loss: 4.6682 (4.6682) \n","Epoch: [126][100/200] Train loss: 4.3157 (4.4658) \n","Epoch: [126][0/200] Train loss: 4.6585 (4.6585) \n","Epoch: [126][100/200] Train loss: 4.7707 (4.4635) \n","accuracy: [52.676, 53.20399999999999, 53.02]\n","\n","== Train RUC ==\n","Epoch: [127][0/200] Train loss: 4.2350 (4.2350) \n","Epoch: [127][100/200] Train loss: 4.6551 (4.4768) \n","Epoch: [127][0/200] Train loss: 4.6667 (4.6667) \n","Epoch: [127][100/200] Train loss: 4.5806 (4.4519) \n","accuracy: [53.16, 53.166000000000004, 53.20399999999999]\n","\n","== Train RUC ==\n","Epoch: [128][0/200] Train loss: 4.1113 (4.1113) \n","Epoch: [128][100/200] Train loss: 4.6003 (4.5266) \n","Epoch: [128][0/200] Train loss: 4.7368 (4.7368) \n","Epoch: [128][100/200] Train loss: 4.4878 (4.5227) \n","accuracy: [52.910000000000004, 53.059999999999995, 53.083999999999996]\n","\n","== Train RUC ==\n","Epoch: [129][0/200] Train loss: 3.6461 (3.6461) \n","Epoch: [129][100/200] Train loss: 4.6160 (4.5102) \n","Epoch: [129][0/200] Train loss: 4.7528 (4.7528) \n","Epoch: [129][100/200] Train loss: 4.5780 (4.5197) \n","accuracy: [52.99, 53.086, 53.126]\n","\n","== Train RUC ==\n","Epoch: [130][0/200] Train loss: 4.4474 (4.4474) \n","Epoch: [130][100/200] Train loss: 4.2087 (4.4492) \n","Epoch: [130][0/200] Train loss: 4.5819 (4.5819) \n","Epoch: [130][100/200] Train loss: 4.3927 (4.4484) \n","accuracy: [52.736000000000004, 53.174, 53.03]\n","\n","== Train RUC ==\n","Epoch: [131][0/200] Train loss: 4.3045 (4.3045) \n","Epoch: [131][100/200] Train loss: 4.4593 (4.4902) \n","Epoch: [131][0/200] Train loss: 4.5683 (4.5683) \n","Epoch: [131][100/200] Train loss: 4.0372 (4.4865) \n","accuracy: [53.013999999999996, 53.23, 53.156000000000006]\n","\n","== Train RUC ==\n","Epoch: [132][0/200] Train loss: 4.6400 (4.6400) \n","Epoch: [132][100/200] Train loss: 3.9384 (4.4904) \n","Epoch: [132][0/200] Train loss: 4.8239 (4.8239) \n","Epoch: [132][100/200] Train loss: 4.6184 (4.5222) \n","accuracy: [53.047999999999995, 53.032000000000004, 53.162]\n","\n","== Train RUC ==\n","Epoch: [133][0/200] Train loss: 4.4346 (4.4346) \n","Epoch: [133][100/200] Train loss: 3.5563 (4.5294) \n","Epoch: [133][0/200] Train loss: 5.2057 (5.2057) \n","Epoch: [133][100/200] Train loss: 4.9329 (4.7198) \n","accuracy: [53.096, 53.168000000000006, 53.198]\n","\n","== Train RUC ==\n","Epoch: [134][0/200] Train loss: 3.4408 (3.4408) \n","Epoch: [134][100/200] Train loss: 3.9028 (4.4796) \n","Epoch: [134][0/200] Train loss: 4.6003 (4.6003) \n","Epoch: [134][100/200] Train loss: 4.7337 (4.4985) \n","accuracy: [52.808, 53.147999999999996, 53.024]\n","\n","== Train RUC ==\n","Epoch: [135][0/200] Train loss: 4.6166 (4.6166) \n","Epoch: [135][100/200] Train loss: 4.5818 (4.4837) \n","Epoch: [135][0/200] Train loss: 4.3594 (4.3594) \n","Epoch: [135][100/200] Train loss: 4.6944 (4.5009) \n","accuracy: [52.916, 53.128, 53.068000000000005]\n","\n","== Train RUC ==\n","Epoch: [136][0/200] Train loss: 4.6904 (4.6904) \n","Epoch: [136][100/200] Train loss: 4.1358 (4.5025) \n","Epoch: [136][0/200] Train loss: 4.7097 (4.7097) \n","Epoch: [136][100/200] Train loss: 4.1784 (4.5017) \n","accuracy: [53.006, 53.102000000000004, 53.086]\n","\n","== Train RUC ==\n","Epoch: [137][0/200] Train loss: 4.3237 (4.3237) \n","Epoch: [137][100/200] Train loss: 4.5367 (4.5001) \n","Epoch: [137][0/200] Train loss: 4.6642 (4.6642) \n","Epoch: [137][100/200] Train loss: 4.4527 (4.4987) \n","accuracy: [52.81400000000001, 53.002, 52.961999999999996]\n","\n","== Train RUC ==\n","Epoch: [138][0/200] Train loss: 4.6013 (4.6013) \n","Epoch: [138][100/200] Train loss: 4.5582 (4.4756) \n","Epoch: [138][0/200] Train loss: 4.6591 (4.6591) \n","Epoch: [138][100/200] Train loss: 4.3349 (4.5089) \n","accuracy: [52.952, 53.018, 53.042]\n","\n","== Train RUC ==\n","Epoch: [139][0/200] Train loss: 3.9327 (3.9327) \n","Epoch: [139][100/200] Train loss: 4.7360 (4.5191) \n","Epoch: [139][0/200] Train loss: 4.7451 (4.7451) \n","Epoch: [139][100/200] Train loss: 4.5967 (4.4734) \n","accuracy: [52.995999999999995, 53.098, 53.11]\n","\n","== Train RUC ==\n","Epoch: [140][0/200] Train loss: 4.4638 (4.4638) \n","Epoch: [140][100/200] Train loss: 4.6531 (4.5101) \n","Epoch: [140][0/200] Train loss: 4.6913 (4.6913) \n","Epoch: [140][100/200] Train loss: 4.8873 (4.4979) \n","accuracy: [52.934000000000005, 53.09199999999999, 53.059999999999995]\n","\n","== Train RUC ==\n","Epoch: [141][0/200] Train loss: 4.6056 (4.6056) \n","Epoch: [141][100/200] Train loss: 4.5821 (4.5639) \n","Epoch: [141][0/200] Train loss: 4.2028 (4.2028) \n","Epoch: [141][100/200] Train loss: 4.0605 (4.4790) \n","accuracy: [52.978, 53.134, 53.086]\n","\n","== Train RUC ==\n","Epoch: [142][0/200] Train loss: 4.5675 (4.5675) \n","Epoch: [142][100/200] Train loss: 3.8922 (4.5192) \n","Epoch: [142][0/200] Train loss: 4.5190 (4.5190) \n","Epoch: [142][100/200] Train loss: 4.4814 (4.5411) \n","accuracy: [52.912000000000006, 53.138, 53.081999999999994]\n","\n","== Train RUC ==\n","Epoch: [143][0/200] Train loss: 4.0557 (4.0557) \n","Epoch: [143][100/200] Train loss: 4.4911 (4.5111) \n","Epoch: [143][0/200] Train loss: 4.7304 (4.7304) \n","Epoch: [143][100/200] Train loss: 4.4187 (4.5439) \n","accuracy: [53.022000000000006, 53.09, 53.086]\n","\n","== Train RUC ==\n","Epoch: [144][0/200] Train loss: 4.4680 (4.4680) \n","Epoch: [144][100/200] Train loss: 4.6856 (4.5576) \n","Epoch: [144][0/200] Train loss: 4.4436 (4.4436) \n","Epoch: [144][100/200] Train loss: 4.6355 (4.5371) \n","accuracy: [52.852, 53.15, 53.05800000000001]\n","\n","== Train RUC ==\n","Epoch: [145][0/200] Train loss: 4.7728 (4.7728) \n","Epoch: [145][100/200] Train loss: 4.5888 (4.5191) \n","Epoch: [145][0/200] Train loss: 4.5130 (4.5130) \n","Epoch: [145][100/200] Train loss: 4.8156 (4.5216) \n","accuracy: [52.882, 53.098, 53.042]\n","\n","== Train RUC ==\n","Epoch: [146][0/200] Train loss: 4.8013 (4.8013) \n","Epoch: [146][100/200] Train loss: 4.5883 (4.5105) \n","Epoch: [146][0/200] Train loss: 4.7500 (4.7500) \n","Epoch: [146][100/200] Train loss: 4.8036 (4.5500) \n","accuracy: [53.093999999999994, 53.154, 53.117999999999995]\n","\n","== Train RUC ==\n","Epoch: [147][0/200] Train loss: 4.5461 (4.5461) \n","Epoch: [147][100/200] Train loss: 4.6163 (4.5940) \n","Epoch: [147][0/200] Train loss: 4.2488 (4.2488) \n","Epoch: [147][100/200] Train loss: 4.6245 (4.5303) \n","accuracy: [53.008, 53.152, 53.05]\n","\n","== Train RUC ==\n","Epoch: [148][0/200] Train loss: 4.8311 (4.8311) \n","Epoch: [148][100/200] Train loss: 4.0951 (4.5554) \n","Epoch: [148][0/200] Train loss: 4.6630 (4.6630) \n","Epoch: [148][100/200] Train loss: 4.4595 (4.5566) \n","accuracy: [53.112, 53.080000000000005, 53.126]\n","\n","== Train RUC ==\n","Epoch: [149][0/200] Train loss: 4.7029 (4.7029) \n","Epoch: [149][100/200] Train loss: 4.8819 (4.5381) \n","Epoch: [149][0/200] Train loss: 4.6090 (4.6090) \n","Epoch: [149][100/200] Train loss: 4.5564 (4.5671) \n","accuracy: [53.16, 53.112, 53.152]\n","\n","== Train RUC ==\n","Epoch: [150][0/200] Train loss: 4.7115 (4.7115) \n","Epoch: [150][100/200] Train loss: 4.2099 (4.5515) \n","Epoch: [150][0/200] Train loss: 4.3968 (4.3968) \n","Epoch: [150][100/200] Train loss: 4.7731 (4.5179) \n","accuracy: [53.038, 53.124, 53.105999999999995]\n","\n","== Train RUC ==\n","Epoch: [151][0/200] Train loss: 4.7295 (4.7295) \n","Epoch: [151][100/200] Train loss: 4.7412 (4.5538) \n","Epoch: [151][0/200] Train loss: 4.5547 (4.5547) \n","Epoch: [151][100/200] Train loss: 4.7379 (4.4955) \n","accuracy: [52.992, 53.09, 53.066]\n","\n","== Train RUC ==\n","Epoch: [152][0/200] Train loss: 4.8463 (4.8463) \n","Epoch: [152][100/200] Train loss: 4.0881 (4.5378) \n","Epoch: [152][0/200] Train loss: 4.6020 (4.6020) \n","Epoch: [152][100/200] Train loss: 4.5654 (4.5233) \n","accuracy: [53.0, 53.09199999999999, 53.042]\n","\n","== Train RUC ==\n","Epoch: [153][0/200] Train loss: 4.6030 (4.6030) \n","Epoch: [153][100/200] Train loss: 4.4244 (4.5192) \n","Epoch: [153][0/200] Train loss: 4.6677 (4.6677) \n","Epoch: [153][100/200] Train loss: 4.8067 (4.5153) \n","accuracy: [53.002, 53.112, 53.078]\n","\n","== Train RUC ==\n","Epoch: [154][0/200] Train loss: 4.7370 (4.7370) \n","Epoch: [154][100/200] Train loss: 4.9255 (4.5385) \n","Epoch: [154][0/200] Train loss: 4.2255 (4.2255) \n","Epoch: [154][100/200] Train loss: 4.7407 (4.5197) \n","accuracy: [52.956, 53.15, 53.066]\n","\n","== Train RUC ==\n","Epoch: [155][0/200] Train loss: 4.2490 (4.2490) \n","Epoch: [155][100/200] Train loss: 3.7436 (4.5054) \n","Epoch: [155][0/200] Train loss: 4.3778 (4.3778) \n","Epoch: [155][100/200] Train loss: 4.5975 (4.5541) \n","accuracy: [52.946000000000005, 53.11, 53.054]\n","\n","== Train RUC ==\n","Epoch: [156][0/200] Train loss: 4.8382 (4.8382) \n","Epoch: [156][100/200] Train loss: 4.1372 (4.5663) \n","Epoch: [156][0/200] Train loss: 4.6788 (4.6788) \n","Epoch: [156][100/200] Train loss: 4.4670 (4.5989) \n","accuracy: [52.974, 53.102000000000004, 53.04600000000001]\n","\n","== Train RUC ==\n","Epoch: [157][0/200] Train loss: 4.8478 (4.8478) \n","Epoch: [157][100/200] Train loss: 4.8530 (4.5533) \n","Epoch: [157][0/200] Train loss: 4.2529 (4.2529) \n","Epoch: [157][100/200] Train loss: 4.7106 (4.6053) \n","accuracy: [52.952, 53.136, 53.068000000000005]\n","\n","== Train RUC ==\n","Epoch: [158][0/200] Train loss: 4.7133 (4.7133) \n","Epoch: [158][100/200] Train loss: 3.5912 (4.5362) \n","Epoch: [158][0/200] Train loss: 4.6037 (4.6037) \n","Epoch: [158][100/200] Train loss: 4.7111 (4.5535) \n","accuracy: [53.002, 53.080000000000005, 53.066]\n","\n","== Train RUC ==\n","Epoch: [159][0/200] Train loss: 4.6074 (4.6074) \n","Epoch: [159][100/200] Train loss: 4.8084 (4.5667) \n","Epoch: [159][0/200] Train loss: 4.8003 (4.8003) \n","Epoch: [159][100/200] Train loss: 4.8493 (4.5092) \n","accuracy: [53.042, 53.15, 53.11]\n","\n","== Train RUC ==\n","Epoch: [160][0/200] Train loss: 4.7915 (4.7915) \n","Epoch: [160][100/200] Train loss: 4.0534 (4.5404) \n","Epoch: [160][0/200] Train loss: 4.4474 (4.4474) \n","Epoch: [160][100/200] Train loss: 4.3460 (4.5787) \n","accuracy: [52.974, 53.062, 53.028]\n","\n","== Train RUC ==\n","Epoch: [161][0/200] Train loss: 4.6152 (4.6152) \n","Epoch: [161][100/200] Train loss: 4.9638 (4.5436) \n","Epoch: [161][0/200] Train loss: 4.4021 (4.4021) \n","Epoch: [161][100/200] Train loss: 4.7471 (4.5109) \n","accuracy: [53.012, 53.132000000000005, 53.074]\n","\n","== Train RUC ==\n","Epoch: [162][0/200] Train loss: 4.7860 (4.7860) \n","Epoch: [162][100/200] Train loss: 4.2726 (4.5991) \n","Epoch: [162][0/200] Train loss: 4.7032 (4.7032) \n","Epoch: [162][100/200] Train loss: 4.6167 (4.5839) \n","accuracy: [53.038, 53.105999999999995, 53.105999999999995]\n","\n","== Train RUC ==\n","Epoch: [163][0/200] Train loss: 4.4352 (4.4352) \n","Epoch: [163][100/200] Train loss: 4.6158 (4.5680) \n","Epoch: [163][0/200] Train loss: 4.6841 (4.6841) \n","Epoch: [163][100/200] Train loss: 4.8212 (4.5399) \n","accuracy: [53.078, 53.162, 53.174]\n","\n","== Train RUC ==\n","Epoch: [164][0/200] Train loss: 4.6723 (4.6723) \n","Epoch: [164][100/200] Train loss: 4.0501 (4.5540) \n","Epoch: [164][0/200] Train loss: 4.1204 (4.1204) \n","Epoch: [164][100/200] Train loss: 4.6684 (4.5941) \n","accuracy: [52.964, 53.09, 53.028]\n","\n","== Train RUC ==\n","Epoch: [165][0/200] Train loss: 4.8630 (4.8630) \n","Epoch: [165][100/200] Train loss: 4.0602 (4.5169) \n","Epoch: [165][0/200] Train loss: 4.5170 (4.5170) \n","Epoch: [165][100/200] Train loss: 4.7626 (4.5440) \n","accuracy: [52.994, 53.096, 53.062]\n","\n","== Train RUC ==\n","Epoch: [166][0/200] Train loss: 4.6612 (4.6612) \n","Epoch: [166][100/200] Train loss: 4.2998 (4.5600) \n","Epoch: [166][0/200] Train loss: 4.8239 (4.8239) \n","Epoch: [166][100/200] Train loss: 4.6948 (4.5234) \n","accuracy: [53.05, 53.068000000000005, 53.056000000000004]\n","\n","== Train RUC ==\n","Epoch: [167][0/200] Train loss: 4.1247 (4.1247) \n","Epoch: [167][100/200] Train loss: 4.8218 (4.5419) \n","Epoch: [167][0/200] Train loss: 4.5426 (4.5426) \n","Epoch: [167][100/200] Train loss: 4.8416 (4.4987) \n","accuracy: [53.064, 53.102000000000004, 53.1]\n","\n","== Train RUC ==\n","Epoch: [168][0/200] Train loss: 4.2232 (4.2232) \n","Epoch: [168][100/200] Train loss: 4.3601 (4.5778) \n","Epoch: [168][0/200] Train loss: 4.4758 (4.4758) \n","Epoch: [168][100/200] Train loss: 4.3584 (4.5544) \n","accuracy: [53.0, 53.114, 53.062]\n","\n","== Train RUC ==\n","Epoch: [169][0/200] Train loss: 3.3188 (3.3188) \n","Epoch: [169][100/200] Train loss: 4.4295 (4.5488) \n","Epoch: [169][0/200] Train loss: 4.4947 (4.4947) \n","Epoch: [169][100/200] Train loss: 3.5965 (4.5886) \n","accuracy: [52.978, 53.06999999999999, 53.034000000000006]\n","\n","== Train RUC ==\n","Epoch: [170][0/200] Train loss: 4.5597 (4.5597) \n","Epoch: [170][100/200] Train loss: 4.8925 (4.5870) \n","Epoch: [170][0/200] Train loss: 4.6581 (4.6581) \n","Epoch: [170][100/200] Train loss: 4.4916 (4.6151) \n","accuracy: [52.971999999999994, 53.03, 53.018]\n","\n","== Train RUC ==\n","Epoch: [171][0/200] Train loss: 4.8114 (4.8114) \n","Epoch: [171][100/200] Train loss: 4.6717 (4.5508) \n","Epoch: [171][0/200] Train loss: 4.5797 (4.5797) \n","Epoch: [171][100/200] Train loss: 4.7779 (4.5427) \n","accuracy: [52.958000000000006, 53.124, 53.056000000000004]\n","\n","== Train RUC ==\n","Epoch: [172][0/200] Train loss: 4.8883 (4.8883) \n","Epoch: [172][100/200] Train loss: 4.4838 (4.6051) \n","Epoch: [172][0/200] Train loss: 4.4197 (4.4197) \n","Epoch: [172][100/200] Train loss: 4.2815 (4.5877) \n","accuracy: [53.010000000000005, 53.006, 53.018]\n","\n","== Train RUC ==\n","Epoch: [173][0/200] Train loss: 4.7164 (4.7164) \n","Epoch: [173][100/200] Train loss: 4.9627 (4.6087) \n","Epoch: [173][0/200] Train loss: 3.7608 (3.7608) \n","Epoch: [173][100/200] Train loss: 3.8879 (4.5733) \n","accuracy: [52.99, 53.162, 53.06999999999999]\n","\n","== Train RUC ==\n","Epoch: [174][0/200] Train loss: 3.9576 (3.9576) \n","Epoch: [174][100/200] Train loss: 4.8476 (4.5167) \n","Epoch: [174][0/200] Train loss: 4.8952 (4.8952) \n","Epoch: [174][100/200] Train loss: 4.7892 (4.5679) \n","accuracy: [52.992, 53.13, 53.054]\n","\n","== Train RUC ==\n","Epoch: [175][0/200] Train loss: 4.8341 (4.8341) \n","Epoch: [175][100/200] Train loss: 4.4341 (4.5441) \n","Epoch: [175][0/200] Train loss: 3.9626 (3.9626) \n","Epoch: [175][100/200] Train loss: 4.9179 (4.5226) \n","accuracy: [52.949999999999996, 53.108, 53.010000000000005]\n","\n","== Train RUC ==\n","Epoch: [176][0/200] Train loss: 4.4092 (4.4092) \n","Epoch: [176][100/200] Train loss: 4.4662 (4.6224) \n","Epoch: [176][0/200] Train loss: 3.9554 (3.9554) \n","Epoch: [176][100/200] Train loss: 4.7187 (4.5833) \n","accuracy: [53.012, 53.156000000000006, 53.05800000000001]\n","\n","== Train RUC ==\n","Epoch: [177][0/200] Train loss: 4.5883 (4.5883) \n","Epoch: [177][100/200] Train loss: 4.4576 (4.5760) \n","Epoch: [177][0/200] Train loss: 3.9930 (3.9930) \n","Epoch: [177][100/200] Train loss: 4.6712 (4.6280) \n","accuracy: [52.916, 53.09, 53.004]\n","\n","== Train RUC ==\n","Epoch: [178][0/200] Train loss: 3.3876 (3.3876) \n","Epoch: [178][100/200] Train loss: 4.8027 (4.5673) \n","Epoch: [178][0/200] Train loss: 4.6124 (4.6124) \n","Epoch: [178][100/200] Train loss: 4.6762 (4.6217) \n","accuracy: [53.034000000000006, 53.093999999999994, 53.03600000000001]\n","\n","== Train RUC ==\n","Epoch: [179][0/200] Train loss: 4.9166 (4.9166) \n","Epoch: [179][100/200] Train loss: 4.9039 (4.6272) \n","Epoch: [179][0/200] Train loss: 4.4829 (4.4829) \n","Epoch: [179][100/200] Train loss: 4.5674 (4.6394) \n","accuracy: [53.0, 53.16, 53.047999999999995]\n","\n","== Train RUC ==\n","Epoch: [180][0/200] Train loss: 4.5818 (4.5818) \n","Epoch: [180][100/200] Train loss: 4.9032 (4.6057) \n","Epoch: [180][0/200] Train loss: 4.8211 (4.8211) \n","Epoch: [180][100/200] Train loss: 4.4494 (4.5930) \n","accuracy: [52.959999999999994, 53.136, 53.047999999999995]\n","\n","== Train RUC ==\n","Epoch: [181][0/200] Train loss: 4.9337 (4.9337) \n","Epoch: [181][100/200] Train loss: 4.6990 (4.5945) \n","Epoch: [181][0/200] Train loss: 4.9123 (4.9123) \n","Epoch: [181][100/200] Train loss: 4.7194 (4.5773) \n","accuracy: [52.964, 53.11, 53.04600000000001]\n","\n","== Train RUC ==\n","Epoch: [182][0/200] Train loss: 4.5178 (4.5178) \n","Epoch: [182][100/200] Train loss: 4.5160 (4.6106) \n","Epoch: [182][0/200] Train loss: 4.1294 (4.1294) \n","Epoch: [182][100/200] Train loss: 4.6822 (4.6447) \n","accuracy: [52.961999999999996, 53.062, 53.03600000000001]\n","\n","== Train RUC ==\n","Epoch: [183][0/200] Train loss: 5.0741 (5.0741) \n","Epoch: [183][100/200] Train loss: 4.8810 (4.5850) \n","Epoch: [183][0/200] Train loss: 4.8210 (4.8210) \n","Epoch: [183][100/200] Train loss: 4.7417 (4.6388) \n","accuracy: [52.994, 53.105999999999995, 53.010000000000005]\n","\n","== Train RUC ==\n","Epoch: [184][0/200] Train loss: 4.8291 (4.8291) \n","Epoch: [184][100/200] Train loss: 4.8437 (4.6514) \n","Epoch: [184][0/200] Train loss: 4.8143 (4.8143) \n","Epoch: [184][100/200] Train loss: 4.5920 (4.5956) \n","accuracy: [53.002, 53.081999999999994, 53.05]\n","\n","== Train RUC ==\n","Epoch: [185][0/200] Train loss: 4.8486 (4.8486) \n","Epoch: [185][100/200] Train loss: 3.9092 (4.5614) \n","Epoch: [185][0/200] Train loss: 4.6353 (4.6353) \n","Epoch: [185][100/200] Train loss: 4.7567 (4.6616) \n","accuracy: [53.006, 53.122, 53.044000000000004]\n","\n","== Train RUC ==\n","Epoch: [186][0/200] Train loss: 4.5150 (4.5150) \n","Epoch: [186][100/200] Train loss: 4.8213 (4.6493) \n","Epoch: [186][0/200] Train loss: 4.5087 (4.5087) \n","Epoch: [186][100/200] Train loss: 4.5187 (4.6005) \n","accuracy: [52.956, 53.083999999999996, 53.013999999999996]\n","\n","== Train RUC ==\n","Epoch: [187][0/200] Train loss: 4.0508 (4.0508) \n","Epoch: [187][100/200] Train loss: 4.7713 (4.6583) \n","Epoch: [187][0/200] Train loss: 4.7561 (4.7561) \n","Epoch: [187][100/200] Train loss: 4.7018 (4.5822) \n","accuracy: [52.988, 53.098, 53.044000000000004]\n","\n","== Train RUC ==\n","Epoch: [188][0/200] Train loss: 4.6432 (4.6432) \n","Epoch: [188][100/200] Train loss: 4.8917 (4.6300) \n","Epoch: [188][0/200] Train loss: 4.2964 (4.2964) \n","Epoch: [188][100/200] Train loss: 4.8335 (4.5798) \n","accuracy: [53.012, 53.114, 53.056000000000004]\n","\n","== Train RUC ==\n","Epoch: [189][0/200] Train loss: 4.7284 (4.7284) \n","Epoch: [189][100/200] Train loss: 4.8747 (4.6107) \n","Epoch: [189][0/200] Train loss: 4.8329 (4.8329) \n","Epoch: [189][100/200] Train loss: 4.9477 (4.6323) \n","accuracy: [52.994, 53.086, 53.024]\n","\n","== Train RUC ==\n","Epoch: [190][0/200] Train loss: 4.8722 (4.8722) \n","Epoch: [190][100/200] Train loss: 4.8847 (4.6115) \n","Epoch: [190][0/200] Train loss: 4.6491 (4.6491) \n","Epoch: [190][100/200] Train loss: 4.1503 (4.6710) \n","accuracy: [53.013999999999996, 53.104, 53.05]\n","\n","== Train RUC ==\n","Epoch: [191][0/200] Train loss: 4.8468 (4.8468) \n","Epoch: [191][100/200] Train loss: 4.7052 (4.6430) \n","Epoch: [191][0/200] Train loss: 4.6317 (4.6317) \n","Epoch: [191][100/200] Train loss: 4.9506 (4.6156) \n","accuracy: [53.02, 53.1, 53.05]\n","\n","== Train RUC ==\n","Epoch: [192][0/200] Train loss: 4.7568 (4.7568) \n","Epoch: [192][100/200] Train loss: 4.8392 (4.6206) \n","Epoch: [192][0/200] Train loss: 4.9302 (4.9302) \n","Epoch: [192][100/200] Train loss: 3.3717 (4.6323) \n","accuracy: [52.96999999999999, 53.098, 53.03]\n","\n","== Train RUC ==\n","Epoch: [193][0/200] Train loss: 4.6602 (4.6602) \n","Epoch: [193][100/200] Train loss: 4.9289 (4.6084) \n","Epoch: [193][0/200] Train loss: 4.8722 (4.8722) \n","Epoch: [193][100/200] Train loss: 4.8674 (4.5865) \n","accuracy: [52.956, 53.076, 53.025999999999996]\n","\n","== Train RUC ==\n","Epoch: [194][0/200] Train loss: 4.7455 (4.7455) \n","Epoch: [194][100/200] Train loss: 4.0289 (4.6151) \n","Epoch: [194][0/200] Train loss: 4.9059 (4.9059) \n","Epoch: [194][100/200] Train loss: 4.9316 (4.6631) \n","accuracy: [52.96999999999999, 53.074, 52.983999999999995]\n","\n","== Train RUC ==\n","Epoch: [195][0/200] Train loss: 4.8141 (4.8141) \n","Epoch: [195][100/200] Train loss: 5.0116 (4.6161) \n","Epoch: [195][0/200] Train loss: 4.3322 (4.3322) \n","Epoch: [195][100/200] Train loss: 4.8311 (4.6978) \n","accuracy: [52.968, 53.083999999999996, 53.042]\n","\n","== Train RUC ==\n","Epoch: [196][0/200] Train loss: 4.4996 (4.4996) \n","Epoch: [196][100/200] Train loss: 4.8098 (4.6778) \n","Epoch: [196][0/200] Train loss: 4.1824 (4.1824) \n","Epoch: [196][100/200] Train loss: 4.9918 (4.6803) \n","accuracy: [52.961999999999996, 53.086, 53.024]\n","\n","== Train RUC ==\n","Epoch: [197][0/200] Train loss: 3.9588 (3.9588) \n","Epoch: [197][100/200] Train loss: 4.6134 (4.6814) \n","Epoch: [197][0/200] Train loss: 4.5846 (4.5846) \n","Epoch: [197][100/200] Train loss: 4.9727 (4.6521) \n","accuracy: [52.982, 53.06999999999999, 53.024]\n","\n","== Train RUC ==\n","Epoch: [198][0/200] Train loss: 4.8415 (4.8415) \n","Epoch: [198][100/200] Train loss: 3.6697 (4.6079) \n","Epoch: [198][0/200] Train loss: 4.4295 (4.4295) \n","Epoch: [198][100/200] Train loss: 4.5791 (4.5842) \n","accuracy: [53.004, 53.06999999999999, 53.05]\n","\n","== Train RUC ==\n","Epoch: [199][0/200] Train loss: 4.6944 (4.6944) \n","Epoch: [199][100/200] Train loss: 5.0200 (4.6472) \n","Epoch: [199][0/200] Train loss: 4.3328 (4.3328) \n","Epoch: [199][100/200] Train loss: 4.8909 (4.6429) \n","accuracy: [52.983999999999995, 53.078, 53.042]\n","\n"]}]}]}